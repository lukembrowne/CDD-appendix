[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Appendix to Conspecific density dependence in plant communities: a theory-based toolkit for empirical studies",
    "section": "",
    "text": "1 Preface\nThis document serves as an appendix to the manuscript Conspecific density dependence in plant communities: a theory-based toolkit for empirical studies, with the goal of providing code and guidance on conducting analyses of density dependence following the best practices outlined in the main manuscript and based on the publication Hülsmann et al. (2024).\nIt is important to note that for many steps in the analysis, there are valid alternative approaches that could be used to model the data, each with their own strengths. For example, while we have chosen to largely follow the modeling approach of Hülsmann et al. (2024) that uses Generalized Additive Models (GAMs) to capture the flexible, nonlinear effects of density dependence, other researchers may prefer linear or nonlinear parametric models. The semi-parametric nature of GAMs offers the advantage of flexibility, helping to avoid biases—particularly when interactions between variables are not strictly linear, as discussed in Detto et al. (2019). However, this flexibility can come at the cost of statistical power. When the functional form of the relationships is well understood or can be reasonably inferred, parametric models may offer a more efficient alternative.\nThis document aims to provide a clear starting point for applying density dependence analyses, while recognizing that the choice of methods may need to be adapted to fit specific datasets and research questions.\n\n\n\n\n\n\nDetto, Matteo, Marco D Visser, S Joseph Wright, and Stephen W Pacala. 2019. “Bias in the Detection of Negative Density Dependence in Plant Communities.” Ecology Letters 22 (September): 1923–39. https://doi.org/10.1111/ele.13372.\n\n\nHülsmann, Lisa, Ryan A Chisholm, Liza Comita, Marco D Visser, Melina de Souza Leite, Salomon Aguilar, Kristina J Anderson-Teixeira, et al. 2024. “Latitudinal Patterns in Stabilizing Density Dependence of Forest Communities.” Nature 627 (8004): 564–71."
  },
  {
    "objectID": "calculating-neighborhood.html#overview",
    "href": "calculating-neighborhood.html#overview",
    "title": "2  Calculating neighborhood densities",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThis tutorial follows from Section 2. “Modeling considerations when estimating CDD” of the main text:\n\n\n\n\n\n\nSection 2. “Modeling considerations when estimating CDD”\n\n\n\nFor survival and growth models, defining the density metric and the size of local neighborhoods also require consideration. The density of conspecific and heterospecific neighbors surrounding a focal individual (or plot) can be measured within different distances (i.e., radii). Within a radius, neighbors should be weighted in the density summation with regard to their size and distance to the focal individuals, with the rationale that neighbors at a greater distance and of smaller size have smaller effects. Typically, weighting involves dividing the basal area or diameter by a linear function or an exponential function of distance to allow competitive effects of an individual of a given size to decline with distance (Uriarte et al. 2010; Canham et al. 2006). Biologically meaningful radii should be tested based on the size/life stage, vital rate of interest, and the likely agents of density dependence in the system. For a system of interest, we suggest comparing models with multiple distances (e.g., using maximum likelihood or information criterion) because forests may differ in how neighbors influence performance.\n\n\nHere, we demonstrate how to calculate the density of conspecific, heterospecific, and all neighbors surrounding a focal individual or plot for a given distance when XY coordinates are known. As a result, this section requires that the user’s dataset contains the location of mapped stems. If locations are not known (as is common in seedling studies or smaller plots), continue to part 2 of this appendix.\nWe then demonstrate how to weight the calculation of neighborhood density by individual neighbor size (i.e., basal area) and distance using an exponential decay function (one of many possible decay functions), allowing the competitive effects of density values to saturate with increasing distances (Uriarte et al. 2010; Canham et al. 2006).\nTo assess which shape parameters of the exponential decay function are most appropriate for the data set, we fit models with multiple combinations of decay function values and compare models using log likelihood.\nFrom a computational perspective, this approach can be relatively resource intensive both in terms of time and object size. It’s possible to make this approach more efficient by subdividing the data (e.g., by plot) or using more efficient data structures such as data.table.\nWe also note that alternative approaches allow the estimation of the effective scale of neighborhood interactions directly from data (see Barber et al. 2022). An excellent case study using the Stan programming language is available here.\n\n\n\n\n\n\nNote\n\n\n\nThe following code is heavily adapted from the latitudinalCNDD repository by Lisa Hülsmann from the publication Hülsmann et al. (2024)."
  },
  {
    "objectID": "calculating-neighborhood.html#load-libraries-and-data",
    "href": "calculating-neighborhood.html#load-libraries-and-data",
    "title": "2  Calculating neighborhood densities",
    "section": "2.2 Load libraries and data",
    "text": "2.2 Load libraries and data\n\n\nCode\n# Load libraries\nlibrary(tidyr) # For data manipulation\nlibrary(dplyr) # For data manipulation\nlibrary(ggplot2) # For data plotting\nlibrary(spatstat.geom) # For spatial analysis\nlibrary(here) # For managing working directories\nlibrary(mgcv) # For fitting gams\nlibrary(lubridate) # For calculating census intervals\nlibrary(broom) # For processing fit models\nlibrary(purrr) # For data manipulation\nlibrary(kableExtra) # For table styling"
  },
  {
    "objectID": "calculating-neighborhood.html#data-format-explanation",
    "href": "calculating-neighborhood.html#data-format-explanation",
    "title": "2  Calculating neighborhood densities",
    "section": "2.3 Data format explanation",
    "text": "2.3 Data format explanation\nFor this tutorial, we will be using a small example subset of data from Barro Colorado Island (BCI), Panama (available here) that includes 7,028 observations, of 3,771 individuals of 16 tree species across two censuses from the larger 50 ha BCI Forest Dynamics Plot data set (more information here). Each stem is mapped and its diameter at 1.3m above ground is measured (DBH), which allows us to calculate neighborhood densities across different distances, and as a function of neighbor size (e.g., basal area), respectively.\nThe code below assumes the data is in a format where each row is an observation for an individual from a census. For this data set, the column descriptions are as follows:\n\ntreeID: unique identifier for each tree\nsp: species code\ngx: spatial coordinate on x axis\ngy: spatial coordinate on y axis\ndbh: diameter at breast height (mm)\nba: basal area (m^2)\nstatus: status at census, A = alive, D = dead\ndate: date of observation\ncensus: census number\nmort: mortality status at census, 1 = dead, 0 = alive\nmort_next: mortality status at next census, 1 = dead, 0 = alive\ninterval: time interval between censuses in years\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\nhead(dat, n = 5)\n\n\n# A tibble: 5 × 12\n  treeID sp        gx    gy   dbh     ba status date        mort mort_next\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 3884   ceibpe  296.  24.8  1500 1.77   A      1981-11-10     0         0\n2 3998   cordbi  280.  45.4   281 0.0620 A      1981-12-11     0         0\n3 4065   ceibpe  289. 266.   2000 3.14   A      1982-01-08     0         0\n4 4070   cordbi  283. 287.    356 0.0995 A      1982-01-08     0         0\n5 4119   ceibpe  258. 224.   1600 2.01   A      1981-12-13     0         0\n# ℹ 2 more variables: interval &lt;dbl&gt;, census &lt;dbl&gt;\n\n\n\nWe can produce a plot Figure 2.1 of the tree locations, where the size of the point is scaled to basal area and colored by species, with each census displayed as a panel:\n\n\nCode\nggplot(dat, aes(x = gx, y = gy, size = ba, col = sp)) + \n  geom_point() + \n  facet_wrap(~census) +\n  theme_bw(10) + \n  theme(legend.position = \"right\") + \n  coord_fixed(ratio = 1) + \n  labs(size = \"Basal area\", col = \"Species\")\n\n\n\n\n\nFigure 2.1: Map of tree locations by census. Points are individual stems scaled by basal area, and colors represent the six-letter identifiers of 16 common species at the BCI forest plot."
  },
  {
    "objectID": "calculating-neighborhood.html#define-an-exponential-decay-function",
    "href": "calculating-neighborhood.html#define-an-exponential-decay-function",
    "title": "2  Calculating neighborhood densities",
    "section": "2.4 Define an exponential decay function",
    "text": "2.4 Define an exponential decay function\nIn our neighborhood, we model neighborhood densities using an exponential decay function. In principle, it’s possible to use various different decay functions to explore various ranges that determine the rate of decay.\nHere, we present the general framework for how one might explore different scenarios, though the ultimate choice of decay function and parameter values will depend on the study and is beyond the scope of this Appendix. Note that this method of calculating neighborhood density is only possible when neighbors’ x-y coordinates are known.\nFirst, we write a function in R for exponential decay with distance, where mu controls the shape of the curve:\n\n\nCode\nexponential_decay &lt;- function(mu, distance){\n  return(exp(-(1/mu * distance)))\n}\n\n\nLet’s see what the exponential decay function looks like across a range of mu values (Figure 2.2):\n\n\nCode\n# Set range of mu values and distances\ndecay_values &lt;- seq(from = 1, to = 25, by = 2)\n\n# Use sprintf to add leading 0s, will help with sorting later on\ndecay_names = paste(\"exp\", sprintf(\"%02s\", decay_values), sep = \"\") \n\ndistances &lt;- seq(1, 100, 1)\n\n# Generate a dataframe with each combination of mu and distance\nexample_decay_values &lt;- expand_grid(decay_values, distances) %&gt;%\n                        # Rename columns\n                        rename(decay_value = decay_values, \n                               distance = distances)\n\n# Evaluate distance decay function for each combination of \n# mu and distance\nexample_decay_values$decay &lt;- exponential_decay(\n                              mu = example_decay_values$decay_value,\n                              distance = example_decay_values$distance)\n\n# Plot results\nggplot(example_decay_values, \n       aes(x = distance, y = decay, \n           color = decay_value, group = decay_value)) + \n  ylab(\"Density\") + \n  xlab(\"Distance (m)\") + \n  scale_color_continuous(name = \"Value of \\ndecay constant\") + \n  geom_line() + \n  theme_bw(12)\n\n\n\n\n\nFigure 2.2: Plot of decay function\n\n\n\n\n\n2.4.1 Determine which trees are at edge of plot\nTrees near the edge of plot boundaries have incomplete information about their neighborhood, because neighboring trees outside of the plot boundaries are not mapped, and we do not advise using boundary corrections to include those stems.Typically trees within certain distance threshold from the plot edge are excluded from analysis, but still included in calculations of neighborhood densities.\nWe are going to add a column to our data set called ‘edge’ that is TRUE if within a set distance to the edge of the plot (e.g., 30 m in the example below).\nFor our example data set, the dimensions of the plot are 300 x 300 m, ranging from 0-300 on both the x and y axis, and representing a subset of the overall 50 ha forest dynamics plot at BCI.\n\n\nCode\n# Set threshold for distance to edge\ndistance_threshold_edge = 30\n\n# Add in min and max values for corners of plot\nmin_x &lt;- 0\nmax_x &lt;- 300\nmin_y &lt;- 0\nmax_y &lt;- 300\n\ndat$edge = dat$gx &lt; min_x + distance_threshold_edge |\n           dat$gx &gt; max_x - distance_threshold_edge |\n           dat$gy &lt; min_y + distance_threshold_edge |\n           dat$gy &gt; max_y - distance_threshold_edge\n\n# How many trees fall within the edge threshold?\ntable(dat$edge)\n\n\n\nFALSE  TRUE \n 4526  2502 \n\n\nBelow is a plot Figure 2.3 of tree locations colored by whether they fall within the edge threshold or not, separated out for each census.\n\n\nCode\nggplot(dat, aes(x = gx, y = gy, col = edge)) + \n  geom_point() + \n  facet_wrap(~census) +\n  coord_fixed(ratio = 1) + \n  theme_bw(12)\n\n\n\n\n\nFigure 2.3: Map of tree locations colored by whether they fall within the edge threshold"
  },
  {
    "objectID": "calculating-neighborhood.html#calculate-distances-among-focal-individual-and-neighbors",
    "href": "calculating-neighborhood.html#calculate-distances-among-focal-individual-and-neighbors",
    "title": "2  Calculating neighborhood densities",
    "section": "2.5 Calculate distances among focal individual and neighbors",
    "text": "2.5 Calculate distances among focal individual and neighbors\nNext, we will calculate distances among individuals in our plot, using an upper threshold distance of what to consider a neighbor.\nWe use the ‘spatstat.geom’ package to efficiently calculate distances among individuals.\nBecause this example only includes one census interval (two censuses total), we will subset down to just the first census to calculate neighborhood density.\nIf the data set were to contain multiple census intervals, it would be necessary to calculate neighborhood density separately for each individual in each census interval, using only the individuals that were alive at the beginning of that census interval, or using an average density value between two consecutive censuses.\n\n\nCode\n# Set distance threshold for considering neighbors\ndistance_threshold_neighborhood &lt;- 30\n\n# Subset to first census - this will be different for different \n# datasets\ndat_first_census &lt;- dat %&gt;%\n                    filter(census == 1)\n\n# Format into 'ppp' object\ndat_ppp = spatstat.geom::ppp(dat_first_census$gx, dat_first_census$gy, \n                             window = owin(range(dat$gx), \n                                           range(dat$gy)), \n                                           checkdup = F)\n\n# Determine close pairs based on distance threshold\n# Returns a list that we convert to tibble later\nneighbors = spatstat.geom::closepairs(dat_ppp, \n              rmax = distance_threshold_neighborhood, # Max radius\n              what = \"ijd\", # return indicies i, j, and distance \n              twice = TRUE)\n\n# Convert to dataframe\nneighbors &lt;- as_tibble(neighbors)\n\n# Take a peek at the data\n# i = index of focal individual\n# j = index of neighbor\n# d = distance to neighbor\nhead(neighbors)\n\n\n# A tibble: 6 × 3\n      i     j     d\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1  3227  3252 27.0 \n2  3227  3238 17.1 \n3  3227  3240  5.75\n4  3227  3229 18.6 \n5  3227  3253 24.4 \n6  3227  3222 20.8 \n\n\nNext we add in additional columns for individual characteristic, (e.g., species, size) and whether they are located near the edge of the plot (blue area in Figure 2.3).\n\n\nCode\n# add additional columns\n\n# Add species for individual i\nneighbors$sp_i = dat_first_census$sp[neighbors$i] \n\n# Add whether individual i is near edge\nneighbors$edge_i = dat_first_census$edge[neighbors$i] \n\n# Add species for individual j\nneighbors$sp_j = dat_first_census$sp[neighbors$j] \n\n# Add basal area of individual j\nneighbors$ba_j = dat_first_census$ba[neighbors$j] \n\n\nWe then want to add a column that indicates whether the comparison between the focal individual and the neighbor is conspecific or heterospecific because we are interested separately estimating the densities of conspecifics and heterospecifics.\n\n\nCode\nneighbors$comparison_type &lt;- ifelse(neighbors$sp_i == neighbors$sp_j,\n                                    yes = \"con\", # conspecific\n                                    no = \"het\") # heterospecific\n\n\nWe then remove focal trees that are too close to the edge of the plot.\n\n\nCode\n# remove focal trees i that are at the edge\nneighbors = neighbors[!neighbors$edge_i, ]\n\n\nNext, we add columns to our neighbors data set that indicates the distance decay multiplier and the distance decay multiplier weighted by basal area.\n\n\nCode\n# Loop through distance decay values\nfor(x in 1:length(decay_values)){\n  \n  #  Add column for distance decay multiplier for each decay value\n  # add _ba suffix to column name - will eventually be summed based on\n  # number of individual neighbors\n  neighbors[, paste0(decay_names[x], \"_N\")] &lt;- exponential_decay(mu = decay_values[x], \n                                                   distance = neighbors$d)\n  \n  # Weight basal area by distance decay multiplier\n  # add _ba suffix to column name\n  neighbors[, paste0(decay_names[x], \"_BA\")] &lt;- exponential_decay(\n                                                mu = decay_values[x], \n                             distance = neighbors$d) * neighbors$ba_j\n}\n\n\nDepending on how many distance decay values are being investigated, there may be many columns in the data frame.\n\n\nCode\nhead(neighbors)\n\n\n# A tibble: 6 × 34\n      i     j     d sp_i  edge_i sp_j     ba_j comparison_type  exp01_N exp01_BA\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1  2973  2993 14.3  cord… FALSE  capp… 1.57e-4 het             6.36e- 7 9.98e-11\n2  2973  3122 20.4  cord… FALSE  des2… 4.91e-4 het             1.40e- 9 6.86e-13\n3  2973  2974  5.86 cord… FALSE  des2… 7.85e-5 het             2.85e- 3 2.24e- 7\n4  2973  3123 27.2  cord… FALSE  des2… 7.85e-5 het             1.47e-12 1.16e-16\n5  2973  3121 14.5  cord… FALSE  mico… 1.77e-4 het             5.27e- 7 9.32e-11\n6  2973  2936 13.4  cord… FALSE  des2… 7.85e-5 het             1.45e- 6 1.14e-10\n# ℹ 24 more variables: exp03_N &lt;dbl&gt;, exp03_BA &lt;dbl&gt;, exp05_N &lt;dbl&gt;,\n#   exp05_BA &lt;dbl&gt;, exp07_N &lt;dbl&gt;, exp07_BA &lt;dbl&gt;, exp09_N &lt;dbl&gt;,\n#   exp09_BA &lt;dbl&gt;, exp11_N &lt;dbl&gt;, exp11_BA &lt;dbl&gt;, exp13_N &lt;dbl&gt;,\n#   exp13_BA &lt;dbl&gt;, exp15_N &lt;dbl&gt;, exp15_BA &lt;dbl&gt;, exp17_N &lt;dbl&gt;,\n#   exp17_BA &lt;dbl&gt;, exp19_N &lt;dbl&gt;, exp19_BA &lt;dbl&gt;, exp21_N &lt;dbl&gt;,\n#   exp21_BA &lt;dbl&gt;, exp23_N &lt;dbl&gt;, exp23_BA &lt;dbl&gt;, exp25_N &lt;dbl&gt;,\n#   exp25_BA &lt;dbl&gt;"
  },
  {
    "objectID": "calculating-neighborhood.html#calculate-neighborhood-density",
    "href": "calculating-neighborhood.html#calculate-neighborhood-density",
    "title": "2  Calculating neighborhood densities",
    "section": "2.6 Calculate neighborhood density",
    "text": "2.6 Calculate neighborhood density\nThen we summarize the neighborhood density for each focal tree separately for conspecifics and heterospecifics.\n\n\nCode\n# Simple calculations of number of neighbors and total basal area, \n# ignoring distance decay multiplier\nneighbors_summary &lt;- neighbors %&gt;%\n                     group_by(i, comparison_type) %&gt;%\n                     summarise(nodecay_N = n(), # count of neighbors\n                          nodecay_BA = sum(ba_j)) # sum of basal area)\n\n# Add in decay columns\nneighbors_summary_decay &lt;- neighbors %&gt;%\n                            group_by(i, comparison_type) %&gt;%\n                      # Select only columns related to distance decay\n                            select(starts_with(\"exp\")) %&gt;% \n                      # Summarize them all by summing columns\n                            summarise_all(sum) \n\n# Join both together\nneighbors_summary &lt;- left_join(neighbors_summary, \n                               neighbors_summary_decay,\n                               by = c(\"i\", \"comparison_type\"))\n\n# Add treeID column\nneighbors_summary$treeID&lt;-dat_first_census$treeID[neighbors_summary$i]\n\n\n   \n# If there are any focal individuals with no neighbors, add values \n# of 0 for neighborhood densities \n      noNeighbors = dat_first_census$treeID[!dat_first_census$treeID \n                                      %in% neighbors_summary$treeID &\n                                          !dat_first_census$edge]\n      \n      # If there are individuals with no neighbors\n      if (length(noNeighbors) &gt; 0) {\n        neighbors_summary = bind_rows(neighbors_summary, \n                                      expand_grid(i = NA, \n                                             treeID = noNeighbors, \n                             comparison_type = c(\"het\", \"cons\"))) %&gt;%\n                            # Add 0s where NA\n                            mutate_all(replace_na, replace = 0) \n        }\n\n# Take a peak at the data\nhead(neighbors_summary)\n\n\n# A tibble: 6 × 31\n# Groups:   i [6]\n      i comparison_type nodecay_N nodecay_BA exp01_N   exp01_BA exp03_N exp03_BA\n  &lt;int&gt; &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1     5 het                   115     0.487  0.0991     4.05e-5   2.03  0.00172 \n2     8 het                    94     0.0539 0.181      3.63e-5   1.21  0.000703\n3     9 het                   151     2.17   0.0464     1.67e-4   1.65  0.00287 \n4    10 het                    76     0.0509 0.00131    4.72e-7   0.464 0.000175\n5    11 het                    59     0.0631 0.00859    1.99e-6   0.493 0.000277\n6    12 het                    93     0.0506 0.0441     9.51e-6   1.57  0.000745\n# ℹ 23 more variables: exp05_N &lt;dbl&gt;, exp05_BA &lt;dbl&gt;, exp07_N &lt;dbl&gt;,\n#   exp07_BA &lt;dbl&gt;, exp09_N &lt;dbl&gt;, exp09_BA &lt;dbl&gt;, exp11_N &lt;dbl&gt;,\n#   exp11_BA &lt;dbl&gt;, exp13_N &lt;dbl&gt;, exp13_BA &lt;dbl&gt;, exp15_N &lt;dbl&gt;,\n#   exp15_BA &lt;dbl&gt;, exp17_N &lt;dbl&gt;, exp17_BA &lt;dbl&gt;, exp19_N &lt;dbl&gt;,\n#   exp19_BA &lt;dbl&gt;, exp21_N &lt;dbl&gt;, exp21_BA &lt;dbl&gt;, exp23_N &lt;dbl&gt;,\n#   exp23_BA &lt;dbl&gt;, exp25_N &lt;dbl&gt;, exp25_BA &lt;dbl&gt;, treeID &lt;chr&gt;\n\n\nAs described in the main text, it can be advantageous to use total density that includes combined conspecific and heterospecific densities as a covariate, rather than heterospecific density.\nHere, we calculate total density by summing heterospecific and conspecific densities.\n\n\nCode\n# First convert to long format which will make it easy to sum across \n# heterospecific and conspecific values\nneighbors_summary_long_format &lt;-  neighbors_summary %&gt;%\n                                  pivot_longer(cols = -c(\"i\", \"comparison_type\", \"treeID\"))\n\n# Sum across heterospecific and conspecific values and rename to 'total'\nneighbors_total_long_format &lt;- neighbors_summary_long_format %&gt;%\n                                group_by(i, treeID, name) %&gt;%\n                                summarize(value = sum(value)) %&gt;%\n                                mutate(comparison_type = \"total\")\n\n# Bind together conspecific and 'total' densities\n# remove heterospecific columns\n# fill in 0s where there are no neighbors\nneighbors_summary_total = bind_rows(neighbors_summary_long_format,\n                                    neighbors_total_long_format) %&gt;%\n                        # Can filter out heterospecific neighborhood \n                        # values by uncommenting this line of the\n                        # objects become too large\n                        #  filter(comparison_type != \"het\") %&gt;% \n                mutate(name = paste0(comparison_type, \"_\", name)) %&gt;%\n                select(-comparison_type) %&gt;%\n                pivot_wider(names_from = name, values_from = value, \n                            values_fill = 0)"
  },
  {
    "objectID": "calculating-neighborhood.html#model-mortality-as-a-function-of-neighborhood-density",
    "href": "calculating-neighborhood.html#model-mortality-as-a-function-of-neighborhood-density",
    "title": "2  Calculating neighborhood densities",
    "section": "2.7 Model mortality as a function of neighborhood density",
    "text": "2.7 Model mortality as a function of neighborhood density\nTo determine the ‘best’ decay parameter to use, we fit species-specific mortality models using Generalized Additive Models GAMs and compare models based on log likelihoods. GAMs are flexible statistical models that combine linear and non-linear components to capture complex relationships in data.\nWe first create our data set that we will use in the GAMs, subsetting down to just one census interval (because our example dataset only has 2 censuses) and removing trees close to the edge as above. In other datasets, you may have multiple census intervals, where it would be common practice to include ‘year’ or ‘census’ as a random effect in the model, but otherwise the overall approach is similar.\n\n\nCode\n# Join census data with neighborhood data\ndat_gam &lt;- left_join(dat_first_census,\n                               neighbors_summary_total,\n                               by = \"treeID\")\n\n# Remove edge trees (these will have NAs for densities calcualations)\ndat_gam &lt;- dat_gam %&gt;%\n                    filter(edge == FALSE)\n\n\nFor each species, we summarize data availability to help determine parameters for the GAM smooth terms.\n\n\nCode\n# Summarize data availability at species level to set the degree of \n# smoothness for GAM smooth terms\nsp_summary &lt;- dat_gam %&gt;% \n  group_by(sp) %&gt;% \n  summarise(ndead = sum(mort_next),\n            range_con_BA = max(con_nodecay_BA) - min(con_nodecay_BA),\n            max_con_BA = max(con_nodecay_BA),\n            unique_con_BA = length(unique(con_nodecay_BA)),\n            unique_total_BA = length(unique(total_nodecay_BA)),\n            range_con_N = max(con_nodecay_N) - min(con_nodecay_N),\n            max_con_N = max(con_nodecay_N),\n            unique_con_N = length(unique(con_nodecay_N)),\n            unique_total_N = length(unique(total_nodecay_N)),\n            unique_dbh = length(unique(dbh))\n  )\n\n# Filter out species that have 0% mortality\nsp_summary &lt;- sp_summary %&gt;%\n  filter(ndead &gt; 0)\n\n\nIn this long block of code, we loop over all possible combinations of decay values for different metrics of neighborhood densities weighted by distance for each species and fit a separate GAM for each model. For each GAM, we assess whether the model was able to be fit, and when it was able to be fit, whether it converged or produced warnings. We save the results of successful model fits into a list that we will process later.\nFor large datasets where individual GAMs take a long time to run, the code could be modified to run in parallel, either locally on a personal computer or across a computing cluster.\n\n\nCode\n# Initialize list that will save model outputs\nres_mod &lt;- list()\n\n\n# Model run settings\nrun_settings &lt;- expand_grid(species = unique(sp_summary$sp),\n                            decay_con = c(\"nodecay\", decay_names),\n                            decay_total = c(\"nodecay\", decay_names),\n                            nhood_data_type = c(\"N\", \"BA\"))\n\n\n# Loop through model run settings\nfor(run_settings_row in 1:nrow(run_settings)){\n  \n  # Extract values from run settings dataframe\n  species &lt;- run_settings$species[run_settings_row]\n  decay_con &lt;- run_settings$decay_con[run_settings_row]\n  decay_total &lt;- run_settings$decay_total[run_settings_row]\n  nhood_data_type &lt;- run_settings$nhood_data_type[run_settings_row]\n\n  # Subset down to just focal species\n  dat_subset &lt;- dat_gam %&gt;%\n    filter(sp == species)\n\n  # Set run name\n  run_name &lt;- paste0(species, \"_total\", decay_total,\"_con\", \n                     decay_con, \"_\", nhood_data_type)\n  \n  # Print status if desired\n  # cat(\"Working on run: \", run_name, \" ...\\n\")\n\n  # Create model formula\n  # Initial DBH included as predictor variable\n  form =  paste0(\"mort_next ~ s(dbh, k = k1) + s(total_\", decay_total, \n                              \"_\", nhood_data_type, \n                              \", k = k2)  + s(con_\", decay_con, \n                              \"_\", nhood_data_type, \n                              \", k = k3)\")\n    \n  # Convert into formula\n  form &lt;- as.formula(form)\n    \n    # Choose penalties for model fitting\n    # set to default 10 (the same as -1)\n    # The higher the value of k, the more flexible the smooth term becomes, allowing for more intricate and wiggly patterns. Conversely, lower values of k result in smoother and simpler representations.\n    k1 = k2 = k3 = 10\n    if (k1 &gt; sp_summary$unique_dbh[sp_summary$sp == species]) { \n      k1 = sp_summary$unique_dbh[sp_summary$sp == species] - 2\n    }\n    if (k2 &gt; sp_summary$unique_total_N[sp_summary$sp == species]) {\n      k2 = sp_summary$unique_total_N [sp_summary$sp == species]- 2\n    }\n    if (k3 &gt; sp_summary$unique_con_N[sp_summary$sp == species]) { \n      k3 = sp_summary$unique_con_N[sp_summary$sp == species] - 2 \n    }\n    \n\n   # Fit model\n   # wrap in a try function to catch any errors\n   mod = try(gam(form,\n            family = binomial(link=cloglog),\n            offset = log(interval),\n            data = dat_subset,\n            method = \"REML\"), \n          silent = T)\n\n   \n    # Check if model was able to fit\n    if (!any(class(mod) == \"gam\")) {\n      # print(paste(\"gam failed for:\", run_name))\n    } else {\n\n    # Check if gam converged\n    if (!mod$converged) {\n      # print(paste(\"no convergence for:\", run_name))\n    } else {\n  \n      # check for complete separation\n      # https://stats.stackexchange.com/questions/336424/issue-with-complete-separation-in-logistic-regression-in-r\n      # Explore warning \"glm.fit: fitted probabilities numerically 0 \n      # or 1 occurred\"\n      eps &lt;- 10 * .Machine$double.eps\n      glm0.resids &lt;- augment(x = mod) %&gt;%\n        mutate(p = 1 / (1 + exp(-.fitted)),\n               warning = p &gt; 1-eps,\n               influence = order(.hat, decreasing = T))\n      infl_limit = round(nrow(glm0.resids)/10, 0)\n      # check if none of the warnings is among the 10% most \n      # influential observations, than it is okay..\n      num = any(glm0.resids$warning & glm0.resids$influence &lt; infl_limit)\n      \n      # complete separation\n      if (num) {\n       # print(paste(\"complete separation is likely for:\", run_name))\n      } else {\n        \n        # missing Vc\n        if (is.null(mod$Vc)) {\n         # print(paste(\"Vc not available for:\", run_name))\n        } else {\n        \n          # Add resulting model to list if it passes all checks\n          res_mod[[run_name]] &lt;- mod\n          \n        } # Vc ifelse\n      } # complete separation ifelse\n    } # convergence ifelse\n  } # model available ifelse\n} # end run settings loop"
  },
  {
    "objectID": "calculating-neighborhood.html#summarize-model-fits",
    "href": "calculating-neighborhood.html#summarize-model-fits",
    "title": "2  Calculating neighborhood densities",
    "section": "2.8 Summarize model fits",
    "text": "2.8 Summarize model fits\nNext we will extract summaries for each model with broom::glance() that provides key information like degrees of freedom, log likelihood, AIC, etc.\n\n\nCode\n# Extract summaries for each model into a list\nsums = lapply(res_mod, broom::glance) \n\n# Add a column for model run to each object in the list\nsums = Map(cbind, sums, run_name = names(sums)) \nsums = do.call(rbind, sums) # Bind elements of list together by rows\nrownames(sums) &lt;- NULL # Remove row names\n\n# Separate run name into columns for species, decay, and density type\nsums &lt;- sums %&gt;%\n        separate(run_name, into = c(\"sp\", \"decay_total\", \n                                    \"decay_con\", \"density_type\"), \n                 remove = FALSE)\n\n# Remove 'total' and 'con' from decay columns\nsums$decay_total &lt;- gsub(\"total\", \"\", sums$decay_total)\nsums$decay_con &lt;- gsub(\"con\", \"\", sums$decay_con)\n\n# Rearrange columns  \nsums &lt;- sums %&gt;%\n          select(run_name, sp, decay_total, decay_con, density_type, \n                 everything()) \n\n# Take a look at the model summaries\nhead(sums)\n\n                       run_name     sp decay_total decay_con density_type\n1 cappfr_totalnodecay_connodecay_N cappfr nodecay nodecay N 2 cappfr_totalnodecay_connodecay_BA cappfr nodecay nodecay BA 3 cappfr_totalexp01_connodecay_N cappfr exp01 nodecay N 4 cappfr_totalexp01_connodecay_BA cappfr exp01 nodecay BA 5 cappfr_totalexp03_connodecay_N cappfr exp03 nodecay N 6 cappfr_totalexp03_connodecay_BA cappfr exp03 nodecay BA df logLik AIC BIC deviance df.residual nobs adj.r.squared 1 4.231224 -22.61561 54.10008 70.37389 45.23123 285.7688 290 0.001949120 2 7.360197 -17.20583 51.02825 81.51869 34.41167 282.6398 290 0.048186759 3 5.063787 -22.64430 56.52330 77.13828 45.28861 284.9362 290 -0.000915769 4 8.081624 -17.53607 53.89847 88.44366 35.07214 281.9184 290 0.134401283 5 8.071055 -17.86595 54.97936 90.29731 35.73189 281.9289 290 0.079728079 6 7.926840 -17.62220 53.84700 87.98169 35.24439 282.0732 290 0.133467764 npar 1 28 2 28 3 28 4 28 5 28 6 28\n\nDue to limited sample sizes, it is likely that GAMs will not fit for each species. For example, in the table below of summary data for species where models did not successfully fit/converge, there are several instances where all individuals of the species survived during the census, leading to no mortality events to use in the model.\n\nWe need to exclude species without complete model runs from our overall calculations when looking for optimal decay parameters across the entire data set.\n\n\nCode\n# Tally up number of model runs by species and total decay values\ntable(sums$sp, sums$decay_total)\n\n\n        \n         exp01 exp03 exp05 exp07 exp09 exp11 exp13 exp15 exp17 exp19 exp21\n  cappfr    28    28    28    28    28    28    28    28    28    28    28\n  cordbi    27    28    28    28    28    28    28    28    28    28    27\n  des2pa    28    28    28    28    28    28    28    28    28    28    28\n  micone    28    28    28    28    28    28    28    28    28    28    28\n  pentma    28    28    28    28    28    23    27    28    28    28    28\n  stylst    28    28    28    28    28    28    28    28    28    28    28\n        \n         exp23 exp25 nodecay\n  cappfr    28    28      28\n  cordbi    28    28      28\n  des2pa    28    28      28\n  micone    28    28      28\n  pentma    28    28      28\n  stylst    28    28      28\n\n\nCode\n# get incomplete run-site-species combinations\nrun_counts_by_sp &lt;- sums %&gt;% \n                    group_by(sp) %&gt;%\n                    tally() %&gt;%\n                    # Join with overall species list\n                    left_join(sp_summary %&gt;% select(sp), ., by = \"sp\") \n\n# Get expected number of runs if all models work\nexpected_total_runs &lt;- run_settings %&gt;%\n                       group_by(species) %&gt;%\n                       tally() %&gt;%\n                       pull(n) %&gt;%\n                       max()\n\n# Save species names where they didn't have all expected combinations \n# of model runs\nincomplete = run_counts_by_sp$sp[run_counts_by_sp$n &lt; \n                                   expected_total_runs | \n                                   is.na(run_counts_by_sp$n)]\n\n# Species with successful runs\nhead(sp_summary[!sp_summary$sp %in% incomplete, ])\n\n\n# A tibble: 4 × 11\n  sp     ndead range_con_BA max_con_BA unique_con_BA unique_total_BA range_con_N\n  &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;int&gt;           &lt;int&gt;       &lt;dbl&gt;\n1 cappfr     5      0.0183     0.0190            248             290          30\n2 des2pa   174      0.144      0.149            1318            1341         123\n3 micone    38      0.00293    0.00293            49              66          12\n4 stylst     3      0.0165     0.0165             92             104          13\n# ℹ 4 more variables: max_con_N &lt;dbl&gt;, unique_con_N &lt;int&gt;,\n#   unique_total_N &lt;int&gt;, unique_dbh &lt;int&gt;\n\n\nCode\n# Species without successful runs\nhead(sp_summary[sp_summary$sp %in% incomplete, ])\n\n\n# A tibble: 4 × 11\n  sp     ndead range_con_BA max_con_BA unique_con_BA unique_total_BA range_con_N\n  &lt;chr&gt;  &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;         &lt;int&gt;           &lt;int&gt;       &lt;dbl&gt;\n1 acalma     2      0.00446    0.00446             4               7           2\n2 cordbi     7      0.249      0.249              36              43          10\n3 pentma    12      0.00363    0.00363            20              33           6\n4 sponra     3      0.0760     0.0760             12              18           5\n# ℹ 4 more variables: max_con_N &lt;dbl&gt;, unique_con_N &lt;int&gt;,\n#   unique_total_N &lt;int&gt;, unique_dbh &lt;int&gt;"
  },
  {
    "objectID": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-across-all-species",
    "href": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-across-all-species",
    "title": "2  Calculating neighborhood densities",
    "section": "2.9 Selecting optimum decay parameter values across all species",
    "text": "2.9 Selecting optimum decay parameter values across all species\nWe then summarize different model criteria across all species runs. To look for the optimal value for decay parameters, we sum log likelihoods across all species for a given decay parameter combination and choose the resulting parameter combination with the highest summed log likelihood.\nOne crucial point is that each run of the grid search should be done with the same set of trees, even if smaller neighborhodoes not need a buffer of 30 m.\n\n\nCode\nsums_total &lt;- sums %&gt;% \n              filter(!sp %in% incomplete) %&gt;%\n              group_by(decay_total, decay_con, density_type) %&gt;%\n              summarise(nvalues = n(),\n                        sumlogLik = sum(logLik),\n                        meanlogLik = mean(logLik)) %&gt;%\n              arrange(decay_total, decay_con, density_type)\n\nsums_total\n\n\n# A tibble: 392 × 6\n# Groups:   decay_total, decay_con [196]\n   decay_total decay_con density_type nvalues sumlogLik meanlogLik\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 exp01       exp01     BA                 4     -556.      -139.\n 2 exp01       exp01     N                  4     -554.      -138.\n 3 exp01       exp03     BA                 4     -553.      -138.\n 4 exp01       exp03     N                  4     -558.      -140.\n 5 exp01       exp05     BA                 4     -554.      -138.\n 6 exp01       exp05     N                  4     -561.      -140.\n 7 exp01       exp07     BA                 4     -557.      -139.\n 8 exp01       exp07     N                  4     -563.      -141.\n 9 exp01       exp09     BA                 4     -558.      -140.\n10 exp01       exp09     N                  4     -564.      -141.\n# ℹ 382 more rows\n\n\nWe then create a heatmap plot Figure 2.4 of summed log likelihoods for all fixed devay (mu) parameter combinations across all species, with the optimal parameter combination marked with an X\n\n\nCode\n# Find optimum value separately for N and BA\n  optimum &lt;- sums_total %&gt;%\n               group_by(density_type) %&gt;%\n               slice_max(sumlogLik)\n  \n# Plot heatmap of log likelihood values\n  ggplot(sums_total, aes(x = decay_total, y = decay_con, \n                         fill = sumlogLik)) +\n    geom_tile(width = 0.9, height = 0.9, col = \"black\") + \n    scale_fill_viridis_c() + # viridis color palette\n    geom_label(data = optimum, label = \"X\") + \n    labs(x = \"Decay total density\", y = \"Decay conspecific density\", \n         fill = \"sumlogLik\") + \n    facet_wrap(~density_type, ncol = 1) + \n    theme_bw(12) + \n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, \n                                     hjust=1))\n\n\n\n\n\nFigure 2.4: Heatmap of optimal values for decay constants\n\n\n\n\nFor this data set, the following are the optimal decay parameter values across all species separately for neighborhood density calculated by abundance (N) and by basal area (BA)\n\n\nCode\noptimum\n\n\n# A tibble: 2 × 6\n# Groups:   density_type [2]\n  decay_total decay_con density_type nvalues sumlogLik meanlogLik\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 exp19       exp25     BA                 4     -542.      -135.\n2 exp03       exp01     N                  4     -542.      -135."
  },
  {
    "objectID": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-separately-for-each-species",
    "href": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-separately-for-each-species",
    "title": "2  Calculating neighborhood densities",
    "section": "2.10 Selecting optimum decay parameter values separately for each species",
    "text": "2.10 Selecting optimum decay parameter values separately for each species\nAlternatively, it may be useful to determine optimum decay parameters on a species by species basis. To do this, we find the maximum log liklihood for each species separately across decay parameter combination and choose the resulting parameter combination with the highest log likelihood.\n\n\nCode\nsums_total_by_spp &lt;- sums %&gt;% \n              filter(!sp %in% incomplete) %&gt;%\n              group_by(decay_total, decay_con, density_type, sp) %&gt;%\n              summarise(nvalues = n(),\n                        sumlogLik = sum(logLik),\n                        meanlogLik = mean(logLik)) %&gt;%\n              arrange(decay_total, decay_con, density_type)\n\nsums_total_by_spp\n\n\n# A tibble: 1,568 × 7\n# Groups:   decay_total, decay_con, density_type [392]\n   decay_total decay_con density_type sp     nvalues sumlogLik meanlogLik\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 exp01       exp01     BA           cappfr       1    -16.6      -16.6 \n 2 exp01       exp01     BA           des2pa       1   -489.      -489.  \n 3 exp01       exp01     BA           micone       1    -43.7      -43.7 \n 4 exp01       exp01     BA           stylst       1     -6.84      -6.84\n 5 exp01       exp01     N            cappfr       1    -14.3      -14.3 \n 6 exp01       exp01     N            des2pa       1   -489.      -489.  \n 7 exp01       exp01     N            micone       1    -41.3      -41.3 \n 8 exp01       exp01     N            stylst       1     -8.64      -8.64\n 9 exp01       exp03     BA           cappfr       1    -20.7      -20.7 \n10 exp01       exp03     BA           des2pa       1   -485.      -485.  \n# ℹ 1,558 more rows\n\n\nWe create a heatmap plot Figure 2.5 of log likelihoods for all fixed decay parameter combinations for each species separately, with the optimal parameter combination marked with an X. We only display the first three species here, because with data sets containing many species, it’s hard to visualize all the species on one graph and you may wish to subdivide the plot further for visualization.\n\n\nCode\n  sums &lt;- sums %&gt;%\n    filter(sp %in% c(\"cappfr\", \"cordbi\", \"des2pa\")) %&gt;%\n    group_by(sp) %&gt;%\n    # Scale loglikihood by species to help with visualization\n    mutate(logLik_scaled = scale(logLik)) %&gt;% \n    ungroup()\n \n# Find optimum value separately for N and BA\n  optimum_by_sp &lt;- sums %&gt;%\n               group_by(sp, density_type) %&gt;%\n               slice_max(logLik_scaled, with_ties = FALSE)\n\n# Plot heatmap of log likelihood values\n  ggplot(sums, aes(x = decay_total, y = decay_con,\n                   fill = logLik_scaled)) +\n    geom_tile(width = 0.9, height = 0.9, col = \"black\") +\n    geom_label(data = optimum_by_sp, label = \"X\") +\n    labs(x = \"Decay total density\", y = \"Decay conspecific density\", \n         fill = \"logLik\") +\n    facet_wrap(~sp + density_type, ncol = 2, scales = \"free\") +\n    scale_fill_viridis_c() + # viridis color palette\n    theme_bw(12) + \n    theme(legend.position = \"right\") + \n    labs(fill = \"Log likelihood\\n(scaled)\") + \n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, \n                                     hjust=1))\n\n\n\n\n\nFigure 2.5: Heatmap of optimal values for decay constants separately for each species\n\n\n\n\nFor this data set, the following are the optimal decay parameter values for each species separately for neighborhood density calculated by abundance (N) and by basal area (BA):\n\n\nCode\noptimum_by_sp\n\n\n# A tibble: 6 × 15\n# Groups:   sp, density_type [6]\n  run_name sp    decay_total decay_con density_type    df    logLik   AIC    BIC\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 cappfr_… capp… exp19       exp25     BA           11.1  -8.74e+ 0  43.3   90.6\n2 cappfr_… capp… exp03       exp01     N            11.0  -1.18e+ 1  49.6   97.4\n3 cordbi_… cord… exp05       exp03     BA            7.61 -1.25e+ 1  43.1   58.9\n4 cordbi_… cord… exp01       exp07     N            14.8  -2.61e-12  31.8   59.5\n5 des2pa_… des2… exp25       exp09     BA            8.36 -4.82e+ 2 984.  1037. \n6 des2pa_… des2… exp03       nodecay   N             7.33 -4.82e+ 2 981.  1026. \n# ℹ 6 more variables: deviance &lt;dbl&gt;, df.residual &lt;dbl&gt;, nobs &lt;int&gt;,\n#   adj.r.squared &lt;dbl&gt;, npar &lt;int&gt;, logLik_scaled &lt;dbl[,1]&gt;\n\n\n\n\n\n\n\n\nBarber, Cristina, Andrii Zaiats, Cara Applestein, Lisa Rosenthal, and T Trevor Caughlin. 2022. “Bayesian Models for Spatially Explicit Interactions Between Neighbouring Plants.” Methods in Ecology and Evolution.\n\n\nCanham, Charles D, Michael J Papaik, Marı́a Uriarte, William H McWilliams, Jennifer C Jenkins, and Mark J Twery. 2006. “Neighborhood Analyses of Canopy Tree Competition Along Environmental Gradients in New England Forests.” Ecological Applications 16 (2): 540–54.\n\n\nHülsmann, Lisa, Ryan A Chisholm, Liza Comita, Marco D Visser, Melina de Souza Leite, Salomon Aguilar, Kristina J Anderson-Teixeira, et al. 2024. “Latitudinal Patterns in Stabilizing Density Dependence of Forest Communities.” Nature 627 (8004): 564–71.\n\n\nUriarte, Marı́a, Nathan G Swenson, Robin L Chazdon, Liza S Comita, W John Kress, David Erickson, Jimena Forero-Montaña, Jess K Zimmerman, and Jill Thompson. 2010. “Trait Similarity, Shared Ancestry and the Structure of Neighbourhood Interactions in a Subtropical Wet Forest: Implications for Community Assembly.” Ecology Letters 13 (12): 1503–14."
  },
  {
    "objectID": "calculating-marginal-effects.html#overview",
    "href": "calculating-marginal-effects.html#overview",
    "title": "3  Calculating Marginal Effects",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIn this section, we examine a subset of the Barro Colorado Island (BCI) 50-ha plot seedling data to illustrate the impact of conspecific density on mortality probability. We calculate the Average Marginal Effect (more about marginal effects in general here) as our metric of the strength conspecific density dependence. We then estimate both the absolute Average Marginal Effect (aAME) and the relative Average Marginal Effect (rAME). The aAME represents the average absolute change in mortality probability due to a specified increase in conspecific density. In contrast, the rAME is the relative change in mortality probability compared to a baseline value.\nThe standard approach for modeling plant CDD patterns presumes a linear relationship between performance (e.g., mortality) and conspecific neighborhood density metrics. However, in this section, we use ‘Generalized Additive Models (GAMs)’ as they offer flexibility for non-linear relationships between performance and predictors when empirically suported.\n\n\n\n\n\n\nNote\n\n\n\nNote: The code is adapted from (Hülsmann et al. 2024) and the latitudinalCNDD repository by Lisa Hülsmann."
  },
  {
    "objectID": "calculating-marginal-effects.html#load-libraries",
    "href": "calculating-marginal-effects.html#load-libraries",
    "title": "3  Calculating Marginal Effects",
    "section": "3.2 Load libraries",
    "text": "3.2 Load libraries\n\n\nCode\n# Load libraries\nlibrary(readr)\nlibrary(skimr)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(kableExtra) \nlibrary(knitr) \nlibrary(mgcViz)\nlibrary(mgcv)\nlibrary(MASS)\nlibrary(tidyr)"
  },
  {
    "objectID": "calculating-marginal-effects.html#load-data",
    "href": "calculating-marginal-effects.html#load-data",
    "title": "3  Calculating Marginal Effects",
    "section": "3.3 Load data",
    "text": "3.3 Load data\nOur demonstration here used seedling data. We utilized a subset of the Barro Colorado Island (BCI) 50-ha forest dynamics plot seedling data, encompassing only 30 species. Seedlings are censused in 1x1 m plots, distributed at 5-m intervals throughout the 50-ha plot. In this example, neighbor densities are calculated using the number of conspecific and heterospecific seedling neighbors within the same 1x1m plot as the focal individual. Please note that this analysis is solely for demonstration purposes; hence, no biological conclusions should be drawn from the results due to the limited data subset used.\nThe code below assumes the data is in a format where each row is an observation for an individual from a census. For this particular data set, the column descriptions are as follows:\n\nId: unique identifier for each seedling\nplot: location\nspp: species\ndate: date of observation\nyear: year of observation\ncensus: census number\nstatus: status at census, 0 = alive, 1 = dead\nheight.last.census: height.last.census\ncon_dens: number of conspecific neighbors within each seedling plot\ntotal_dens: number of total neighbors\nhet_dens: number of heterospecifics neighbors\ntime.since.last.census: time interval between censuses\n\n\n\nCode\ndata_BCI &lt;- read_csv(\"./data/BCI_seedling_data_30_spp_2023.csv\")\ncolnames(data_BCI)\n\n\n [1] \"id\"                            \"q20\"                          \n [3] \"plot\"                          \"tag\"                          \n [5] \"spp\"                           \"date\"                         \n [7] \"year\"                          \"census\"                       \n [9] \"status\"                        \"height.last.census\"           \n[11] \"height.last.census.log.scaled\" \"con_dens\"                     \n[13] \"total_dens\"                    \"het_dens\"                     \n[15] \"time.since.last.census\"       \n\n\nCode\n# make sure variables are taken as factor\ndata_BCI$census &lt;- factor(data_BCI$census)\ndata_BCI$spp &lt;- factor(data_BCI$spp)\ndata_BCI$plot &lt;- factor(data_BCI$plot)\ndata_BCI$height=data_BCI$height.last.census\ndata_BCI$interval=as.numeric(data_BCI$time.since.last.census)\n\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\n# Exploring data \n  # visualize \n  ggplot(data_BCI, aes(x = con_dens)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"steelblue2\") +\n  labs(x = \"Conspecific density\", y = \"Count\", \n       title = \"Conspecific densities\") + \n  theme_bw(12)"
  },
  {
    "objectID": "calculating-marginal-effects.html#handling-data-deficient-species",
    "href": "calculating-marginal-effects.html#handling-data-deficient-species",
    "title": "3  Calculating Marginal Effects",
    "section": "3.4 Handling “data deficient” species",
    "text": "3.4 Handling “data deficient” species\nWe categorize a species as data deficient if it has fewer than four unique conspecific density values. This threshold is important because estimating a reliable “slope” requires at least a few different values along the variable of interest. At the end of this section of the code, a data frame named ‘nsp’ will be generated. This data frame will classify each species as either “data deficient” or “not data deficient”.\nFrom our data set 9 species out of 30 were assigned as data deficient. Here, we used the thresholds of 4 unique values of conspecific densities and a minimum range of 1 for conspecific as our thresholds for what constitutes a data deficient species.\n\n\nCode\nnval = 4  ## this is the number of 'unique' conspecific values \nminrange = 1    # minimum range for conspecific density\n\ndata_BCI %&gt;% \n  group_by(spp) %&gt;% \n  summarise(\n            range_con_dens = max(con_dens) - min(con_dens),\n            max_con_dens = max(con_dens),\n            unique_con_dens = length(unique(con_dens)),\n            unique_total_dens = length(unique(total_dens)),\n            unique_height = length(unique(height.last.census))\n  ) %&gt;% \n  \n  # check if conspecific less than defined nval\n  mutate(issue_nval = unique_con_dens &lt; nval,\n  # range should at least be equal to minrange\n  issue_range = range_con_dens &lt; minrange,                 \n  trymodel = !(issue_nval|issue_range),\n  # Assignment of \"data deficient\" species\n  data_deficient = !trymodel                                          \n  ) -&gt; nsp # Store the resulting dataframe in the object 'nsp'\n\n# Visualize the top rows of the table 'nsp' in a formatted manner\nhead(nsp)\n\n\n# A tibble: 6 × 10\n  spp    range_con_dens max_con_dens unique_con_dens unique_total_dens\n  &lt;fct&gt;           &lt;dbl&gt;        &lt;dbl&gt;           &lt;int&gt;             &lt;int&gt;\n1 ACALDI              7            7               8                35\n2 AEGICE              4            4               5                28\n3 BEILPE             98           98              69                72\n4 CAPPFR              5            5               6                52\n5 CECRIN              7            7               7                20\n6 CORDLA              4            4               5                32\n# ℹ 5 more variables: unique_height &lt;int&gt;, issue_nval &lt;lgl&gt;, issue_range &lt;lgl&gt;,\n#   trymodel &lt;lgl&gt;, data_deficient &lt;lgl&gt;"
  },
  {
    "objectID": "calculating-marginal-effects.html#function-for-fitting-models",
    "href": "calculating-marginal-effects.html#function-for-fitting-models",
    "title": "3  Calculating Marginal Effects",
    "section": "3.5 Function for fitting models",
    "text": "3.5 Function for fitting models\nIn the context of the Janzen-Connell Hypothesis, we focus on the differences between CDD and HDD (‘Stabilizing effect’), where conspecific neighbors’ negative effects surpass those from heterospecifics, leading to population stabilization (Broekman et al. 2019). This effect is vital for estimating a species’ self-limitation.\nIn this section we quantify the conspecific density impact, using a model with conspecific density and total density. By using total density in the model instead of heterospecific density, the estimated effect (slope) of conspecific density in our analysis corresponds to the result of subtracting HDD from CDD (Hülsmann et al. 2024).\nWe use here a Generalized Additive Model (GAM) with a complementary log-log (cloglog) link function to model the seedling status (‘alive’ or ‘dead’) as a function of conspecific density con_dens, total density total_dens and tree height or size of the focal individual (e.g., height or dbh), the latter serving as a covariate.\nThe cloglog link allows accounting for differences in observation time Δ𝑡 through an offset term, applicable to datasets where (0=alive and 1=dead)(Currie 2016).In other words, to use a cloglog link to account for differences in census interval length, you must be modeling mortality, not survival. (more about cloglog link here - section 3.2)\nNext, we specify the smooth term in the model formula. The k value determines the complexity of the smooth. If k exceeds the number of unique values in a variable, it’s adjusted to be two less than that number. We also monitor model convergence and any warnings that may arise. With this setup, we establish ‘k=2’ as the minimum value since ‘k=1’ results in a linear relationship. Hence, we defined in the previous section a minimum of 4 unique values for conspecific densities as threshold, as setting a threshold of 3 would enforce linearity in the model.\n\n\nCode\n# Define a function to fit a model to the given data\nmodel_fit = function(data, spinfo, reduced = F) {\n  \n  # create new factor with correct factor levels per species \n  data$census = factor(data$census)\n  \n  \n  # # Determine if there's more than one unique value in 'census' \n  # and construct the relevant term for the model formula\n  \n  term_c = ifelse(length(unique(data$census)) &gt; 1, \n                  \"+ s(census, bs = 're')\", \"\") \n  #term_p = \"+ s(plot, bs = 're')\"\n  \n  if (reduced) {\n    form =  as.formula(paste0(\"status ~ s(height, k = k1) + \n                              s(total_dens, k = k2)\"\n                              , term_c)) # reduced model #,term_p\n  } else {\n    form =  as.formula(paste0(\"status ~ s(height, k = k1) + \n                              s(total_dens, k = k2) + \n                              s(con_dens, k = k3)\" \n                              , term_c)) # full model #, term_p\n  }\n  \n  # Choose penalty\n  # set to default k=10 \n  k1 = k2 = k3 = 10\n  if (k1 &gt; spinfo$unique_height) k1 = spinfo$unique_height - 2\n  if (k2 &gt; spinfo$unique_total_dens) k2 = spinfo$unique_total_dens - 2\n  if (k3 &gt; spinfo$unique_con_dens) k3 = spinfo$unique_con_dens - 2\n  \n  # k = 1 would force linearity for that term, and we aim to consider \n  # also potential non-linear relationships, so conspecific k is k=2 \n  # minimum.\n  \n # Fit the Generalized Additive Model (GAM)\n  mod = try(gam(form\n                , family = binomial(link=cloglog)\n                , offset = log(interval)\n                , data = data\n                , method = \"REML\"\n  ) , silent = T\n  )\n   # Return the fitted model\n  return(mod)\n  \n}\n\n# check model run\n\n# Define a function to check the convergence of the model\nmodel_convergence = function(model) {\n  \n  # gam not available\n  if (!any(class(model)==\"gam\")) {\n    print(paste(spp, \"gam failed\"))\n  } else {\n    \n    # gam not converged\n    if (!model$converged) {\n      print(paste(spp, \"no convergence\"))\n    } else {\n      \n    \n# Explore warning \"glm.fit: fitted probabilities numerically 0 or 1 \n      # occurred (complete separation)\"\n      eps &lt;- 10 * .Machine$double.eps\n      glm0.resids &lt;- augment(x = model) %&gt;%\n        mutate(p = 1 / (1 + exp(-.fitted)),\n               warning = p &gt; 1-eps,\n               influence = order(.hat, decreasing = T))\n      infl_limit = round(nrow(glm0.resids)/10, 0)\n     \n       # Check if any of the warnings correspond to the 10% most \n      # influential observations. If not, then it is okay.\n\n      num = any(glm0.resids$warning & glm0.resids$influence &lt; \n                  infl_limit)\n      \n    # If there's complete separation\n      if (num) {\n        print(paste(spp, \"complete separation is likely\"))\n      } else {\n        \n       # Check if the Vc component of the model is missing\n        if (is.null(model$Vc)) {\n          print(paste(spp, \"Vc not available\"))\n        } else {\n        \n        # If everything is fine, return the model\n        return(model)\n        }\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "calculating-marginal-effects.html#fit-models",
    "href": "calculating-marginal-effects.html#fit-models",
    "title": "3  Calculating Marginal Effects",
    "section": "3.6 Fit models",
    "text": "3.6 Fit models\nHere we fit the models for all species. Data deficient species are treated as one group since there is not sufficient data to be treated independently. In this dataset, we have 9 species that are flagged as ‘data deficient’,\n\n\nCode\n# Check the distribution of species marked as 'data deficient' \n# (T or F) \n# and determine the number of species for which we will try the model \n\n# We have 9 species that are flagged as 'data deficient'\ntable(data_deficient = nsp$data_deficient, trymodel = nsp$trymodel) \n\n\n              trymodel\ndata_deficient FALSE TRUE\n         FALSE     0   21\n         TRUE      9    0\n\n\nCode\n# Convert spp to character\ndata_BCI$spp &lt;- as.character(data_BCI$spp)\n\n# Extract species that are flagged as 'data deficient' from the nsp \n# dataframe\ndata_deficient_species &lt;- nsp$spp[nsp$data_deficient == TRUE]\n\n# Replace species names with \"data_deficient_seedling\" for those \n# flagged as 'data_deficient'\ndata_BCI2 &lt;- data_BCI %&gt;% \n  mutate(spp = ifelse(spp %in% data_deficient_species, \n                      \"data_deficient_seedling\", spp))\n\n# # Summarize attributes for each species in the modified dataframe \n# (including the new #\"data_deficient_seedling\" group)\n\ndata_BCI2 %&gt;%\n  group_by(spp) %&gt;%\n  summarise(\n    range_con_dens = max(con_dens, na.rm = TRUE) - min(con_dens, \n                                                       na.rm = TRUE),\n    max_con_dens = max(con_dens, na.rm = TRUE),\n    unique_con_dens = length(unique(con_dens)),\n    unique_total_dens = length(unique(total_dens)),\n    unique_height = length(unique(height.last.census))\n  ) %&gt;%\n  mutate(\n    # less than nval unique values in consp densities\n    issue_nval = unique_con_dens &lt; nval, \n    # range should at least be equal to minrange\n    issue_range = range_con_dens &lt; minrange,              \n    trymodel = !(issue_nval | issue_range),\n    # preliminary assignment of data_deficient species\n    data_deficient = !trymodel                                      \n  ) -&gt; nsp_data_deficient\n\n\n####\n## Fit model for each species\n###\n\n# Create lists to store results of the main and reduced model fits\n\n# List for main model fits\nres_mod = list()      \n# List for reduced model fits (for calculating Pseudo R2)\nres_red_mod = list()  \n\n\n# Loop through species in the nsp_data_deficient dataframe for which \n# we will try modeling \n# (Here, the group of data_deficient species is treated as a single \n# species)\n  \n  for(spp in nsp_data_deficient$spp[nsp_data_deficient$trymodel]) {\n  \n  # select data for individual species\n  dat_sp = data_BCI2[data_BCI2$spp == spp, ]\n  \n  # Fit the main and reduced models for the current species\n  mod = model_fit(data = dat_sp, \n                  spinfo = nsp_data_deficient[nsp_data_deficient$spp \n                                              == spp, ])\n  mod_red = model_fit(data = dat_sp, \n                  spinfo = nsp_data_deficient[nsp_data_deficient$spp \n                                              == spp, ], reduced = T)\n  \n  # Check the convergence of both models\n  res = model_convergence(model = mod)\n  res_red = model_convergence(model = mod_red)\n  \n  # save result\n  if (is.character(res)) {\n    nsp$data_deficient[nsp$spp == spp] = T  \n  } else {\n    res_mod[[spp]] = res\n    res_red_mod[[spp]] = res_red\n  }\n}"
  },
  {
    "objectID": "calculating-marginal-effects.html#summarize-model-fits",
    "href": "calculating-marginal-effects.html#summarize-model-fits",
    "title": "3  Calculating Marginal Effects",
    "section": "3.7 Summarize model fits",
    "text": "3.7 Summarize model fits\nRegression table via broom::tidy()\n\n\nCode\ncoefs = lapply(res_mod, broom::tidy)\ncoefs = Map(cbind, coefs, sp = names(coefs))\ncoefs = do.call(rbind, coefs)\n\n\nModel summary via broom::glance()\n\n\nCode\n# For each model result in 'res_mod', extract summary statistics \n# (like log-likelihood, AIC, BIC #, etc.) using the 'glance' function\n# from the 'broom' package\n\n# df logLik AIC BIC deviance df.residuals nobs \nsums = lapply(res_mod, broom::glance)\nsums = Map(cbind, sums, sp = names(sums))\nsums = do.call(rbind, sums)\nhead(sums)\n\n\n              df      logLik        AIC        BIC   deviance df.residual  nobs\nACALDI  7.738186   -428.3495   876.0896   924.9334   856.6990   1131.2618  1139\nAEGICE  8.763874   -524.7678  1070.5298  1123.3674  1049.5355   1125.2361  1134\nBEILPE 17.776666 -11142.3710 22323.8580 22478.1537 22284.7420  19697.2233 19715\nCAPPFR 10.232458  -2024.4980  4075.8754  4174.3285  4048.9960  11210.7675 11221\nCECRIN  6.221103   -143.4318   301.0402   326.2521   286.8635    252.7789   259\nCORDLA  6.840230   -531.0591  1079.4468  1125.3893  1062.1182   1477.1598  1484\n       adj.r.squared npar     sp\nACALDI   0.019973930   37 ACALDI\nAEGICE   0.049305090   34 AEGICE\nBEILPE   0.069072474   41 BEILPE\nCAPPFR   0.008391332   35 CAPPFR\nCECRIN   0.118798167   26 CECRIN\nCORDLA   0.015528602   34 CORDLA\n\n\nCode\n# AUC\naucs = lapply(res_mod, function(x) {\n  roc &lt;- performance::performance_roc(x, new_data = x$model)\n  bayestestR::area_under_curve(roc$Spec, roc$Sens)\n})\nsums$AUC = unlist(aucs)\n\n\n# Pseudo R2\nsums$pseudoR2 = 1 - (unlist(lapply(res_mod, function(x) x$deviance)) /\n                       unlist(lapply(res_red_mod, function(x) \n                         x$deviance)))"
  },
  {
    "objectID": "calculating-marginal-effects.html#plotting-results",
    "href": "calculating-marginal-effects.html#plotting-results",
    "title": "3  Calculating Marginal Effects",
    "section": "3.8 Plotting results",
    "text": "3.8 Plotting results\n\n\nCode\n# plot splines in pdf ----------------------------------------------\n\n# # Specify the name of the PDF file where the plots will be saved\npdf_file &lt;- \"mortality.pdf\"\n\n# Open the PDF file for writing\npdf(pdf_file)\n\n# Loop through all the model results stored in 'res_mod'\nfor (i in 1:length(res_mod)) {\n  \n  # Get the vizmod for the current species\n  vizmod &lt;- getViz(res_mod[[i]], post = T, unconditional = T)\n  pl &lt;- plot(vizmod, nsim = 20, allTerms = T) + \n    # Add confidence interval line/ fit line/ simulation line\n    l_ciLine() + l_fitLine() + l_simLine() +\n     #Add confidence interval bar/# Add fitted points\n    l_ciBar() + l_fitPoints(size = 1) +  \n    l_rug() +                             # Add rug plot\n    # Add title with the name of the current species\n    labs(title = names(res_mod)[i])       \n  \n  # Print the plot to the R console only for the first 2 species\n  if (i &lt;= 2) {\n    print(pl, pages = 1)\n  }\n\n}\n\n# Close the PDF file\ndev.off()\n\n\nquartz_off_screen \n                2"
  },
  {
    "objectID": "calculating-marginal-effects.html#ames-absolute-and-relative",
    "href": "calculating-marginal-effects.html#ames-absolute-and-relative",
    "title": "3  Calculating Marginal Effects",
    "section": "3.9 AMEs Absolute and Relative",
    "text": "3.9 AMEs Absolute and Relative\nIn this section we illustrate the calculation of the average marginal effects, including both the absolute Average Marginal Effect (aAME) and the relative Average Marginal Effect (rAME).\nHere, AMEs are computed by determining the marginal effect (essentially the partial derivative or slope) of a predictor for a unit change in conspecific density at each observed value, and then averaging these effects. This method yields a single, interpretable measure that offers an averaged estimate of the predictor’s impact. It’s worth noting that the AME, compared to traditional effect sizes, provides a clearer measure of a predictor’s influence by quantifying the average change in the outcome for each unit change in the predictor rather than the raw coefficient (effect size) that may be influenced by the scale of the variable or confounded by interactions with other predictors. In the analyses illustrated here, we are interested in calculating the average marginal effect of conspecific density on mortality.\nFurthermore, rAMEs enhance the level of interpretability by delivering a normalized measure of these averaged effects. They represent the percentage change in the response variable due to a unit change in the predictor relative to the base mortality, providing an intuitive, relative grasp of the predictor’s influence. This normalization process allows for the comparison of effects across different species, each with its own base level of mortality.\nTo calculate aAMEs and rAMEs we need the file “res_model” that includes all the models results.\nThere is also alternative approaches for calculating average marginal effects using ‘marginaleffects’ package that we encourage the user to explore.\n\n3.9.1 Settings for AMEs\nHere we provide three scenarios for calculating aAMEs or rAMEs through the change argument.\nEquilibrium: This scenario computes aAMEs or rAMEs for a unit increase in conspecific density of above observed values, providing insight into the marginal effect of density increase in an existing ecosystem. Invasion: This scenario models the effects of introducing species into a new community in which it did not previously occur, transitioning the conspecific density from zero to a specified unit, helping understand the impact of sudden species introductions linking to theoretical considerations from coexistence theory (Chesson 2000). IQR: This scenario evaluates aAMEs or rAMEs within the middle 50% range of conspecific density, offering a perspective on the ecological relevance of CDD within typical density ranges, however without being comparable among species.\n\n\nCode\n#### chose predictors for local density -setting AMEs\n  \n\n# Define a vector of predictors with their corresponding names\npredictors &lt;- c(con_dens = \"con_dens\", \n                total_dens = \"total_dens\")\n\n# change in conspecific density for AME calculations----\n\n# One more neighbor seedling (not for adult trees)\n\nadditive=1     \n  \n# set interval to 1 for annual mortality probabilities\n\ninterval = 1\n\n# Specify how the conspecific density should be changed for \n# different scenarios: \n# 'equilibrium', 'invasion', and 'iqr' (interquartile range)\n\nchange = list(equilibrium = data.frame(con_dens = \n                                         \"paste('+', additive)\")\n              , invasion = data.frame(con_dens = \"c(0, additive)\") \n              , iqr = data.frame(con_dens = \"c(q1, q3)\")\n              )\n\n## Set the number of iterations for any subsequent calculations or \n# simulations\niter = 500 \n\n\n\n\n3.9.2 Functions to calculate aAMEs and rAME\nThis section will write and define two functions. The setstep function and get_AME function. Get_AME is a function to calculate the Average Marginal Effects for a given term in a model. The function first creates two copies of the data, d0 and d1, and adjusts the term of interest based on the change argument (one of the three scenarios described in the previous section). It then calculates the predictions for the two data sets and computes the marginal effects.\n\n\nCode\n# Define a function to set the step size for numerical derivatives\n# This function determines an appropriate step size using machine's \n# precision/machine epsilon to strike a balance between accuracy and \n# rounding errors.\n\nsetstep = function(x) {\n  eps = .Machine$double.eps\n  return(x + (max(abs(x), 1, na.rm = TRUE) * sqrt(eps)) - x)\n}\n\n\n# Function to compute absolute and relative average marginal effects \n# (AME and rAME) for a given model--------------------\n\nget_AME = function(mod, data, term\n                   , change = NULL\n                   , at = NULL\n                   , offset = 1\n                   , relative = F\n                   , iterations = 1000\n                   , seed = 10\n                   , samples = F) {\n  \n  # Prepare two dataframes for different scenarios in marginal \n  # effect computation\n  d0 = d1 = data\n  \n # Adjust the 'term' in the data based on the 'change' parameter\n  if (is.null(change)) {\n    \n    # If change is NULL, adjust the term for numerical derivative \n    # computation\n    d0[[term]] = d0[[term]] - setstep(d0[[term]])\n    d1[[term]] = d1[[term]] + setstep(d1[[term]])\n    \n  } \n  \n  # If change has an additive component, adjust the term accordingly\n  if (grepl(\"\\\\+\", paste(change, collapse = \"_\"))) {\n    \n    d1[[term]] = d1[[term]] + as.numeric(gsub(\"\\\\+\", \"\", change))\n    \n  } \n  \n  # If change is explicit with two values, set the term values \n  # directly\n  if (length(change) == 2) {\n    \n    d0[[term]] = as.numeric(change[1])\n    d1[[term]] = as.numeric(change[2])\n    \n  }\n  \n   # If 'at' is specified, set predictor values in the data to these \n  # fixed values\n  # (allows the function to calculate the marginal effects at the \n  # specified values)\n  if (!is.null(at)) {\n    for (i in names(at))\n      d0[[i]] = at[[i]]\n      d1[[i]] = at[[i]]\n  }\n  \n   # Create matrices for prediction based on the model\n  Xp0 &lt;- predict(mod, newdata = d0, type=\"lpmatrix\")\n  Xp1 &lt;- predict(mod, newdata = d1, type=\"lpmatrix\")\n  \n # Extract model parameters\n  ilink &lt;- family(mod)$linkinv\n  beta &lt;- coef(mod)\n  vc &lt;- mod$Vc # covariance matrix \n  \n\n # Compute marginal effects based on the adjusted data\n  pred0   &lt;- 1 - (1-ilink(Xp0 %*% beta))^offset\n  pred1   &lt;- 1 - (1-ilink(Xp1 %*% beta))^offset\n  ME &lt;- (pred1-pred0)\n  \n # Adjust for numerical derivative if change is NULL\n  if (is.null(change)) {\n    ME &lt;- ME/(d1[[term]] - d0[[term]])\n  } \n  \n  # convert to relative if requested\n  if (relative == T) ME = ME/pred0\n  \n  # average marginal effect\n  AME = mean(ME)\n  \n  \n  # Simulate AMEs to compute uncertainty in the estimates\n  \n   # Compute the variance of the average marginal effect through a \n   # \"posterior\" simulation.\n   # This involves simulating from a multivariate normal distribution \n   # using the model's  \n   #coefficient means and covariance matrix\n  \n  if (!is.null(seed)) set.seed(seed)\n  coefmat = mvrnorm(n = iterations\n                    , mu = beta\n                    , Sigma = vc)\n  \n    # For each simulated coefficient vector, estimate the Average \n    # Marginal Effect (AME).\n  AMEs = apply(coefmat, 1, function(coefrow) {\n    \n    # Calculate marginal effects based on the simulated coefficient\n    pred0   &lt;- 1 - (1-ilink(Xp0 %*% coefrow))^offset\n    pred1   &lt;- 1 - (1-ilink(Xp1 %*% coefrow))^offset\n    ME &lt;- (pred1-pred0)\n    \n    # if change is NULL, use numerical derivative\n    if (is.null(change)) {\n      ME &lt;- ME/(d1[[term]] - d0[[term]])\n    } \n    \n    # convert to relative if requested\n    if (relative == T) ME = ME/pred0\n    \n    # average marginal effect\n    AME = mean(ME)\n    return(AME)\n  })\n  \n  # Combine results\n   # If the 'samples' flag is FALSE, return the summary results.\n  # Otherwise, return both the summary and the sample results.\n  \n  if (!samples) {\n    res = data.frame(term\n                     , estimate = AME\n                     , std.error = sqrt(var(AMEs))  \n                     , estimate.sim = mean(AMEs)    \n                     , offset\n                     , change.value = paste(change, collapse = \"_\"))\n    return(res) \n    \n  } else {\n    \n    res_sums = data.frame(term\n                     , estimate = AME\n                     , std.error = sqrt(var(AMEs)) \n                     , offset\n                     , change.value = paste(change, collapse = \"_\"))\n    \n    res_samples = data.frame(term\n                             , estimate = AMEs\n                             , MLE = AME\n                             , offset\n                             , change.value = paste(change, \n                                                    collapse = \"_\"))\n    res = list(res_sums, res_samples)\n    return(res)  \n    \n  }\n}\n\n\n\n\n3.9.3 Calculating Absolute Average Marginal Effect (aAMEs)\nAt the end of this code segment, the aAME data frame contains average marginal estimates for each predictor, each corresponding to different change scenarios. In essence, aAME provides the average effect that a predictor has on the outcome across these scenarios. On the other hand, the aAMEsamples data frame contains multiple samples of aAME for each predictor, each aligned with a specific type of change scenario, which allows for an evaluation of the uncertainty inherent in the aAME estimates.\n\n\nCode\n# Absolute AMEs ---------------------------------------------------\n\n# Initialize empty data frames to store the results\naAME = data.frame()\naAMEsamples = data.frame()\n\n# Loop through predictor names that match \"con_\"\nfor (i in names(predictors)[grepl(\"con_\", names(predictors))]) { \n\n# Loop through different change settings (e.g., equilibrium, invasion,\n  # iqr)\n  for (j in names(change)) {\n    \n# Calculate the AME for each model in res_mod\n    temp = lapply(res_mod, function(x){\n      \n# If the change is based on IQR (interquartile range), calculate the\n      # 1st and 3rd quartiles\n      if (j == \"iqr\") {\n        q1 = quantile(x$model$con_dens, probs = 0.25)\n        q3 = quantile(x$model$con_dens, probs = 0.75)\n      }\n\n# Use the get_AME function to calculate the AME for the current model\n      get_AME(x\n              , data = x$model\n              , offset = interval\n              , term = i\n              , change = eval(parse(text = change[[j]][,i]))\n              , iterations = iter\n              , samples = T\n      )\n    }\n    )\n    \n    # AME\n    tempAME = lapply(temp, function(x) x[[1]])\n    tempAME = Map(cbind, tempAME, change = j, sp = names(tempAME))\n    tempAME = do.call(rbind, tempAME)\n    aAME = rbind(aAME, tempAME)\n    \n    # AME samples\n    tempSamples = lapply(temp, function(x) x[[2]])\n    tempSamples = Map(cbind, tempSamples, change = j, \n                      sp = names(tempSamples), iter = iter)\n    tempSamples = do.call(rbind, tempSamples)\n    aAMEsamples = rbind(aAMEsamples, tempSamples)\n  }\n}\nhead(aAME)\n\n\n           term    estimate   std.error offset change.value      change     sp\nACALDI con_dens 0.017532523 0.016062221      1          + 1 equilibrium ACALDI\nAEGICE con_dens 0.089367793 0.025499132      1          + 1 equilibrium AEGICE\nBEILPE con_dens 0.006043419 0.001063042      1          + 1 equilibrium BEILPE\nCAPPFR con_dens 0.020944544 0.004140303      1          + 1 equilibrium CAPPFR\nCECRIN con_dens 0.028621858 0.027033954      1          + 1 equilibrium CECRIN\nCORDLA con_dens 0.013464775 0.030698485      1          + 1 equilibrium CORDLA\n\n\n\n\n3.9.4 Calculating Relative Average Marginal Effect (rAMEs)\nAt the end of this code segment, the rAME data frame contains therAME estimates for the predictor and each type of change, and the rAMEsamples data frame contains the rAME samples for each predictor and type of change.\n\n\nCode\n# Relative rAMEs -----------------------------------------------------------\n\n\n# Initialize empty data frames to store the results\nrAME = data.frame()\nrAMEsamples = data.frame()\n\n# Loop through predictor names that match \"con_\" \nfor (i in names(predictors)[grepl(\"con_\", names(predictors))]) { \n  \n# Loop through different change settings \n  # (e.g., equilibrium, invasion, iqr)\n  for (j in names(change)) {\n\n# Calculate the relative AME (rAME) for each model in res_mod\n    temp = lapply(res_mod, function(x){\n      \n# If the change is based on IQR (interquartile range), \n  # calculate the 1st and 3rd quartiles\n      if (j == \"iqr\") {\n        q1 = quantile(x$model$con_dens, probs = 0.25)\n        q3 = quantile(x$model$con_dens, probs = 0.75)\n      }\n# Use the get_AME function to calculate the rAME for the \n  # current model, setting the 'relative' argument to TRUE\n      get_AME(x\n              , data = x$model\n              , offset = interval\n              , term = i\n              , change = eval(parse(text = change[[j]][, i]))\n              , iterations = iter\n              , relative = T\n              , samples = T\n      )\n    }\n    )\n    \n    # rAME\n    tempAME = lapply(temp, function(x) x[[1]])\n    tempAME = Map(cbind, tempAME, change = j, sp = names(tempAME))\n    tempAME = do.call(rbind, tempAME)\n    rAME = rbind(rAME, tempAME)\n    \n    # rAME samples\n    tempSamples = lapply(temp, function(x) x[[2]])\n    tempSamples = Map(cbind, tempSamples, change = j, \n                      sp = names(tempSamples), iter = iter)\n    tempSamples = do.call(rbind, tempSamples)\n    rAMEsamples = rbind(rAMEsamples, tempSamples)\n  }\n}\nhead(rAME)\n\n\n           term   estimate  std.error offset change.value      change     sp\nACALDI con_dens 0.12190548 0.11757479      1          + 1 equilibrium ACALDI\nAEGICE con_dens 0.46332386 0.13830487      1          + 1 equilibrium AEGICE\nBEILPE con_dens 0.02026319 0.00362583      1          + 1 equilibrium BEILPE\nCAPPFR con_dens 0.42931069 0.08161156      1          + 1 equilibrium CAPPFR\nCECRIN con_dens 0.04121409 0.04242789      1          + 1 equilibrium CECRIN\nCORDLA con_dens 0.10440629 0.24200786      1          + 1 equilibrium CORDLA"
  },
  {
    "objectID": "calculating-marginal-effects.html#saving-results",
    "href": "calculating-marginal-effects.html#saving-results",
    "title": "3  Calculating Marginal Effects",
    "section": "3.10 Saving results",
    "text": "3.10 Saving results\n\n\nCode\n# Save results ---------------------------------------------------\nsave(list = c(\"aAME\", \"aAMEsamples\", \"rAME\", \"rAMEsamples\", \"nsp\", \n              \"coefs\", \"sums\") # \"nsp_data_deficient\"\n     , file = paste0( \"./data/mortality.Rdata\"))\nwrite.csv(aAME, \"./data/aAME.csv\")\nwrite.csv(rAME, \"./data/rAME.csv\")\n\n\n\n\n\n\n\n\nBroekman, Maarten JE, Helene C Muller-Landau, Marco D Visser, Eelke Jongejans, SJ Wright, and Hans de Kroon. 2019. “Signs of Stabilisation and Stable Coexistence.” Ecology Letters 22 (11): 1957–75.\n\n\nChesson, Peter. 2000. “Mechanisms of Maintenance of Species Diversity.” Annual Review of Ecology and Systematics 31 (1): 343–66.\n\n\nCurrie, Iain D. 2016. “On Fitting Generalized Linear and Non-Linear Models of Mortality.” Scandinavian Actuarial Journal 2016 (4): 356–83.\n\n\nHülsmann, Lisa, Ryan A Chisholm, Liza Comita, Marco D Visser, Melina de Souza Leite, Salomon Aguilar, Kristina J Anderson-Teixeira, et al. 2024. “Latitudinal Patterns in Stabilizing Density Dependence of Forest Communities.” Nature 627 (8004): 564–71."
  },
  {
    "objectID": "metaregression-comparisons.html#overview",
    "href": "metaregression-comparisons.html#overview",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nFollowing from Section 4. How does CDD vary across species, abiotic gradients or in time? in the main text of the article, here, we demonstrate one approach to comparing the strength of CDD across species using a meta-analysis framework. The same approach can be used to compare CDD across sites, plots, or any other unit of interest, as long as it is possible to generate reliable estimates of the strength of CDD (suitable sample sizes, etc.). As noted in the main text, “Correct propagation of uncertainty in CDD estimates requires meta-regressions in frequentist or Bayesian frameworks. Weighted regressions (e.g., lm, lmer, gam) can also estimate how CDD varies e.g., across latitude or between species with different life-history strategies, but incorrectly estimate the associated uncertainty.”\nWe use a subset of the BCI seedling data (Comita et al. 2023) of only 30 species to compare how species abundance is related to the strength of CDD at the species level. This is the same dataset used in section 2. This analysis is for demonstration purposes only, and biological conclusions should not be made about the results, given this is only a small subset of the data.\nAn advantage of using meta-regressions over simple weighted regressions is that the models are able to simultaneously account for uncertainty in species-specific CDD estimates as well as systematic differences in species’ CDD when regressed against a predictor via the inclusion of random effects. A simple weighted regression, on the other hand, assumes that there is only one error for which the relative strength is known when regressing the estimates against a predictor. The latter can lead to incorrect weighting of species in the metaregression.\nIn this tutorial, we use relative average marginal effect (rAME) calculated in the previous chapter as our response variable, calculated separately for each species. rAME in this case estimates the relative increase in the probability of annual mortality with the addition of one new conspecific neighbor, while keeping total densities constant. Positive numbers indicate a relative increase in mortality with an increase in conspecific density, a signature of negative CDD. In principle, any metric of the strength of CDD can be used, though care must be taken to ensure that the metrics are comparable across species and sites (see main text for more information).\nWe use the popular metafor package to fit the meta-regression models.\n\n\n\n\n\n\nNote\n\n\n\nThe following code is adapted from the latitudinalCNDD repository by Lisa Hülsmann as demonstrated in the publication Hülsmann et al. (2024)."
  },
  {
    "objectID": "metaregression-comparisons.html#load-libraries-and-data",
    "href": "metaregression-comparisons.html#load-libraries-and-data",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.2 Load libraries and data",
    "text": "4.2 Load libraries and data\n\n\nCode\n# Load libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(metafor)\nlibrary(sessioninfo)\n\n# Load in species abundances for BCI data subset\nabundances &lt;- read_csv(\nhere(\"./data/BCI seedling data - 30 species - abundance 2023_05_18.csv\")\n  )\n\n# Load marginal effects calculations from previous section\nload(here(\"./data/mortality.Rdata\"))\n\n# Subset down to just equilibrium change\nrAME &lt;- rAME %&gt;%\n  filter(change == \"equilibrium\")\n\n# Join marginal effects and abundance data\nrAME &lt;- left_join(rAME, abundances, by = c(\"sp\" = \"spp\"))\n\n# Add in average abundance for 'rare' species\nrAME$abundance[rAME$sp == \"data_deficient_seedling\"] &lt;- \n  mean(abundances$abundance[abundances$spp %in% \n                              nsp$spp[nsp$data_deficient]])\n\n# Log transform abundance to use in models\nrAME$log_abundance &lt;- log(rAME$abundance)\n\n\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\nhead(rAME, n = 10)\n\n\n       term    estimate  std.error offset change.value      change     sp\n1  con_dens  0.12190548 0.11757479      1          + 1 equilibrium ACALDI\n2  con_dens  0.46332386 0.13830487      1          + 1 equilibrium AEGICE\n3  con_dens  0.02026319 0.00362583      1          + 1 equilibrium BEILPE\n4  con_dens  0.42931069 0.08161156      1          + 1 equilibrium CAPPFR\n5  con_dens  0.04121409 0.04242789      1          + 1 equilibrium CECRIN\n6  con_dens  0.10440629 0.24200786      1          + 1 equilibrium CORDLA\n7  con_dens  0.11658356 0.20036851      1          + 1 equilibrium DAVINI\n8  con_dens  0.01113153 0.12611629      1          + 1 equilibrium DESMAX\n9  con_dens  0.23375216 0.22766635      1          + 1 equilibrium HEISCO\n10 con_dens -0.06498043 0.06101709      1          + 1 equilibrium JUSTGR\n   abundance log_abundance\n1        169      5.129899\n2        153      5.030438\n3       5574      8.625868\n4       1604      7.380256\n5         51      3.931826\n6        248      5.513429\n7         27      3.295837\n8         55      4.007333\n9         56      4.025352\n10       136      4.912655"
  },
  {
    "objectID": "metaregression-comparisons.html#reformat-data-for-model-fitting",
    "href": "metaregression-comparisons.html#reformat-data-for-model-fitting",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.3 Reformat data for model fitting",
    "text": "4.3 Reformat data for model fitting\nFirst, we use the ‘escalc’ function in the metafor package to essentially repackage our data frame into a format used in the meta-regression model fitting. Since we already calculated our effect size (rAME), we just pass through the 𝑟𝐴𝑀𝐸 estimate and corresponding standard error using the ‘GEN’ option for the ‘measure’ argument, rather than calculating an effect size within the ‘escalc’ function.\nIn some cases, based upon inspecting the resulting residuals from the meta-regression model, it may be a good idea to transform rAME estimates prior to meta-regression model fitting.\n\n\nCode\n# Reformat data for model fitting\n# Set measure to generic, which passes the observed effect sizes or \n# outcomes via the yi argument and the corresponding sampling \n# variances via the vi argument (or the standard errors via the sei \n# argument) to the function.\n    dat_meta = metafor::escalc(measure = \"GEN\", \n                               yi = estimate, # observed outcomes\n                               sei = std.error, # standard errors\n                               slab = sp, # label for species\n                               data = rAME)"
  },
  {
    "objectID": "metaregression-comparisons.html#fit-meta-regression-model",
    "href": "metaregression-comparisons.html#fit-meta-regression-model",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.4 Fit meta-regression model",
    "text": "4.4 Fit meta-regression model\nNext, we use the ‘rma’ function to fit a meta-regression model, where rAME is our response variable (renamed as yi in the previous step) and log species abundance is our predictor. While not shown here, it is possible to fit more complicated mixed effects meta-regression models with the ‘rma.mv’ function. We suggest consulting the extensive documentation for the metafor package for further details.\n\n\nCode\n# Fit model\nmetamod = metafor::rma(yi = yi,\n                       vi = vi,\n                       mods = ~ log_abundance,\n                       method = \"REML\",\n                       data = dat_meta)"
  },
  {
    "objectID": "metaregression-comparisons.html#print-model-summary",
    "href": "metaregression-comparisons.html#print-model-summary",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.5 Print model summary",
    "text": "4.5 Print model summary\n\n\nCode\nsummary(metamod)\n\n\n\nMixed-Effects Model (k = 22; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  2.2040   -4.4080    1.5920    4.5792    3.0920   \n\ntau^2 (estimated amount of residual heterogeneity):     0.0173 (SE = 0.0093)\ntau (square root of estimated tau^2 value):             0.1317\nI^2 (residual heterogeneity / unaccounted variability): 91.88%\nH^2 (unaccounted variability / sampling variability):   12.32\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 20) = 77.6457, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nQM(df = 1) = 0.3461, p-val = 0.5563\n\nModel Results:\n\n               estimate      se     zval    pval    ci.lb   ci.ub    \nintrcpt          0.1934  0.1422   1.3599  0.1739  -0.0853  0.4721    \nlog_abundance   -0.0138  0.0235  -0.5883  0.5563  -0.0598  0.0322    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "metaregression-comparisons.html#plot-the-estimated-rame-for-all-species-with-a-forest-plot",
    "href": "metaregression-comparisons.html#plot-the-estimated-rame-for-all-species-with-a-forest-plot",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.6 Plot the estimated rAME for all species with a forest plot",
    "text": "4.6 Plot the estimated rAME for all species with a forest plot\nIn this case, a forest plot shows the estimates of the strength of CDD for individual species, here ordered by least to most abundant going from top to bottom.\n\n\nCode\nforest(metamod, \n       header = \"Species\", \n       xlab = \"rAME\",\n       order = log_abundance)"
  },
  {
    "objectID": "metaregression-comparisons.html#model-diagnostics",
    "href": "metaregression-comparisons.html#model-diagnostics",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.7 Model diagnostics",
    "text": "4.7 Model diagnostics\nThe plot method displays model diagnostics of the meta-regression model (more info here) in addition to a forest plot.\n\n\nCode\nplot(metamod)"
  },
  {
    "objectID": "metaregression-comparisons.html#model-predictions-of-how-species-abundance-is-related-to-strength-of-cdd",
    "href": "metaregression-comparisons.html#model-predictions-of-how-species-abundance-is-related-to-strength-of-cdd",
    "title": "4  Using meta-regressions to compare CDD across species or sites",
    "section": "4.8 Model predictions of how species abundance is related to strength of CDD",
    "text": "4.8 Model predictions of how species abundance is related to strength of CDD\nHere, we generate predictions and corresponding confidence intervals for how our predictor of interest is related to the related to strength of CDD using the ‘predict’ function. The y-axis here indicates the relative increase in annual mortality probablity with the addition of one conspecific neighbor. Higher values indicate stronger negative conspecific density dependence. In this example, our predictor of interest is species abundance. We also scale the size of the points based on their weight in the meta-analysis, with larger points indicating higher weights. Note that this analysis is for demonstration purposes only, and biological conclusions should not be made about the results, given this is only a small subset of the data.\n\n\nCode\n# Generate a prediction dataframe\npred &lt;- expand_grid(log_abundance = seq(min(dat_meta$log_abundance, \n                                            na.rm = TRUE), \n                                        max(dat_meta$log_abundance, \n                                            na.rm = TRUE),\n                                        length.out = 50))\n\npred$abundance &lt;- exp(pred$log_abundance) # Back transform abundance\n\n# Bind predictions to dataframe\npred &lt;- cbind(pred, predict(object = metamod, \n                            newmods = pred$log_abundance))\n\n# Extract observed values\nobserved_values &lt;- broom::augment(metamod)\n\n# Add in variance estimates to be able to scale size of points by \n# amount of variance in estimate\nobserved_values &lt;- left_join(observed_values,\n                             dat_meta %&gt;% \n                               dplyr::select(sp, vi, log_abundance),\n                             by = c(\".rownames\" = \"sp\"))\n\n\n\n# Set abundance values for x axis\nabundances_x_axis &lt;- c(25, 50, 100, 1000, 5000)\n\n# Plot prediction\nggplot(pred, aes(x = log_abundance, y = pred)) + \n  geom_ribbon(aes(ymin = ci.lb, ymax = ci.ub), \n              fill = \"steelblue2\", alpha = 0.75) + \n  geom_line() + \n  geom_hline(yintercept = 0, lty = 2) + \n  labs(x = \"Species Abundance\", y = \"rAME of CDD\") + \n  scale_x_continuous(breaks = log(abundances_x_axis),\n                     labels = abundances_x_axis) + \n  # Add observed points\n  geom_point(data = observed_values,\n             aes(x = log_abundance, y = .observed, size = 1/vi), \n             alpha = 0.75) + \n  theme_bw(15) +\n  theme(legend.position = \"none\")\n\n\n\n\n\nIn principle, the approach outlined in this appendix of estimating marginal effects followed by a meta-regression analysis can be used to test for a correlation between strength of CDD and other species-level traits (e.g., wood density, leaf area, etc) or to determine how site-level CDD varies with site-level variables, such as latitude Hülsmann et al. (2024) precipitation, soil fertility, etc. \n\n\nCode\nsession_info()\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.0 (2022-04-22)\n os       macOS Big Sur/Monterey 10.16\n system   x86_64, darwin17.0\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       America/New_York\n date     2024-11-07\n pandoc   3.1.12.1 @ /usr/local/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version    date (UTC) lib source\n bit           4.0.5      2022-11-15 [1] CRAN (R 4.2.0)\n bit64         4.0.5      2020-08-30 [1] CRAN (R 4.2.0)\n cli           3.6.3      2024-06-21 [1] CRAN (R 4.2.0)\n codetools     0.2-18     2020-11-04 [1] CRAN (R 4.2.0)\n colorspace    2.1-1      2024-07-26 [1] CRAN (R 4.2.0)\n crayon        1.5.3      2024-06-20 [1] CRAN (R 4.2.0)\n digest        0.6.37     2024-08-19 [1] CRAN (R 4.2.0)\n dplyr       * 1.1.4      2023-11-17 [1] CRAN (R 4.2.0)\n evaluate      0.24.0     2024-06-10 [1] CRAN (R 4.2.0)\n fansi         1.0.6      2023-12-08 [1] CRAN (R 4.2.0)\n fastmap       1.2.0      2024-05-15 [1] CRAN (R 4.2.0)\n generics      0.1.3      2022-07-05 [1] CRAN (R 4.2.0)\n ggplot2     * 3.5.1      2024-04-23 [1] CRAN (R 4.2.0)\n glue          1.7.0      2024-01-09 [1] CRAN (R 4.2.0)\n gtable        0.3.5      2024-04-22 [1] CRAN (R 4.2.0)\n here        * 1.0.1      2020-12-13 [1] CRAN (R 4.2.0)\n hms           1.1.3      2023-03-21 [1] CRAN (R 4.2.0)\n htmltools     0.5.8.1    2024-04-04 [1] CRAN (R 4.2.0)\n htmlwidgets   1.6.4      2023-12-06 [1] CRAN (R 4.2.0)\n jsonlite      1.8.8      2023-12-04 [1] CRAN (R 4.2.0)\n knitr         1.48       2024-07-07 [1] CRAN (R 4.2.0)\n lattice       0.20-45    2021-09-22 [1] CRAN (R 4.2.0)\n lifecycle     1.0.4      2023-11-07 [1] CRAN (R 4.2.0)\n magrittr      2.0.3      2022-03-30 [1] CRAN (R 4.2.0)\n mathjaxr      1.6-0      2022-02-28 [1] CRAN (R 4.2.0)\n Matrix      * 1.4-1      2022-03-23 [1] CRAN (R 4.2.0)\n metadat     * 1.2-0      2022-04-06 [1] CRAN (R 4.2.0)\n metafor     * 4.6-0      2024-03-28 [1] CRAN (R 4.2.0)\n munsell       0.5.1      2024-04-01 [1] CRAN (R 4.2.0)\n nlme          3.1-157    2022-03-25 [1] CRAN (R 4.2.0)\n numDeriv    * 2016.8-1.1 2019-06-06 [1] CRAN (R 4.2.0)\n pillar        1.9.0      2023-03-22 [1] CRAN (R 4.2.0)\n pkgconfig     2.0.3      2019-09-22 [1] CRAN (R 4.2.0)\n purrr         1.0.2      2023-08-10 [1] CRAN (R 4.2.0)\n R6            2.5.1      2021-08-19 [1] CRAN (R 4.2.0)\n readr       * 2.1.5      2024-01-10 [1] CRAN (R 4.2.0)\n rlang         1.1.4      2024-06-04 [1] CRAN (R 4.2.0)\n rmarkdown     2.28       2024-08-17 [1] CRAN (R 4.2.0)\n rprojroot     2.0.4      2023-11-05 [1] CRAN (R 4.2.0)\n rstudioapi    0.16.0     2024-03-24 [1] CRAN (R 4.2.0)\n scales        1.3.0      2023-11-28 [1] CRAN (R 4.2.0)\n sessioninfo * 1.2.2      2021-12-06 [1] CRAN (R 4.2.0)\n tibble        3.2.1      2023-03-20 [1] CRAN (R 4.2.0)\n tidyr       * 1.3.1      2024-01-24 [1] CRAN (R 4.2.0)\n tidyselect    1.2.1      2024-03-11 [1] CRAN (R 4.2.0)\n tzdb          0.4.0      2023-05-12 [1] CRAN (R 4.2.0)\n utf8          1.2.4      2023-10-22 [1] CRAN (R 4.2.0)\n vctrs         0.6.5      2023-12-01 [1] CRAN (R 4.2.0)\n vroom         1.6.5      2023-12-05 [1] CRAN (R 4.2.0)\n withr         3.0.1      2024-07-31 [1] CRAN (R 4.2.0)\n xfun          0.47       2024-08-17 [1] CRAN (R 4.2.0)\n yaml          2.3.10     2024-07-26 [1] CRAN (R 4.2.0)\n\n [1] /Library/Frameworks/R.framework/Versions/4.2/Resources/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\n\nComita, Liza S, Salomón Aguilar, Stephen P Hubbell, and Rolando Pérez. 2023. “Long-Term Seedling and Small Sapling Census Data from the b Arro c Olorado i Sland 50 Ha f Orest d Ynamics p Lot, p Anama.” Ecology 104 (9): e4140.\n\n\nHülsmann, Lisa, Ryan A Chisholm, Liza Comita, Marco D Visser, Melina de Souza Leite, Salomon Aguilar, Kristina J Anderson-Teixeira, et al. 2024. “Latitudinal Patterns in Stabilizing Density Dependence of Forest Communities.” Nature 627 (8004): 564–71."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barber, Cristina, Andrii Zaiats, Cara Applestein, Lisa Rosenthal, and T\nTrevor Caughlin. 2022. “Bayesian Models for Spatially Explicit\nInteractions Between Neighbouring Plants.” Methods in Ecology\nand Evolution.\n\n\nBroekman, Maarten JE, Helene C Muller-Landau, Marco D Visser, Eelke\nJongejans, SJ Wright, and Hans de Kroon. 2019. “Signs of\nStabilisation and Stable Coexistence.” Ecology Letters\n22 (11): 1957–75.\n\n\nCanham, Charles D, Michael J Papaik, Marı́a Uriarte, William H\nMcWilliams, Jennifer C Jenkins, and Mark J Twery. 2006.\n“Neighborhood Analyses of Canopy Tree Competition Along\nEnvironmental Gradients in New England Forests.” Ecological\nApplications 16 (2): 540–54.\n\n\nChesson, Peter. 2000. “Mechanisms of Maintenance of Species\nDiversity.” Annual Review of Ecology and Systematics 31\n(1): 343–66.\n\n\nComita, Liza S, Salomón Aguilar, Stephen P Hubbell, and Rolando Pérez.\n2023. “Long-Term Seedling and Small Sapling Census Data from the b\nArro c Olorado i Sland 50 Ha f Orest d Ynamics p Lot, p Anama.”\nEcology 104 (9): e4140.\n\n\nCurrie, Iain D. 2016. “On Fitting Generalized Linear and\nNon-Linear Models of Mortality.” Scandinavian Actuarial\nJournal 2016 (4): 356–83.\n\n\nDetto, Matteo, Marco D Visser, S Joseph Wright, and Stephen W Pacala.\n2019. “Bias in the Detection of Negative Density Dependence in\nPlant Communities.” Ecology Letters 22 (September):\n1923–39. https://doi.org/10.1111/ele.13372.\n\n\nHülsmann, Lisa, Ryan A Chisholm, Liza Comita, Marco D Visser, Melina de\nSouza Leite, Salomon Aguilar, Kristina J Anderson-Teixeira, et al. 2024.\n“Latitudinal Patterns in Stabilizing Density Dependence of Forest\nCommunities.” Nature 627 (8004): 564–71.\n\n\nUriarte, Marı́a, Nathan G Swenson, Robin L Chazdon, Liza S Comita, W John\nKress, David Erickson, Jimena Forero-Montaña, Jess K Zimmerman, and Jill\nThompson. 2010. “Trait Similarity, Shared Ancestry and the\nStructure of Neighbourhood Interactions in a Subtropical Wet Forest:\nImplications for Community Assembly.” Ecology Letters 13\n(12): 1503–14."
  }
]