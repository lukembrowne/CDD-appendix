[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Appendix to Conspecific density dependence in plant communities: a theory-based toolkit for empirical studies",
    "section": "",
    "text": "1 Preface\nThis document serves as an appendix to the manuscript Conspecific density dependence in plant communities: a theory-based toolkit for empirical studies, with the goal of providing code and guidance on conducting analyses of density dependence following the best practices outlined in the main manuscript."
  },
  {
    "objectID": "calculating-neighborhood.html#overview",
    "href": "calculating-neighborhood.html#overview",
    "title": "2  Calculating neighborhood densities",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nThis tutorial follows from the Section 2. “Modeling considerations when estimating CDD” of the main text:\n\n\n\n\n\n\nSection 2. “Modeling considerations when estimating CDD”\n\n\n\nFor survival and growth models, defining the density metric and the size of local neighborhoods also require consideration. The density of conspecific and heterospecific neighbors surrounding a focal individual (or plot) can be measured within different distances (i.e., radii). Within a radius, neighbors should be weighted in the density summation with regard to their size and distance to the focal individuals, with the rationale that neighbors at a greater distance and of smaller size have smaller effects. Typically, weighting involves dividing the basal area or diameter by a linear function or an exponential function of distance to allow competitive effects of an individual of a given size to decline with distance (Uriarte et al. 2010; Canham et al. 2006). Biologically meaningful radii should be tested based on the size/life stage, vital rate of interest, and the likely agents of density dependence in the system. For a system of interest, we suggest comparing models with multiple distances (e.g., using maximum likelihood or information criterion) because forests may differ in how neighbors influence performance.\n\n\nHere, we demonstrate how to calculate the density of conspecific, heterospecific, and all neighbors surrounding a focal individual or plot for a given distance when XY coordinates are known. As a result, this section requires that the data set contains the location of mapped stems. If that is not available (as is common in seedling studies or smaller plots), continue to part 2 of this appendix.\nWe then demonstrate how to weight the calculation of neighborhood density by individual size (i.e., basal area) and distance using an exponential decay function (one of many possible decay functions), allowing the competitive effects of density effects to saturate (Uriarte et al. 2010; Canham et al. 2006).\nTo assess which shape parameters of the exponential decay function is most appropriate for the data set, we fit models with multiple combinations of decay function values and compare models using log likelihood.\nFrom a computational perspective, this approach can be relatively resource intensive both in terms of time and object size. It’s possible to make this approach more efficient by subdividing the data (e.g., by plot) or using more efficient data structures such as data.table.\nWe also note that alternative approaches allow the estimation of the effective scale of neighborhood interactions directly from data (Barber et al. 2022). An excellent case study using Stan is available here.\n\n\n\n\n\n\nNote\n\n\n\nThe following code is adapted from the latitudinalCNDD repository by Lisa Hülsmann."
  },
  {
    "objectID": "calculating-neighborhood.html#load-libraries-and-data",
    "href": "calculating-neighborhood.html#load-libraries-and-data",
    "title": "2  Calculating neighborhood densities",
    "section": "2.2 Load libraries and data",
    "text": "2.2 Load libraries and data\n\n\nCode\n# Load libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(parallel)\nlibrary(here)\nlibrary(spatstat.geom)\nlibrary(mgcv) # For fitting gams\nlibrary(lubridate) # For calculating census intervals\nlibrary(broom) # For processing fit models\nlibrary(purrr)\nlibrary(kableExtra) # For table styling"
  },
  {
    "objectID": "calculating-neighborhood.html#data-format-explanation",
    "href": "calculating-neighborhood.html#data-format-explanation",
    "title": "2  Calculating neighborhood densities",
    "section": "2.3 Data format explanation",
    "text": "2.3 Data format explanation\nFor this tutorial, we will be using an example data set from Barro Colorado Island (BCI) (available here) that includes 7,028 observations, of 3,771 individuals of 16 species across two census intervals with the 50 ha BCI 50 ha forest dynamics plot (more information here). Each stem is individually mapped, which allows us to calculate neighborhood density across different distance thresholds.\nThe code below assumes the data is in a format where each row is an observation for an individual from a census. For this data set, the column descriptions are as follows:\n\ntreeID: unique identifier for each tree\nsp: species code\ngx: spatial coordinate on x axis\ngy: spatial coordinate on y axis\ndbh: diameter at breast height (mm)\nba: basal area (m^2)\nstatus: status at census, A = alive, D = dead\ndate: date of observation\ncensus: census number\nsurv: survival status at census, 1 = alive, 0 = dead\nsurv_next: survival status at next census, 1 = alive, 0 = dead\nmort: mortality status at census, 1 = dead, 0 = alive\nmort_next: mortality status at next census, 1 = dead, 0 = alive\ninterval: time interval between censuses in years\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\nhead(dat, n = 5)\n\n\n# A tibble: 5 × 14\n  treeID sp        gx    gy   dbh     ba status date        surv surv_next  mort\n  &lt;chr&gt;  &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 3884   ceibpe  296.  24.8  1500 1.77   A      1981-11-10     1         1     0\n2 3998   cordbi  280.  45.4   281 0.0620 A      1981-12-11     1         1     0\n3 4065   ceibpe  289. 266.   2000 3.14   A      1982-01-08     1         1     0\n4 4070   cordbi  283. 287.    356 0.0995 A      1982-01-08     1         1     0\n5 4119   ceibpe  258. 224.   1600 2.01   A      1981-12-13     1         1     0\n# ℹ 3 more variables: mort_next &lt;dbl&gt;, interval &lt;dbl&gt;, census &lt;dbl&gt;\n\n\n\nWe can produce a plot Figure 2.1 of the tree locations, where the size of the point is scaled to basal area and colored by species, with each census displayed as a panel:\n\n\nCode\nggplot(dat, aes(x = gx, y = gy, size = ba, col = sp)) + \n  geom_point() + \n  facet_wrap(~census) +\n  theme_bw(10) + \n  theme(legend.position = \"right\") + \n  labs(size = \"Basal area\", col = \"Species\")\n\n\n\n\n\nFigure 2.1: Map of tree locations by census"
  },
  {
    "objectID": "calculating-neighborhood.html#define-exponential-decay-function",
    "href": "calculating-neighborhood.html#define-exponential-decay-function",
    "title": "2  Calculating neighborhood densities",
    "section": "2.4 Define exponential decay function",
    "text": "2.4 Define exponential decay function\nWe will demonstrate how to calculate neighborhood densities using an exponential decay function. In principle, it’s possible to use any number of different decay functions and to explore various ranges of values that determine the rate of decay. Here, we present the general framework for how one might explore different scenarios, though the ultimate choice of decay function and parameter values will depend on the study. Note that this method of calculating neighborhood density is only possible when neighbors’ x-y coordinates are known.\n\n\nCode\nexponential_decay &lt;- function(mu, distance){\n  return(exp(-(1/mu * distance)))\n}\n\n\nLet’s see what the exponential decay function looks like across a range of mu values (Figure 2.2):\n\n\nCode\n# Set range of mu values and distances\ndecay_values &lt;- seq(from = 1, to = 25, by = 2)\n\n# Use sprintf to add leading 0s, will help with sorting later on\ndecay_names = paste(\"exp\", sprintf(\"%02s\", decay_values), sep = \"\") \n\ndistances &lt;- seq(1, 100, 1)\n\n# Generate a dataframe with each combination of mu and distance\nexample_decay_values &lt;- expand_grid(decay_values, distances) %&gt;%\n                        # Rename columns\n                        rename(decay_value = decay_values, \n                               distance = distances)\n\n# Evaluate distance decay function for each combination of \n# mu and distance\nexample_decay_values$decay &lt;- exponential_decay(\n                              mu = example_decay_values$decay_value,\n                              distance = example_decay_values$distance)\n\n# Plot results\nggplot(example_decay_values, \n       aes(x = distance, y = decay, \n           color = decay_value, group = decay_value)) + \n  ylab(\"Density\") + \n  xlab(\"Distance (m)\") + \n  scale_color_continuous(name = \"Value of \\ndecay constant\") + \n  geom_line() + \n  theme_bw(12)\n\n\n\n\n\nFigure 2.2: Plot of decay function\n\n\n\n\n\n2.4.1 Determine which trees are at edge of plot\nTrees near the edge of plot boundaries have incomplete information about their neighborhood, because trees outside of the plot boundaries are not mapped. Typically trees within certain distance threshold from the plot edge are excluded from analysis, but still included in calculations of neighborhood densities.\nWe are going to add a column to our data set called ‘edge’ that is TRUE if within a set distance to the edge of the plot.\nFor this example data set, the dimensions of the overall plot are 300 x 300 m, ranging from 0-300 on both the x and y axis, and representing a subset of the overall 50 ha forest dynamics plot at BCI.\n\n\nCode\n# Set threshold for distance to edge\ndistance_threshold_edge = 30\n\n# Add in min and max values for corners of plot\nmin_x &lt;- 0\nmax_x &lt;- 300\nmin_y &lt;- 0\nmax_y &lt;- 300\n\ndat$edge = dat$gx &lt; min_x + distance_threshold_edge |\n           dat$gx &gt; max_x - distance_threshold_edge |\n           dat$gy &lt; min_y + distance_threshold_edge |\n           dat$gy &gt; max_y - distance_threshold_edge\n\n# How many trees fall within the edge threshold?\ntable(dat$edge)\n\n\n\nFALSE  TRUE \n 4526  2502 \n\n\nBelow is a plot Figure 2.3 of tree locations colored by whether they fall within the edge threshold or not, separated out for each census.\n\n\nCode\nggplot(dat, aes(x = gx, y = gy, col = edge)) + \n  geom_point() + \n  facet_wrap(~census) +\n  coord_fixed(ratio = 1) + \n  theme_bw(12)\n\n\n\n\n\nFigure 2.3: Map of tree locations colored by whether they fall within the edge threshold"
  },
  {
    "objectID": "calculating-neighborhood.html#calculate-distances-among-focal-individual-and-neighbors",
    "href": "calculating-neighborhood.html#calculate-distances-among-focal-individual-and-neighbors",
    "title": "2  Calculating neighborhood densities",
    "section": "2.5 Calculate distances among focal individual and neighbors",
    "text": "2.5 Calculate distances among focal individual and neighbors\nNext, we will calculate distances among individuals in our plot, using an upper threshold distance of what to consider a neighbor.\nWe use the ‘spatstat.geom’ package to efficiently calculate distances among individuals.\nBecause this example only includes one census interval (two censuses total), we will subset down to just the first census to calculate neighborhood density.\nIf the data set were to contain multiple census intervals, it would be necessary to calculate neighborhood density separately for each census interval, using only the individuals that were alive at the beginning of that census interval.\n\n\nCode\n# Set distance threshold for considering neighbors\ndistance_threshold_neighborhood &lt;- 30\n\n# Subset to first census - this will be different for different \n# datasets\ndat_first_census &lt;- dat %&gt;%\n                    filter(census == 1)\n\n# Format into 'ppp' object\ndat_ppp = spatstat.geom::ppp(dat_first_census$gx, dat_first_census$gy, \n                             window = owin(range(dat$gx), \n                                           range(dat$gy)), \n                                           checkdup = F)\n\n# Determine close pairs based on distance threshold\n# Returns a list that we convert to tibble later\nneighbors = spatstat.geom::closepairs(dat_ppp, \n              rmax = distance_threshold_neighborhood, # Max radius\n              what = \"ijd\", # return indicies i, j, and distance \n              twice = TRUE)\n\n# Convert to dataframe\nneighbors &lt;- as_tibble(neighbors)\n\n# Take a peek at the data\n# i = index of focal individual\n# j = index of neighbor\n# d = distance to neighbor\nhead(neighbors)\n\n\n# A tibble: 6 × 3\n      i     j     d\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1  3227  3252 27.0 \n2  3227  3238 17.1 \n3  3227  3240  5.75\n4  3227  3229 18.6 \n5  3227  3253 24.4 \n6  3227  3222 20.8 \n\n\nNext we add in additional columns for neighbor characteristic, (e.g., species, size) and whether they are located near the edge of the plot (blue area in Figure 2.3).\n\n\nCode\n# add additional columns\n\n# Add species for individual i\nneighbors$sp_i = dat_first_census$sp[neighbors$i] \n\n# Add whether individual i is near edge\nneighbors$edge_i = dat_first_census$edge[neighbors$i] \n\n# Add species for individual j\nneighbors$sp_j = dat_first_census$sp[neighbors$j] \n\n# Add basal area of individual j\nneighbors$ba_j = dat_first_census$ba[neighbors$j] \n\n\nWe then want to add a column that indicates whether the comparison between the focal individual and the neighbor is conspecific or heterospecific because we are interested separately estimating the densities of conspecifics and heterospecifics.\n\n\nCode\nneighbors$comparison_type &lt;- ifelse(neighbors$sp_i == neighbors$sp_j,\n                                    yes = \"con\", # conspecific\n                                    no = \"het\") # heterospecific\n\n\nWe then remove focal trees that are too close to the edge of the plot\n\n\nCode\n# remove focal trees i that are at the edge\nneighbors = neighbors[!neighbors$edge_i, ]\n\n\nNext, we add columns to our neighbors data set that indicates the distance decay multiplier and the distance decay multiplier weighted by basal area\n\n\nCode\n# Loop through distance decay values\nfor(x in 1:length(decay_values)){\n  \n  #  Add in column for distance decay multiplier for each decay value\n  # add _ba suffix to column name - will eventually be summed based on\n  # number of individual neighbors\n  neighbors[, paste0(decay_names[x], \"_N\")] &lt;- exponential_decay(mu = decay_values[x], \n                                                   distance = neighbors$d)\n  \n  # Weight distance decay multiplier by basal area of neighbor\n  # add _ba suffix to column name\n  neighbors[, paste0(decay_names[x], \"_BA\")] &lt;- exponential_decay(\n                                                mu = decay_values[x], \n                             distance = neighbors$d) * neighbors$ba_j\n}\n\n\nDepending on how many distance decay values are being investigated, there may be many columns in the data frame.\n\n\nCode\nhead(neighbors)\n\n\n# A tibble: 6 × 34\n      i     j     d sp_i  edge_i sp_j     ba_j comparison_type  exp01_N exp01_BA\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt;  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;    &lt;dbl&gt;\n1  2973  2993 14.3  cord… FALSE  capp… 1.57e-4 het             6.36e- 7 9.98e-11\n2  2973  3122 20.4  cord… FALSE  des2… 4.91e-4 het             1.40e- 9 6.86e-13\n3  2973  2974  5.86 cord… FALSE  des2… 7.85e-5 het             2.85e- 3 2.24e- 7\n4  2973  3123 27.2  cord… FALSE  des2… 7.85e-5 het             1.47e-12 1.16e-16\n5  2973  3121 14.5  cord… FALSE  mico… 1.77e-4 het             5.27e- 7 9.32e-11\n6  2973  2936 13.4  cord… FALSE  des2… 7.85e-5 het             1.45e- 6 1.14e-10\n# ℹ 24 more variables: exp03_N &lt;dbl&gt;, exp03_BA &lt;dbl&gt;, exp05_N &lt;dbl&gt;,\n#   exp05_BA &lt;dbl&gt;, exp07_N &lt;dbl&gt;, exp07_BA &lt;dbl&gt;, exp09_N &lt;dbl&gt;,\n#   exp09_BA &lt;dbl&gt;, exp11_N &lt;dbl&gt;, exp11_BA &lt;dbl&gt;, exp13_N &lt;dbl&gt;,\n#   exp13_BA &lt;dbl&gt;, exp15_N &lt;dbl&gt;, exp15_BA &lt;dbl&gt;, exp17_N &lt;dbl&gt;,\n#   exp17_BA &lt;dbl&gt;, exp19_N &lt;dbl&gt;, exp19_BA &lt;dbl&gt;, exp21_N &lt;dbl&gt;,\n#   exp21_BA &lt;dbl&gt;, exp23_N &lt;dbl&gt;, exp23_BA &lt;dbl&gt;, exp25_N &lt;dbl&gt;,\n#   exp25_BA &lt;dbl&gt;"
  },
  {
    "objectID": "calculating-neighborhood.html#calculate-neighborhood-density",
    "href": "calculating-neighborhood.html#calculate-neighborhood-density",
    "title": "2  Calculating neighborhood densities",
    "section": "2.6 Calculate neighborhood density",
    "text": "2.6 Calculate neighborhood density\nThen we summarize neighborhood density for each focal tree separately for conspecifics and heterospecifics.\n\n\nCode\n# Simple calculations of number of neighbors and total basal area, \n# ignoring distance decay\nneighbors_summary &lt;- neighbors %&gt;%\n                     group_by(i, comparison_type) %&gt;%\n                     summarise(nodecay_N = n(), # count of neighbors\n                          nodecay_BA = sum(ba_j)) # sum of basal area)\n\n# Add in decay columns\nneighbors_summary_decay &lt;- neighbors %&gt;%\n                            group_by(i, comparison_type) %&gt;%\n                      # Select only columns related to distance decay\n                            select(starts_with(\"exp\")) %&gt;% \n                      # Summarize them all by summing columns\n                            summarise_all(sum) \n\n# Join both together\nneighbors_summary &lt;- left_join(neighbors_summary, \n                               neighbors_summary_decay,\n                               by = c(\"i\", \"comparison_type\"))\n\n# Add treeID column\nneighbors_summary$treeID&lt;-dat_first_census$treeID[neighbors_summary$i]\n\n\n   \n# If there are any focal individuals with no neighbors, add values \n# of 0 for neighborhood densities \n      noNeighbors = dat_first_census$treeID[!dat_first_census$treeID \n                                      %in% neighbors_summary$treeID &\n                                          !dat_first_census$edge]\n      \n      # If there are individuals with no neighbors\n      if (length(noNeighbors) &gt; 0) {\n        neighbors_summary = bind_rows(neighbors_summary, \n                                      expand_grid(i = NA, \n                                             treeID = noNeighbors, \n                             comparison_type = c(\"het\", \"cons\"))) %&gt;%\n                            # Add 0s where NA\n                            mutate_all(replace_na, replace = 0) \n        }\n\n# Take a peak at the data\nhead(neighbors_summary)\n\n\n# A tibble: 6 × 31\n# Groups:   i [6]\n      i comparison_type nodecay_N nodecay_BA exp01_N   exp01_BA exp03_N exp03_BA\n  &lt;int&gt; &lt;chr&gt;               &lt;int&gt;      &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n1     5 het                   115     0.487  0.0991     4.05e-5   2.03  0.00172 \n2     8 het                    94     0.0539 0.181      3.63e-5   1.21  0.000703\n3     9 het                   151     2.17   0.0464     1.67e-4   1.65  0.00287 \n4    10 het                    76     0.0509 0.00131    4.72e-7   0.464 0.000175\n5    11 het                    59     0.0631 0.00859    1.99e-6   0.493 0.000277\n6    12 het                    93     0.0506 0.0441     9.51e-6   1.57  0.000745\n# ℹ 23 more variables: exp05_N &lt;dbl&gt;, exp05_BA &lt;dbl&gt;, exp07_N &lt;dbl&gt;,\n#   exp07_BA &lt;dbl&gt;, exp09_N &lt;dbl&gt;, exp09_BA &lt;dbl&gt;, exp11_N &lt;dbl&gt;,\n#   exp11_BA &lt;dbl&gt;, exp13_N &lt;dbl&gt;, exp13_BA &lt;dbl&gt;, exp15_N &lt;dbl&gt;,\n#   exp15_BA &lt;dbl&gt;, exp17_N &lt;dbl&gt;, exp17_BA &lt;dbl&gt;, exp19_N &lt;dbl&gt;,\n#   exp19_BA &lt;dbl&gt;, exp21_N &lt;dbl&gt;, exp21_BA &lt;dbl&gt;, exp23_N &lt;dbl&gt;,\n#   exp23_BA &lt;dbl&gt;, exp25_N &lt;dbl&gt;, exp25_BA &lt;dbl&gt;, treeID &lt;chr&gt;\n\n\nAs described in the main text, it can be advantageous to use total density that includes both conspecific and heterospecific density as a covariate, rather than only heterospecific density.\nHere, we calculate overall density by summing heterospecific and conspecific densities.\n\n\nCode\n# First convert to long format which will make it easy to sum across \n# heterospecific and conspecific values\nneighbors_summary_long_format &lt;-  neighbors_summary %&gt;%\n                                  pivot_longer(cols = -c(\"i\", \"comparison_type\", \"treeID\"))\n\n# Sum across heterospecific and conspecific values and rename to 'all'\nneighbors_total_long_format &lt;- neighbors_summary_long_format %&gt;%\n                                group_by(i, treeID, name) %&gt;%\n                                summarize(value = sum(value)) %&gt;%\n                                mutate(comparison_type = \"all\")\n\n# Bind together conspecific and 'all' densities\n# remove heterospecific columns\n# fill in 0s where there are no neighbors\nneighbors_summary_total = bind_rows(neighbors_summary_long_format,\n                                    neighbors_total_long_format) %&gt;%\n                        # Can filter out heterospecific neighborhood \n                        # values by uncommenting this line of the\n                        # objects become too large\n                        #  filter(comparison_type != \"het\") %&gt;% \n                mutate(name = paste0(comparison_type, \"_\", name)) %&gt;%\n                select(-comparison_type) %&gt;%\n                pivot_wider(names_from = name, values_from = value, \n                            values_fill = 0)"
  },
  {
    "objectID": "calculating-neighborhood.html#model-mortality-as-a-function-of-neighborhood-density",
    "href": "calculating-neighborhood.html#model-mortality-as-a-function-of-neighborhood-density",
    "title": "2  Calculating neighborhood densities",
    "section": "2.7 Model mortality as a function of neighborhood density",
    "text": "2.7 Model mortality as a function of neighborhood density\nTo determine the ‘best’ decay parameter to use, we fit species-specific mortality models using Generalized Additive Models GAMs and compare models based on log likelihoods. GAMs are flexible statistical models that combine linear and non-linear components to capture complex relationships in data.\nWe first create our data set that we will use in the GAMs, subsetting down to just one census interval (because our example dataset only has 2 censuses) and removing trees close to the edge. In other datasets, you may have multiple census intervals, where it would be common practice to include ‘year’ or ‘census’ as a random effect in the model, but otherwise the overall approach is similar.\n\n\nCode\n# Join census data with neighborhood data\ndat_gam &lt;- left_join(dat_first_census,\n                               neighbors_summary_total,\n                               by = \"treeID\")\n\n# Remove edge trees\ndat_gam &lt;- dat_gam %&gt;%\n                    filter(edge == FALSE)\n\n\nFor each species, we summarize data availability to help determine parameters for the GAM smooth terms.\n\n\nCode\n# Summarize data availability at species level to set the degree of \n# smoothness for GAM smooth terms\nsp_summary &lt;- dat_gam %&gt;% \n  group_by(sp) %&gt;% \n  summarise(ndead = sum(mort_next),\n            nsurv = sum(surv_next),\n            range_con_BA = max(con_nodecay_BA) - min(con_nodecay_BA),\n            max_con_BA = max(con_nodecay_BA),\n            unique_con_BA = length(unique(con_nodecay_BA)),\n            unique_all_BA = length(unique(all_nodecay_BA)),\n            range_con_N = max(con_nodecay_N) - min(con_nodecay_N),\n            max_con_N = max(con_nodecay_N),\n            unique_con_N = length(unique(con_nodecay_N)),\n            unique_all_N = length(unique(all_nodecay_N)),\n            unique_dbh = length(unique(dbh))\n  )\n\n# Filter out species that have 0 or 100% mortality\nsp_summary &lt;- sp_summary %&gt;%\n  filter(ndead &gt; 0) %&gt;%\n  filter(nsurv &gt; 0)\n\n\nIn this long block of code, we loop over all possible combinations of decay values for neighborhood densities weighted by abundance (N) and size (BA) for each species and fit a separate GAM for each model. For each GAM, we assess whether the model was able to be fit, and if it was able to be fit, whether it converged and for potential warnings. We save the results of successful model fits into a list that we will process later.\nFor large datasets where individual GAMs take a long time to run, the code could be modified to run in parallel, either locally on a personal computer or across a computing cluster.\n\n\nCode\n# Initialize list that will save model outputs\nres_mod &lt;- list()\n\n\n# Model run settings\nrun_settings &lt;- expand_grid(species = unique(sp_summary$sp),\n                            decay_con = c(\"nodecay\", decay_names),\n                            decay_total = c(\"nodecay\", decay_names),\n                            nhood_data_type = c(\"N\", \"BA\"))\n\n\n# Loop through model run settings\nfor(run_settings_row in 1:nrow(run_settings)){\n  \n  # Extract values from run settings dataframe\n  species &lt;- run_settings$species[run_settings_row]\n  decay_con &lt;- run_settings$decay_con[run_settings_row]\n  decay_total &lt;- run_settings$decay_total[run_settings_row]\n  nhood_data_type &lt;- run_settings$nhood_data_type[run_settings_row]\n\n  # Subset down to just focal species\n  dat_subset &lt;- dat_gam %&gt;%\n    filter(sp == species)\n\n  # Set run name\n  run_name &lt;- paste0(species, \"_total\", decay_total,\"_con\", \n                     decay_con, \"_\", nhood_data_type)\n  \n  # Print status if desired\n  # cat(\"Working on run: \", run_name, \" ...\\n\")\n\n  # Create model formula\n  # Initial DBH included as predictor variable\n  form =  paste0(\"mort_next ~ s(dbh, k = k1) + s(all_\", decay_total, \n                              \"_\", nhood_data_type, \n                              \", k = k2)  + s(con_\", decay_con, \n                              \"_\", nhood_data_type, \n                              \", k = k3)\")\n    \n  # Convert into formula\n  form &lt;- as.formula(form)\n    \n    # Choose penalties for model fitting\n    # set to default 10 (the same as -1)\n    # The higher the value of k, the more flexible the smooth term becomes, allowing for more intricate and wiggly patterns. Conversely, lower values of k result in smoother and simpler representations.\n    k1 = k2 = k3 = 10\n    if (k1 &gt; sp_summary$unique_dbh[sp_summary$sp == species]) { \n      k1 = sp_summary$unique_dbh[sp_summary$sp == species] - 2\n    }\n    if (k2 &gt; sp_summary$unique_all_N[sp_summary$sp == species]) {\n      k2 = sp_summary$unique_all_N [sp_summary$sp == species]- 2\n    }\n    if (k3 &gt; sp_summary$unique_con_N[sp_summary$sp == species]) { \n      k3 = sp_summary$unique_con_N[sp_summary$sp == species] - 2 \n    }\n    \n\n   # Fit model\n   # wrap in a try function to catch any errors\n   mod = try(gam(form,\n            family = binomial(link=cloglog),\n            offset = log(interval),\n            data = dat_subset,\n            method = \"REML\"), \n          silent = T)\n\n   \n    # Check if model was able to fit\n    if (!any(class(mod) == \"gam\")) {\n      # print(paste(\"gam failed for:\", run_name))\n    } else {\n\n    # Check if gam converged\n    if (!mod$converged) {\n      # print(paste(\"no convergence for:\", run_name))\n    } else {\n  \n      # check for complete separation\n      # https://stats.stackexchange.com/questions/336424/issue-with-complete-separation-in-logistic-regression-in-r\n      # Explore warning \"glm.fit: fitted probabilities numerically 0 \n      # or 1 occurred\"\n      eps &lt;- 10 * .Machine$double.eps\n      glm0.resids &lt;- augment(x = mod) %&gt;%\n        mutate(p = 1 / (1 + exp(-.fitted)),\n               warning = p &gt; 1-eps,\n               influence = order(.hat, decreasing = T))\n      infl_limit = round(nrow(glm0.resids)/10, 0)\n      # check if none of the warnings is among the 10% most \n      # influential observations, than it is okay..\n      num = any(glm0.resids$warning & glm0.resids$influence &lt; infl_limit)\n      \n      # complete separation\n      if (num) {\n       # print(paste(\"complete separation is likely for:\", run_name))\n      } else {\n        \n        # missing Vc\n        if (is.null(mod$Vc)) {\n         # print(paste(\"Vc not available for:\", run_name))\n        } else {\n        \n          # Add resulting model to list if it passes all checks\n          res_mod[[run_name]] &lt;- mod\n          \n        } # Vc ifelse\n      } # complete separation ifelse\n    } # convergence ifelse\n  } # model available ifelse\n} # end run settings loop"
  },
  {
    "objectID": "calculating-neighborhood.html#summarize-model-fits",
    "href": "calculating-neighborhood.html#summarize-model-fits",
    "title": "2  Calculating neighborhood densities",
    "section": "2.8 Summarize model fits",
    "text": "2.8 Summarize model fits\nNext, we will extract regression coefficients into a dataframe using broom::tidy()\n\n\nCode\n# Extract coefficients for each model into a list\ncoefs = lapply(res_mod, broom::tidy) \n\n# Add a column for model run to each object in the list\ncoefs = Map(cbind, coefs, run_name = names(coefs)) \ncoefs = do.call(rbind, coefs) # Bind elements of list together by rows\nrownames(coefs) &lt;- NULL # Remove row names\ncoefs &lt;- coefs %&gt;%\n          select(run_name, everything()) # Rearrange columns\n\n# Take a look at the data\nknitr::kable(head(coefs), digits = 2, booktabs = T) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nrun_name\nterm\nedf\nref.df\nstatistic\np.value\n\n\n\n\ncappfr_totalnodecay_connodecay_N\ns(dbh)\n1.00\n1.00\n2.46\n0.12\n\n\ncappfr_totalnodecay_connodecay_N\ns(all_nodecay_N)\n1.23\n1.43\n1.25\n0.31\n\n\ncappfr_totalnodecay_connodecay_N\ns(con_nodecay_N)\n1.00\n1.00\n1.84\n0.17\n\n\ncappfr_totalnodecay_connodecay_BA\ns(dbh)\n4.36\n5.31\n9.83\n0.09\n\n\ncappfr_totalnodecay_connodecay_BA\ns(all_nodecay_BA)\n1.00\n1.00\n0.36\n0.55\n\n\ncappfr_totalnodecay_connodecay_BA\ns(con_nodecay_BA)\n1.00\n1.00\n3.04\n0.08\n\n\n\n\n\n\n\nNext we will extract model summaries for each model with broom::glance() that provides key information like degrees of freedom, log likelihood, AIC, etc.\n\n\nCode\n# Extract summaries for each model into a list\nsums = lapply(res_mod, broom::glance) \n\n# Add a column for model run to each object in the list\nsums = Map(cbind, sums, run_name = names(sums)) \nsums = do.call(rbind, sums) # Bind elements of list together by rows\nrownames(sums) &lt;- NULL # Remove row names\n\n# Separate run name into columns for species, decay, and density type\nsums &lt;- sums %&gt;%\n        separate(run_name, into = c(\"sp\", \"decay_total\", \n                                    \"decay_con\", \"density_type\"), \n                 remove = FALSE)\n\n# Remove 'total' and 'con' from decay columns\nsums$decay_total &lt;- gsub(\"total\", \"\", sums$decay_total)\nsums$decay_con &lt;- gsub(\"con\", \"\", sums$decay_con)\n\n# Rearrange columns  \nsums &lt;- sums %&gt;%\n          select(run_name, sp, decay_total, decay_con, density_type, \n                 everything()) \n\n# Take a look at the model summaries\nknitr::kable(head(sums), digits = 2) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nrun_name\nsp\ndecay_total\ndecay_con\ndensity_type\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\ncappfr_totalnodecay_connodecay_N\ncappfr\nnodecay\nnodecay\nN\n4.23\n-22.62\n54.10\n70.37\n45.23\n285.77\n290\n\n\ncappfr_totalnodecay_connodecay_BA\ncappfr\nnodecay\nnodecay\nBA\n7.36\n-17.21\n51.03\n81.52\n34.41\n282.64\n290\n\n\ncappfr_totalexp01_connodecay_N\ncappfr\nexp01\nnodecay\nN\n5.06\n-22.64\n56.52\n77.14\n45.29\n284.94\n290\n\n\ncappfr_totalexp01_connodecay_BA\ncappfr\nexp01\nnodecay\nBA\n8.08\n-17.54\n53.90\n88.44\n35.07\n281.92\n290\n\n\ncappfr_totalexp03_connodecay_N\ncappfr\nexp03\nnodecay\nN\n8.07\n-17.87\n54.98\n90.30\n35.73\n281.93\n290\n\n\ncappfr_totalexp03_connodecay_BA\ncappfr\nexp03\nnodecay\nBA\n7.93\n-17.62\n53.85\n87.98\n35.24\n282.07\n290\n\n\n\n\n\n\n\nDue to limited sample sizes, it is likely that GAMs will not fit for each species. For example, in the table below of summary data of species where models did not successfully fit/converge, there are several instances of where all individuals of the species survived during the census, leading to no mortality events to use in the model.\n\nWe need to exclude species without complete model runs from our overall calculations when looking for optimal decay parameters across the entire data set.\n\n\nCode\n# Tally up number of model runs by species and total decay values\ntable(sums$sp, sums$decay_total)\n\n\n        \n         exp01 exp03 exp05 exp07 exp09 exp11 exp13 exp15 exp17 exp19 exp21\n  cappfr    28    28    28    28    28    28    28    28    28    28    28\n  cordbi    27    28    28    28    28    28    28    28    28    27    27\n  des2pa    28    28    28    28    28    28    28    28    28    28    28\n  micone    28    28    28    28    28    28    28    28    28    28    28\n  pentma    28    28    28    28    28    23    27    28    28    28    28\n  stylst    28    28    28    28    28    28    28    28    28    28    28\n        \n         exp23 exp25 nodecay\n  cappfr    28    28      28\n  cordbi    28    28      28\n  des2pa    28    28      28\n  micone    28    28      28\n  pentma    28    28      28\n  stylst    28    28      28\n\n\nCode\n# get incomplete run-site-species combinations\nrun_counts_by_sp &lt;- sums %&gt;% \n                    group_by(sp) %&gt;%\n                    tally() %&gt;%\n                    # Join with overall species list\n                    left_join(sp_summary %&gt;% select(sp), ., by = \"sp\") \n\n# Get expected number of runs if all models work\nexpected_total_runs &lt;- run_settings %&gt;%\n                       group_by(species) %&gt;%\n                       tally() %&gt;%\n                       pull(n) %&gt;%\n                       max()\n\n# Save species names where they didn't have all expected combinations \n# of model runs\nincomplete = run_counts_by_sp$sp[run_counts_by_sp$n &lt; \n                                   expected_total_runs | \n                                   is.na(run_counts_by_sp$n)]\n\n# Species with successful runs\nknitr::kable(sp_summary[!sp_summary$sp %in% incomplete, ], \n             digits = 2) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nsp\nndead\nnsurv\nrange_con_BA\nmax_con_BA\nunique_con_BA\nunique_all_BA\nrange_con_N\nmax_con_N\nunique_con_N\nunique_all_N\nunique_dbh\n\n\n\n\ncappfr\n5\n285\n0.02\n0.02\n273\n290\n30\n32\n31\n111\n31\n\n\ndes2pa\n174\n1172\n0.14\n0.15\n1335\n1343\n123\n140\n123\n157\n77\n\n\nmicone\n38\n28\n0.00\n0.00\n48\n66\n12\n12\n13\n55\n11\n\n\nstylst\n3\n101\n0.02\n0.02\n91\n104\n13\n13\n14\n68\n30\n\n\n\n\n\n\n\nCode\n# Species without successful runs\nknitr::kable(sp_summary[sp_summary$sp %in% incomplete, ], \n             digits = 2) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nsp\nndead\nnsurv\nrange_con_BA\nmax_con_BA\nunique_con_BA\nunique_all_BA\nrange_con_N\nmax_con_N\nunique_con_N\nunique_all_N\nunique_dbh\n\n\n\n\nacalma\n2\n5\n0.00\n0.00\n4\n7\n2\n2\n2\n7\n6\n\n\ncordbi\n7\n36\n0.25\n0.25\n36\n43\n10\n10\n11\n38\n28\n\n\npentma\n12\n21\n0.00\n0.00\n20\n33\n6\n6\n6\n31\n11\n\n\nsponra\n3\n15\n0.08\n0.08\n12\n18\n5\n5\n4\n17\n16"
  },
  {
    "objectID": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-across-all-species",
    "href": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-across-all-species",
    "title": "2  Calculating neighborhood densities",
    "section": "2.9 Selecting optimum decay parameter values across all species",
    "text": "2.9 Selecting optimum decay parameter values across all species\nWe then summarize different model criteria across all species runs. To look for the optimal value for decay parameters, we sum log likelihoods across all species for a given decay parameter combination and choose the resulting parameter combination with the highest summed log likelihood.\n\n\nCode\nsums_total &lt;- sums %&gt;% \n              filter(!sp %in% incomplete) %&gt;%\n              group_by(decay_total, decay_con, density_type) %&gt;%\n              summarise(nvalues = n(),\n                        sumlogLik = sum(logLik),\n                        meanlogLik = mean(logLik)) %&gt;%\n              arrange(decay_total, decay_con, density_type)\n\nsums_total\n\n\n# A tibble: 392 × 6\n# Groups:   decay_total, decay_con [196]\n   decay_total decay_con density_type nvalues sumlogLik meanlogLik\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;          &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 exp01       exp01     BA                 4     -556.      -139.\n 2 exp01       exp01     N                  4     -554.      -138.\n 3 exp01       exp03     BA                 4     -553.      -138.\n 4 exp01       exp03     N                  4     -558.      -140.\n 5 exp01       exp05     BA                 4     -554.      -138.\n 6 exp01       exp05     N                  4     -561.      -140.\n 7 exp01       exp07     BA                 4     -557.      -139.\n 8 exp01       exp07     N                  4     -563.      -141.\n 9 exp01       exp09     BA                 4     -558.      -140.\n10 exp01       exp09     N                  4     -564.      -141.\n# ℹ 382 more rows\n\n\nWe create a heatmap plot Figure 2.4 of summed log likelihoods for all parameter combinations across all species, with the optimal parameter combination marked with an X\n\n\nCode\n# Find optimum value separately for N and BA\n  optimum &lt;- sums_total %&gt;%\n               group_by(density_type) %&gt;%\n               slice_max(sumlogLik)\n  \n# Plot heatmap of log likelihood values\n  ggplot(sums_total, aes(x = decay_total, y = decay_con, \n                         fill = sumlogLik)) +\n    geom_tile(width = 0.9, height = 0.9, col = \"black\") + \n    scale_fill_viridis_c() + # viridis color palette\n    geom_label(data = optimum, label = \"X\") + \n    labs(x = \"Decay total density\", y = \"Decay conspecific density\", \n         fill = \"sumlogLik\") + \n    facet_wrap(~density_type, ncol = 1) + \n    theme_bw(12) + \n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, \n                                     hjust=1))\n\n\n\n\n\nFigure 2.4: Heatmap of optimal values for decay constants\n\n\n\n\nFor this data set, the following are the optimal decay parameter values across all species separately for neighborhood density calculated by abundance (N) and by basal area (BA)\n\n\nCode\nknitr::kable(optimum, digits = 2) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\ndecay_total\ndecay_con\ndensity_type\nnvalues\nsumlogLik\nmeanlogLik\n\n\n\n\nexp19\nexp25\nBA\n4\n-541.69\n-135.42\n\n\nexp03\nexp01\nN\n4\n-541.59\n-135.40"
  },
  {
    "objectID": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-separately-for-each-species",
    "href": "calculating-neighborhood.html#selecting-optimum-decay-parameter-values-separately-for-each-species",
    "title": "2  Calculating neighborhood densities",
    "section": "2.10 Selecting optimum decay parameter values separately for each species",
    "text": "2.10 Selecting optimum decay parameter values separately for each species\nAlternatively, it may be useful to determine optimum decay parameters on a species by species basis. To do this, we find the maximum log liklihood for each species separately across decay parameter combination and choose the resulting parameter combination with the highest log likelihood.\n\n\nCode\nsums_total_by_spp &lt;- sums %&gt;% \n              filter(!sp %in% incomplete) %&gt;%\n              group_by(decay_total, decay_con, density_type, sp) %&gt;%\n              summarise(nvalues = n(),\n                        sumlogLik = sum(logLik),\n                        meanlogLik = mean(logLik)) %&gt;%\n              arrange(decay_total, decay_con, density_type)\n\nsums_total_by_spp\n\n\n# A tibble: 1,568 × 7\n# Groups:   decay_total, decay_con, density_type [392]\n   decay_total decay_con density_type sp     nvalues sumlogLik meanlogLik\n   &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n 1 exp01       exp01     BA           cappfr       1    -16.6      -16.6 \n 2 exp01       exp01     BA           des2pa       1   -489.      -489.  \n 3 exp01       exp01     BA           micone       1    -43.7      -43.7 \n 4 exp01       exp01     BA           stylst       1     -6.84      -6.84\n 5 exp01       exp01     N            cappfr       1    -14.3      -14.3 \n 6 exp01       exp01     N            des2pa       1   -489.      -489.  \n 7 exp01       exp01     N            micone       1    -41.3      -41.3 \n 8 exp01       exp01     N            stylst       1     -8.64      -8.64\n 9 exp01       exp03     BA           cappfr       1    -20.7      -20.7 \n10 exp01       exp03     BA           des2pa       1   -485.      -485.  \n# ℹ 1,558 more rows\n\n\nWe create a heatmap plot Figure 2.5 of log likelihoods for all parameter combinations for each species separately, with the optimal parameter combination marked with an X. We only display the first three species here, because with data sets containing many species, it will be hard to visualize all the species on one graph and you may need to subdivide the plot further for visualization.\n\n\nCode\n  sums &lt;- sums %&gt;%\n    filter(sp %in% c(\"cappfr\", \"cordbi\", \"des2pa\")) %&gt;%\n    group_by(sp) %&gt;%\n    # Scale loglikihood by species to help with visualization\n    mutate(logLik_scaled = scale(logLik)) %&gt;% \n    ungroup()\n \n# Find optimum value separately for N and BA\n  optimum_by_sp &lt;- sums %&gt;%\n               group_by(sp, density_type) %&gt;%\n               slice_max(logLik_scaled, with_ties = FALSE)\n\n# Plot heatmap of log likelihood values\n  ggplot(sums, aes(x = decay_total, y = decay_con,\n                   fill = logLik_scaled)) +\n    geom_tile(width = 0.9, height = 0.9, col = \"black\") +\n    geom_label(data = optimum_by_sp, label = \"X\") +\n    labs(x = \"Decay total density\", y = \"Decay conspecific density\", \n         fill = \"logLik\") +\n    facet_wrap(~sp + density_type, ncol = 2, scales = \"free\") +\n    scale_fill_viridis_c() + # viridis color palette\n    theme_bw(12) + \n    theme(legend.position = \"right\") + \n    labs(fill = \"Log likelihood\\n(scaled)\") + \n    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, \n                                     hjust=1))\n\n\n\n\n\nFigure 2.5: Heatmap of optimal values for decay constants separately for each species\n\n\n\n\nFor this data set, the following are the optimal decay parameter values for each species separately for neighborhood density calculated by abundance (N) and by basal area (BA):\n\n\nCode\nknitr::kable(optimum_by_sp, digits = 2) %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nrun_name\nsp\ndecay_total\ndecay_con\ndensity_type\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\nlogLik_scaled\n\n\n\n\ncappfr_totalexp19_conexp25_BA\ncappfr\nexp19\nexp25\nBA\n11.09\n-8.74\n43.29\n90.63\n17.48\n278.91\n290\n2.9564926\n\n\ncappfr_totalexp03_conexp01_N\ncappfr\nexp03\nexp01\nN\n11.03\n-11.80\n49.64\n97.43\n23.59\n278.97\n290\n2.1291275\n\n\ncordbi_totalexp05_conexp03_BA\ncordbi\nexp05\nexp03\nBA\n7.61\n-12.46\n43.10\n58.90\n24.93\n34.39\n42\n0.2471958\n\n\ncordbi_totalexp01_conexp07_N\ncordbi\nexp01\nexp07\nN\n14.84\n0.00\n31.83\n59.48\n0.00\n27.16\n42\n4.5489731\n\n\ndes2pa_totalexp25_conexp09_BA\ndes2pa\nexp25\nexp09\nBA\n8.36\n-481.80\n984.07\n1037.31\n963.61\n1337.64\n1346\n1.7645474\n\n\ndes2pa_totalexp03_connodecay_N\ndes2pa\nexp03\nnodecay\nN\n7.33\n-481.92\n981.23\n1026.49\n963.84\n1338.67\n1346\n1.6992914\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarber, Cristina, Andrii Zaiats, Cara Applestein, Lisa Rosenthal, and T Trevor Caughlin. 2022. “Bayesian Models for Spatially Explicit Interactions Between Neighbouring Plants.” Methods in Ecology and Evolution.\n\n\nCanham, Charles D, Michael J Papaik, Marı́a Uriarte, William H McWilliams, Jennifer C Jenkins, and Mark J Twery. 2006. “Neighborhood Analyses of Canopy Tree Competition Along Environmental Gradients in New England Forests.” Ecological Applications 16 (2): 540–54.\n\n\nUriarte, Marı́a, Nathan G Swenson, Robin L Chazdon, Liza S Comita, W John Kress, David Erickson, Jimena Forero-Montaña, Jess K Zimmerman, and Jill Thompson. 2010. “Trait Similarity, Shared Ancestry and the Structure of Neighbourhood Interactions in a Subtropical Wet Forest: Implications for Community Assembly.” Ecology Letters 13 (12): 1503–14."
  },
  {
    "objectID": "calculating-marginal-effects.html#overview",
    "href": "calculating-marginal-effects.html#overview",
    "title": "3  Calculating Marginal Effects",
    "section": "3.1 Overview",
    "text": "3.1 Overview\nIn this section, we examine a subset of the Barro Colorado Island (BCI) seedling data to illustrate the impact of conspecific density on mortality probability. We calculate the Average Marginal Effect (more about marginal effects in general here) as our metric of the strength conspecific density dependence. We then estimate both the absolute Average Marginal Effect (AME) and the relative Average Marginal Effect (rAME). The AME represents the average absolute change in mortality probability due to a specified increase in conspecific density. In contrast, the rAME is the relative change in mortality probability compared to a baseline value.\nThe standard approach for modeling plant CDD patterns presumes a linear relationship between performance (e.g., mortality/survival) and conspecific neighborhood density metrics. However, in this section, we use ‘Generalized Additive Models (GAMs)’ as it offer flexibility for non-linear relationships between performance and predictors.\n\n\n\n\n\n\nNote\n\n\n\nNote: The code is adapted from the latitudinalCNDD repository by Lisa Hüelsmann."
  },
  {
    "objectID": "calculating-marginal-effects.html#load-libraries",
    "href": "calculating-marginal-effects.html#load-libraries",
    "title": "3  Calculating Marginal Effects",
    "section": "3.2 Load libraries",
    "text": "3.2 Load libraries\n\n\nCode\n# Load libraries\nlibrary(boot)\nlibrary(broom)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gratia)\nlibrary(here)\nlibrary(kableExtra) \nlibrary(knitr) \nlibrary(lubridate)\nlibrary(mgcViz)\nlibrary(mgcv)\nlibrary(MASS)\nlibrary(parallel)\nlibrary(pbapply)\nlibrary(readr)\nlibrary(skimr)\nlibrary(spatstat.geom)\nlibrary(tidyr)"
  },
  {
    "objectID": "calculating-marginal-effects.html#load-data",
    "href": "calculating-marginal-effects.html#load-data",
    "title": "3  Calculating Marginal Effects",
    "section": "3.3 Load data",
    "text": "3.3 Load data\nOur demonstration utilizes a subset of the Barro Colorado Island (BCI) seedling data, encompassing only 30 species. Please note that this analysis is solely for demonstration purposes; hence, no biological conclusions should be drawn from the results due to the limited data subset used.\nThe code below assumes the data is in a format where each row is an observation for an individual from a census. For this particular data set, the column descriptions are as follows:\n\nId: unique identifier for each seedling\nplot: location\nspp: species\ndate: date of observation\nyear: year of observation\ncensus: census number\nstatus: status at census, 0 = alive, 1 = dead\nheight.last.census: height.last.census\ncon_dens: number of conspecific neighbors\ntotal_dens: number of total neighbors\nhet_dens: number of heterospecifics neighbors\ntime.since.last.census: time interval between censuses\n\n\n\nCode\ndata_BCI &lt;- read_csv(\"./data/BCI_seedling_data_30_spp_2023.csv\")\ncolnames(data_BCI)\n\n\n [1] \"id\"                            \"q20\"                          \n [3] \"plot\"                          \"tag\"                          \n [5] \"spp\"                           \"date\"                         \n [7] \"year\"                          \"census\"                       \n [9] \"status\"                        \"height.last.census\"           \n[11] \"height.last.census.log.scaled\" \"con_dens\"                     \n[13] \"total_dens\"                    \"het_dens\"                     \n[15] \"time.since.last.census\"       \n\n\nCode\n# make sure variables are taken as factor\ndata_BCI$census &lt;- factor(data_BCI$census)\ndata_BCI$spp &lt;- factor(data_BCI$spp)\ndata_BCI$plot &lt;- factor(data_BCI$plot)\ndata_BCI$height=data_BCI$height.last.census\ndata_BCI$interval=as.numeric(data_BCI$time.since.last.census)\n\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\n# Exploring data \n  # visualize \n  ggplot(data_BCI, aes(x = con_dens)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"steelblue2\") +\n  labs(x = \"Conspecific density\", y = \"Count\", \n       title = \"Conspecific densities\") + \n  theme_bw(12)"
  },
  {
    "objectID": "calculating-marginal-effects.html#handling-data-deficient-species",
    "href": "calculating-marginal-effects.html#handling-data-deficient-species",
    "title": "3  Calculating Marginal Effects",
    "section": "3.4 Handling “data deficient” species",
    "text": "3.4 Handling “data deficient” species\nWe categorize a species as data deficient if it has fewer than four unique conspecific density values. At the end of this section of the code, a data frame named ‘nsp’ will be generated. This data frame will classify each species as either “data deficient” or “not data deficient”.\nFrom our data set 9 species out of 30 were assigned as data deficient. Here, we used the arbitrary thresholds of 4 unique values of conspecific densities and a minimum range of 1 for conspecific as our thresholds for what constitutes a data deficient species. These exact values will depend on the data set and the questions of the study, so we suggest thinking carefully about what the appropriate values are for the purposes of a given study.\n\n\nCode\nnval = 4  ## this is the number of 'unique' conspecific values \nminrange = 1    # minimum range for conspecific density\n\ndata_BCI %&gt;% \n  group_by(spp) %&gt;% \n  summarise(\n            range_con_dens = max(con_dens) - min(con_dens),\n            max_con_dens = max(con_dens),\n            unique_con_dens = length(unique(con_dens)),\n            unique_total_dens = length(unique(total_dens)),\n            unique_height = length(unique(height.last.census))\n  ) %&gt;% \n  \n  # check if conspecific less than defined nval\n  mutate(issue_nval = unique_con_dens &lt; nval,\n  # range should at least be equal to minrange\n  issue_range = range_con_dens &lt; minrange,                 \n  trymodel = !(issue_nval|issue_range),\n  # Assignment of \"data deficient\" species\n  data_deficient = !trymodel                                          \n  ) -&gt; nsp # Store the resulting dataframe in the object 'nsp'\n\n# Visualize the top rows of the table 'nsp' in a formatted manner\nnsp %&gt;%\n  head() %&gt;%\n  kable() %&gt;%\n  kable_styling(latex_options = c(\"striped\", \"scale_down\"))\n\n\n\n\n\nspp\nrange_con_dens\nmax_con_dens\nunique_con_dens\nunique_total_dens\nunique_height\nissue_nval\nissue_range\ntrymodel\ndata_deficient\n\n\n\n\nACALDI\n7\n7\n8\n35\n379\nFALSE\nFALSE\nTRUE\nFALSE\n\n\nAEGICE\n4\n4\n5\n28\n499\nFALSE\nFALSE\nTRUE\nFALSE\n\n\nBEILPE\n98\n98\n69\n72\n894\nFALSE\nFALSE\nTRUE\nFALSE\n\n\nCAPPFR\n5\n5\n6\n52\n767\nFALSE\nFALSE\nTRUE\nFALSE\n\n\nCECRIN\n7\n7\n7\n20\n177\nFALSE\nFALSE\nTRUE\nFALSE\n\n\nCORDLA\n4\n4\n5\n32\n457\nFALSE\nFALSE\nTRUE\nFALSE"
  },
  {
    "objectID": "calculating-marginal-effects.html#function-for-fitting-models",
    "href": "calculating-marginal-effects.html#function-for-fitting-models",
    "title": "3  Calculating Marginal Effects",
    "section": "3.5 Function for fitting models",
    "text": "3.5 Function for fitting models\nIn the context of the Janzen-Connell Hypothesis, we focus on the differences between CDD and HDD (‘Stabilizing effect’), where conspecific neighbors’ negative effects surpass those from heterospecifics, leading to population stabilization (Broekman et al. 2019). This effect is vital for estimating species’ self-limitation.\nIn this section we illustrate the conspecific density impact, considering total tree density. The estimated effect (slope) in our analysis corresponds to the result of subtracting HDD from CDD.\nWe use here a Generalized Additive Model (GAM) with a complementary log-log (cloglog) link function to model the seedling status (‘alive’ or ‘dead’) as a function of conspecific density con_dens, total density total_dens and tree height or size, the latter serving as a potential confounder or precision covariate.\nThe cloglog link accounts for differences in observation time Δ𝑡 through an offset term, applicable to datasets where (0=alive and 1=alive)(Currie 2016).\nAfter defining covariates for the GAM models, we specify the smooth term in the model formula. If a k value exceeds the number of unique values in the variable, it is adjusted to be two less than that number. Model convergence and warnings are also checked. which this set up here we define k=2 as minimum as K=1 is a linear relation.\nAfter defining the covariates for the GAMs, we specify the smooth term in the model formula. The k value determines the complexity of the smooth. If k exceeds the number of unique values in a variable, it’s adjusted to be two less than that number. We also monitor model convergence and any warnings that may arise.\n\n\nCode\n# Define a function to fit a model to the given data\nmodel_fit = function(data, spinfo, reduced = F) {\n  \n  # create new factor with correct factor levels per species \n  data$census = factor(data$census)\n  \n  \n  # # Determine if there's more than one unique value in 'census' \n  # and construct the relevant term for the model formula\n  \n  term_c = ifelse(length(unique(data$census)) &gt; 1, \n                  \"+ s(census, bs = 're')\", \"\") \n  #term_p = \"+ s(plot, bs = 're')\"\n  \n  if (reduced) {\n    form =  as.formula(paste0(\"status ~ s(height, k = k1) + \n                              s(total_dens, k = k2)\"\n                              , term_c)) # reduced model #,term_p\n  } else {\n    form =  as.formula(paste0(\"status ~ s(height, k = k1) + \n                              s(total_dens, k = k2) + \n                              s(con_dens, k = k3)\" \n                              , term_c)) # full model #, term_p\n  }\n  \n  # Choose penalty\n  # set to default k=10 \n  k1 = k2 = k3 = 10\n  if (k1 &gt; spinfo$unique_height) k1 = spinfo$unique_height - 2\n  if (k2 &gt; spinfo$unique_total_dens) k2 = spinfo$unique_total_dens - 2\n  if (k3 &gt; spinfo$unique_con_dens) k3 = spinfo$unique_con_dens - 2\n  \n  # k = 1 would force linearity for that term, and we aim to consider \n  # also potential non-linear relationships, so conspecific k is k=2 \n  # minimum.\n  \n # Fit the Generalized Additive Model (GAM)\n  mod = try(gam(form\n                , family = binomial(link=cloglog)\n                , offset = log(interval)\n                , data = data\n                , method = \"REML\"\n  ) , silent = T\n  )\n   # Return the fitted model\n  return(mod)\n  \n}\n\n# check model run\n\n# Define a function to check the convergence of the model\nmodel_convergence = function(model) {\n  \n  # gam not available\n  if (!any(class(model)==\"gam\")) {\n    print(paste(spp, \"gam failed\"))\n  } else {\n    \n    # gam not converged\n    if (!model$converged) {\n      print(paste(spp, \"no convergence\"))\n    } else {\n      \n    \n# Explore warning \"glm.fit: fitted probabilities numerically 0 or 1 \n      # occurred (complete separation)\"\n      eps &lt;- 10 * .Machine$double.eps\n      glm0.resids &lt;- augment(x = model) %&gt;%\n        mutate(p = 1 / (1 + exp(-.fitted)),\n               warning = p &gt; 1-eps,\n               influence = order(.hat, decreasing = T))\n      infl_limit = round(nrow(glm0.resids)/10, 0)\n     \n       # Check if any of the warnings correspond to the 10% most \n      # influential observations. If not, then it is okay.\n\n      num = any(glm0.resids$warning & glm0.resids$influence &lt; \n                  infl_limit)\n      \n    # If there's complete separation\n      if (num) {\n        print(paste(spp, \"complete separation is likely\"))\n      } else {\n        \n       # Check if the Vc component of the model is missing\n        if (is.null(model$Vc)) {\n          print(paste(spp, \"Vc not available\"))\n        } else {\n        \n        # If everything is fine, return the model\n        return(model)\n        }\n      }\n    }\n  }\n}"
  },
  {
    "objectID": "calculating-marginal-effects.html#fit-models",
    "href": "calculating-marginal-effects.html#fit-models",
    "title": "3  Calculating Marginal Effects",
    "section": "3.6 Fit models",
    "text": "3.6 Fit models\nHere we fit the model for all species. data deficient species are treated as one group since there is not sufficient data to be treated independently. In this dataset, we have 9 species that are flagged as ‘data deficient’,\n\n\nCode\n# Check the distribution of species marked as 'data deficient' \n# (T or F) \n# and determine the number of species for which we will try the model \n\n# We have 9 species that are flagged as 'data deficient'\ntable(data_deficient = nsp$data_deficient, trymodel = nsp$trymodel) \n\n\n              trymodel\ndata_deficient FALSE TRUE\n         FALSE     0   21\n         TRUE      9    0\n\n\nCode\n# Convert spp to character\ndata_BCI$spp &lt;- as.character(data_BCI$spp)\n\n# Extract species that are flagged as 'data deficient' from the nsp \n# dataframe\ndata_deficient_species &lt;- nsp$spp[nsp$data_deficient == TRUE]\n\n# Replace species names with \"data_deficient_seedling\" for those \n# flagged as 'data_deficient'\ndata_BCI2 &lt;- data_BCI %&gt;% \n  mutate(spp = ifelse(spp %in% data_deficient_species, \n                      \"data_deficient_seedling\", spp))\n\n# # Summarize attributes for each species in the modified dataframe \n# (including the new #\"data_deficient_seedling\" group)\n\ndata_BCI2 %&gt;%\n  group_by(spp) %&gt;%\n  summarise(\n    range_con_dens = max(con_dens, na.rm = TRUE) - min(con_dens, \n                                                       na.rm = TRUE),\n    max_con_dens = max(con_dens, na.rm = TRUE),\n    unique_con_dens = length(unique(con_dens)),\n    unique_total_dens = length(unique(total_dens)),\n    unique_height = length(unique(height.last.census))\n  ) %&gt;%\n  mutate(\n    # less than nval unique values in consp densities\n    issue_nval = unique_con_dens &lt; nval, \n    # range should at least be equal to minrange\n    issue_range = range_con_dens &lt; minrange,              \n    trymodel = !(issue_nval | issue_range),\n    # preliminary assignment of data_deficient species\n    data_deficient = !trymodel                                      \n  ) -&gt; nsp_data_deficient\n\n\n####\n## Fit model for each species\n###\n\n# Create lists to store results of the main and reduced model fits\n\n# List for main model fits\nres_mod = list()      \n# List for reduced model fits (for calculating Pseudo R2)\nres_red_mod = list()  \n\n\n# Loop through species in the nsp_data_deficient dataframe for which \n# we will try modeling \n# (Here, the group of data_deficient species is treated as a single \n# species)\n  \n  for(spp in nsp_data_deficient$spp[nsp_data_deficient$trymodel]) {\n  \n  # select data for individual species\n  dat_sp = data_BCI2[data_BCI2$spp == spp, ]\n  \n  # Fit the main and reduced models for the current species\n  mod = model_fit(data = dat_sp, \n                  spinfo = nsp_data_deficient[nsp_data_deficient$spp \n                                              == spp, ])\n  mod_red = model_fit(data = dat_sp, \n                  spinfo = nsp_data_deficient[nsp_data_deficient$spp \n                                              == spp, ], reduced = T)\n  \n  # Check the convergence of both models\n  res = model_convergence(model = mod)\n  res_red = model_convergence(model = mod_red)\n  \n  # save result\n  if (is.character(res)) {\n    nsp$data_deficient[nsp$spp == spp] = T  \n  } else {\n    res_mod[[spp]] = res\n    res_red_mod[[spp]] = res_red\n  }\n}"
  },
  {
    "objectID": "calculating-marginal-effects.html#summarize-model-fits",
    "href": "calculating-marginal-effects.html#summarize-model-fits",
    "title": "3  Calculating Marginal Effects",
    "section": "3.7 Summarize model fits",
    "text": "3.7 Summarize model fits\nRegression table via broom::tidy()\n\n\nCode\ncoefs = lapply(res_mod, broom::tidy)\ncoefs = Map(cbind, coefs, sp = names(coefs))\ncoefs = do.call(rbind, coefs)\n\n\nModel summary via broom::glance()\n\n\nCode\n# For each model result in 'res_mod', extract summary statistics \n# (like log-likelihood, AIC, BIC #, etc.) using the 'glance' function\n# from the 'broom' package\n\n# df logLik AIC BIC deviance df.residuals nobs \nsums = lapply(res_mod, broom::glance)\nsums = Map(cbind, sums, sp = names(sums))\nsums = do.call(rbind, sums)\nhead(sums)\n\n\n              df      logLik        AIC        BIC   deviance df.residual  nobs\nACALDI  7.738186   -428.3495   876.0873   924.9256   856.6990   1131.2618  1139\nAEGICE  8.763874   -524.7678  1070.5298  1123.3674  1049.5355   1125.2361  1134\nBEILPE 17.776666 -11142.3710 22323.8580 22478.1537 22284.7420  19697.2233 19715\nCAPPFR 10.232458  -2024.4980  4075.8754  4174.3285  4048.9960  11210.7675 11221\nCECRIN  6.221103   -143.4318   301.0402   326.2521   286.8635    252.7789   259\nCORDLA  6.840230   -531.0591  1079.4468  1125.3893  1062.1182   1477.1598  1484\n           sp\nACALDI ACALDI\nAEGICE AEGICE\nBEILPE BEILPE\nCAPPFR CAPPFR\nCECRIN CECRIN\nCORDLA CORDLA\n\n\nCode\n# AUC\naucs = lapply(res_mod, function(x) {\n  roc &lt;- performance::performance_roc(x, new_data = x$model)\n  bayestestR::area_under_curve(roc$Spec, roc$Sens)\n})\nsums$AUC = unlist(aucs)\n\n\n# Pseudo R2\nsums$pseudoR2 = 1 - (unlist(lapply(res_mod, function(x) x$deviance)) /\n                       unlist(lapply(res_red_mod, function(x) \n                         x$deviance)))"
  },
  {
    "objectID": "calculating-marginal-effects.html#plotting-results",
    "href": "calculating-marginal-effects.html#plotting-results",
    "title": "3  Calculating Marginal Effects",
    "section": "3.8 Plotting results",
    "text": "3.8 Plotting results\n\n\nCode\n# plot splines in pdf ----------------------------------------------\n\n# # Specify the name of the PDF file where the plots will be saved\npdf_file &lt;- \"mortality.pdf\"\n\n# Open the PDF file for writing\npdf(pdf_file)\n\n# Loop through all the model results stored in 'res_mod'\nfor (i in 1:length(res_mod)) {\n  \n  # Get the vizmod for the current species\n  vizmod &lt;- getViz(res_mod[[i]], post = T, unconditional = T)\n  pl &lt;- plot(vizmod, nsim = 20, allTerms = T) + \n    # Add confidence interval line/ fit line/ simulation line\n    l_ciLine() + l_fitLine() + l_simLine() +\n     #Add confidence interval bar/# Add fitted points\n    l_ciBar() + l_fitPoints(size = 1) +  \n    l_rug() +                             # Add rug plot\n    # Add title with the name of the current species\n    labs(title = names(res_mod)[i])       \n  \n  # Print the plot to the R console only for the first 3 species\n  if (i &lt;= 3) {\n    print(pl, pages = 1)\n  }\n  \n  # Save the plot to the PDF\n  print(pl, pages = 1)\n}\n\n# Close the PDF file\ndev.off()\n\n\nquartz_off_screen \n                2"
  },
  {
    "objectID": "calculating-marginal-effects.html#ames-absolute-and-relative",
    "href": "calculating-marginal-effects.html#ames-absolute-and-relative",
    "title": "3  Calculating Marginal Effects",
    "section": "3.9 AMEs Absolute and Relative",
    "text": "3.9 AMEs Absolute and Relative\nIn this section we illustrate the calculation of the average marginal effect and both the absolute Average Marginal Effect (AME) and the relative Average Marginal Effect (rAME).\nHere, AMEs are computed by determining the marginal effect (essentially the partial derivative or slope) of a predictor for a unit change change in conspecific density at each observed value, and then averaging these effects. This method yields a single, interpretable measure that offers an averaged estimate of the predictor’s impact. It’s worth noting that the AME, compared to traditional effect sizes, provides a clearer measure of a predictor’s influence by quantifying the average change in the outcome for each unit change in the predictor rather than the raw coefficient (effect size) that may be influenced by the scale of the variable or confounded by interactions with other predictors.\nFurthermore, rAMEs enhance the level of interpretability by delivering a normalized measure of these averaged effects. They represent the percentage change in the response variable due to a unit change in the predictor relative to the base mortality, providing an intuitive, relative grasp of the predictor’s influence. This normalization process allows for the comparison of effects across different species, each with its own base level of mortality.\nFor AMEs we need the file “res_model” that includes all the models results.\nThere is also alternative approaches for calculating average marginal effects using ‘marginaleffects’ package that we encourage the user to explore.\n\n3.9.1 Settings for AMEs\nHere we provide three scenarios for calculating AMEs or rAMEs through the change argument.\nEquilibrium: This scenario computes AMEs or rAMEs for a unit increase in conspecific density, providing insight into the marginal effect of density increase in an existing ecosystem. Invasion: This scenario models the effects of introducing species into a new habitat, transitioning the conspecific density from zero to a specified unit, helping understand the impact of sudden species introductions. IQR: This scenario evaluates AMEs or rAMEs within the middle 50% range of conspecific density, offering a perspective less influenced by extremes, hence providing a more robust understanding of effects within typical density ranges.\n\n\nCode\n#### chose predictors for local density -setting AMEs\n  \n\n# Define a vector of predictors with their corresponding names\npredictors &lt;- c(con_dens = \"con_dens\", \n                total_dens = \"total_dens\")\n\n# change in conspecific density for AME calculations----\n\n# One more neighbor seedling  \n# or for adult trees: pi*((dbh_neighbor/1000)/2)^2 *\n                        #dec_fun(decay_con,dist_neighbor, decay_type) \nadditive=1     \n  \n# different change settings for con-specific densities\n\ninterval = 1\n\n# Specify how the conspecific density should be changed for \n# different scenarios: \n# 'equilibrium', 'invasion', and 'iqr' (interquartile range)\n\nchange = list(equilibrium = data.frame(con_dens = \n                                         \"paste('+', additive)\")\n              , invasion = data.frame(con_dens = \"c(0, additive)\") \n              , iqr = data.frame(con_dens = \"c(q1, q3)\")\n              )\n\n## Set the number of iterations for any subsequent calculations or \n# simulations\niter = 500 \n\n\n\n\n3.9.2 Functions to calculate AMEs and rAME\nThis section defines two functions. setstep and get_AME. Get_AME is the function calculates the Average Marginal Effects for a given term in a model. The function first creates two copies of the data, d0 and d1, and adjusts the term of interest based on the change argument (one of the three scenarios). It then calculates the predictions for the two data sets and computes the marginal effects.\n\n\nCode\n# Define a function to set the step size for numerical derivatives\n# This function determines an appropriate step size using machine's \n# precision/machine epsilon to strike a balance between accuracy and \n# rounding errors.\n\nsetstep = function(x) {\n  eps = .Machine$double.eps\n  return(x + (max(abs(x), 1, na.rm = TRUE) * sqrt(eps)) - x)\n}\n\n\n# Function to compute Average and relative Marginal Effects \n# (AME and rAME) for a given model--------------------\n\nget_AME = function(mod, data, term\n                   , change = NULL\n                   , at = NULL\n                   , offset = 1\n                   , relative = F\n                   , iterations = 1000\n                   , seed = 10\n                   , samples = F) {\n  \n  # Prepare two dataframes for different scenarios in marginal \n  # effect computation\n  d0 = d1 = data\n  \n # Adjust the 'term' in the data based on the 'change' parameter\n  if (is.null(change)) {\n    \n    # If change is NULL, adjust the term for numerical derivative \n    # computation\n    d0[[term]] = d0[[term]] - setstep(d0[[term]])\n    d1[[term]] = d1[[term]] + setstep(d1[[term]])\n    \n  } \n  \n  # If change has an additive component, adjust the term accordingly\n  if (grepl(\"\\\\+\", paste(change, collapse = \"_\"))) {\n    \n    d1[[term]] = d1[[term]] + as.numeric(gsub(\"\\\\+\", \"\", change))\n    \n  } \n  \n  # If change is explicit with two values, set the term values \n  # directly\n  if (length(change) == 2) {\n    \n    d0[[term]] = as.numeric(change[1])\n    d1[[term]] = as.numeric(change[2])\n    \n  }\n  \n   # If 'at' is specified, set predictor values in the data to these \n  # fixed values\n  # (allows the function to calculate the marginal effects at the \n  # specified values)\n  if (!is.null(at)) {\n    for (i in names(at))\n      d0[[i]] = at[[i]]\n      d1[[i]] = at[[i]]\n  }\n  \n   # Create matrices for prediction based on the model\n  Xp0 &lt;- predict(mod, newdata = d0, type=\"lpmatrix\")\n  Xp1 &lt;- predict(mod, newdata = d1, type=\"lpmatrix\")\n  \n # Extract model parameters\n  ilink &lt;- family(mod)$linkinv\n  beta &lt;- coef(mod)\n  vc &lt;- mod$Vc # covariance matrix \n  \n\n # Compute marginal effects based on the adjusted data\n  pred0   &lt;- 1 - (1-ilink(Xp0 %*% beta))^offset\n  pred1   &lt;- 1 - (1-ilink(Xp1 %*% beta))^offset\n  ME &lt;- (pred1-pred0)\n  \n # Adjust for numerical derivative if change is NULL\n  if (is.null(change)) {\n    ME &lt;- ME/(d1[[term]] - d0[[term]])\n  } \n  \n  # convert to relative if requested\n  if (relative == T) ME = ME/pred0\n  \n  # average marginal effect\n  AME = mean(ME)\n  \n  \n  # Simulate AMEs to compute uncertainty in the estimates\n  \n   # Compute the variance of the average marginal effect through a \n   # \"posterior\" simulation.\n   # This involves simulating from a multivariate normal distribution \n   # using the model's  \n   #coefficient means and covariance matrix\n  \n  if (!is.null(seed)) set.seed(seed)\n  coefmat = mvrnorm(n = iterations\n                    , mu = beta\n                    , Sigma = vc)\n  \n    # For each simulated coefficient vector, estimate the Average \n    # Marginal Effect (AME).\n  AMEs = apply(coefmat, 1, function(coefrow) {\n    \n    # Calculate marginal effects based on the simulated coefficient\n    pred0   &lt;- 1 - (1-ilink(Xp0 %*% coefrow))^offset\n    pred1   &lt;- 1 - (1-ilink(Xp1 %*% coefrow))^offset\n    ME &lt;- (pred1-pred0)\n    \n    # if change is NULL, use numerical derivative\n    if (is.null(change)) {\n      ME &lt;- ME/(d1[[term]] - d0[[term]])\n    } \n    \n    # convert to relative if requested\n    if (relative == T) ME = ME/pred0\n    \n    # average marginal effect\n    AME = mean(ME)\n    return(AME)\n  })\n  \n  # Combine results\n   # If the 'samples' flag is FALSE, return the summary results.\n  # Otherwise, return both the summary and the sample results.\n  \n  if (!samples) {\n    res = data.frame(term\n                     , estimate = AME\n                     , std.error = sqrt(var(AMEs))  \n                     , estimate.sim = mean(AMEs)    \n                     , offset\n                     , change.value = paste(change, collapse = \"_\"))\n    return(res) \n    \n  } else {\n    \n    res_sums = data.frame(term\n                     , estimate = AME\n                     , std.error = sqrt(var(AMEs)) \n                     , offset\n                     , change.value = paste(change, collapse = \"_\"))\n    \n    res_samples = data.frame(term\n                             , estimate = AMEs\n                             , MLE = AME\n                             , offset\n                             , change.value = paste(change, \n                                                    collapse = \"_\"))\n    res = list(res_sums, res_samples)\n    return(res)  \n    \n  }\n}\n\n\n\n\n3.9.3 Calculating Absolute Average Marginal Effect (AMEs)\nAt the end of this code segment, the AME data frame contains average marginal estimates for each predictor, each corresponding to different change scenarios. In essence, AME provides the average effect that a predictor has on the outcome across these scenarios. On the other hand, the AMEsamples data frame contains multiple samples of AME for each predictor, each aligned with a specific type of change scenario, which allows for an evaluation of the uncertainty inherent in the AME estimates.\n\n\nCode\n# Absolute AMEs ---------------------------------------------------\n\n# Initialize empty data frames to store the results\nAME = data.frame()\nAMEsamples = data.frame()\n\n# Loop through predictor names that match \"con_\"\nfor (i in names(predictors)[grepl(\"con_\", names(predictors))]) { \n\n# Loop through different change settings (e.g., equilibrium, invasion,\n  # iqr)\n  for (j in names(change)) {\n    \n# Calculate the AME for each model in res_mod\n    temp = lapply(res_mod, function(x){\n      \n# If the change is based on IQR (interquartile range), calculate the\n      # 1st and 3rd quartiles\n      if (j == \"iqr\") {\n        q1 = quantile(x$model$con_dens, probs = 0.25)\n        q3 = quantile(x$model$con_dens, probs = 0.75)\n      }\n\n# Use the get_AME function to calculate the AME for the current model\n      get_AME(x\n              , data = x$model\n              , offset = interval\n              , term = i\n              , change = eval(parse(text = change[[j]][,i]))\n              , iterations = iter\n              , samples = T\n      )\n    }\n    )\n    \n    # AME\n    tempAME = lapply(temp, function(x) x[[1]])\n    tempAME = Map(cbind, tempAME, change = j, sp = names(tempAME))\n    tempAME = do.call(rbind, tempAME)\n    AME = rbind(AME, tempAME)\n    \n    # AME samples\n    tempSamples = lapply(temp, function(x) x[[2]])\n    tempSamples = Map(cbind, tempSamples, change = j, \n                      sp = names(tempSamples), iter = iter)\n    tempSamples = do.call(rbind, tempSamples)\n    AMEsamples = rbind(AMEsamples, tempSamples)\n  }\n}\nhead(AME)\n\n\n           term    estimate   std.error offset change.value      change     sp\nACALDI con_dens 0.017532523 0.016546708      1          + 1 equilibrium ACALDI\nAEGICE con_dens 0.089367793 0.025930769      1          + 1 equilibrium AEGICE\nBEILPE con_dens 0.006043419 0.001062937      1          + 1 equilibrium BEILPE\nCAPPFR con_dens 0.020944544 0.004310472      1          + 1 equilibrium CAPPFR\nCECRIN con_dens 0.028621858 0.027053981      1          + 1 equilibrium CECRIN\nCORDLA con_dens 0.013464775 0.030623821      1          + 1 equilibrium CORDLA\n\n\n\n\n3.9.4 Calculating Relative Average Marginal Effect (rAMEs)\nAt the end of this code segment, the rAME data frame contains therAME estimates for the predictor and each type of change, and the rAMEsamples data frame contains the rAME samples for each predictor and type of change.\n\n\nCode\n# Relative rAMEs -----------------------------------------------------------\n\n\n# Initialize empty data frames to store the results\nrAME = data.frame()\nrAMEsamples = data.frame()\n\n# Loop through predictor names that match \"con_\" \nfor (i in names(predictors)[grepl(\"con_\", names(predictors))]) { \n  \n# Loop through different change settings \n  # (e.g., equilibrium, invasion, iqr)\n  for (j in names(change)) {\n\n# Calculate the relative AME (rAME) for each model in res_mod\n    temp = lapply(res_mod, function(x){\n      \n# If the change is based on IQR (interquartile range), \n  # calculate the 1st and 3rd quartiles\n      if (j == \"iqr\") {\n        q1 = quantile(x$model$con_dens, probs = 0.25)\n        q3 = quantile(x$model$con_dens, probs = 0.75)\n      }\n# Use the get_AME function to calculate the rAME for the \n  # current model, setting the 'relative' argument to TRUE\n      get_AME(x\n              , data = x$model\n              , offset = interval\n              , term = i\n              , change = eval(parse(text = change[[j]][, i]))\n              , iterations = iter\n              , relative = T\n              , samples = T\n      )\n    }\n    )\n    \n    # rAME\n    tempAME = lapply(temp, function(x) x[[1]])\n    tempAME = Map(cbind, tempAME, change = j, sp = names(tempAME))\n    tempAME = do.call(rbind, tempAME)\n    rAME = rbind(rAME, tempAME)\n    \n    # rAME samples\n    tempSamples = lapply(temp, function(x) x[[2]])\n    tempSamples = Map(cbind, tempSamples, change = j, \n                      sp = names(tempSamples), iter = iter)\n    tempSamples = do.call(rbind, tempSamples)\n    rAMEsamples = rbind(rAMEsamples, tempSamples)\n  }\n}\nhead(rAME)\n\n\n           term   estimate   std.error offset change.value      change     sp\nACALDI con_dens 0.12190548 0.122096554      1          + 1 equilibrium ACALDI\nAEGICE con_dens 0.46332386 0.140120748      1          + 1 equilibrium AEGICE\nBEILPE con_dens 0.02026319 0.003624488      1          + 1 equilibrium BEILPE\nCAPPFR con_dens 0.42931069 0.087277501      1          + 1 equilibrium CAPPFR\nCECRIN con_dens 0.04121409 0.042571758      1          + 1 equilibrium CECRIN\nCORDLA con_dens 0.10440629 0.241547627      1          + 1 equilibrium CORDLA"
  },
  {
    "objectID": "calculating-marginal-effects.html#saving-results",
    "href": "calculating-marginal-effects.html#saving-results",
    "title": "3  Calculating Marginal Effects",
    "section": "3.10 Saving results",
    "text": "3.10 Saving results\n\n\nCode\n# Save results ---------------------------------------------------\nsave(list = c(\"AME\", \"AMEsamples\", \"rAME\", \"rAMEsamples\", \"nsp\", \n              \"coefs\", \"sums\") # \"nsp_data_deficient\"\n     , file = paste0( \"./data/mortality.Rdata\"))\nwrite.csv(AME, \"./data/AME.csv\")\nwrite.csv(rAME, \"./data/rAME.csv\")\n\n\n\n\n\n\n\n\nBroekman, Maarten JE, Helene C Muller-Landau, Marco D Visser, Eelke Jongejans, SJ Wright, and Hans de Kroon. 2019. “Signs of Stabilisation and Stable Coexistence.” Ecology Letters 22 (11): 1957–75.\n\n\nCurrie, Iain D. 2016. “On Fitting Generalized Linear and Non-Linear Models of Mortality.” Scandinavian Actuarial Journal 2016 (4): 356–83."
  },
  {
    "objectID": "metaregression-comparisons.html#overview",
    "href": "metaregression-comparisons.html#overview",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nFollowing from Section 4. How does CDD vary across species, abiotic gradients or in time? in the main text, here, we demonstrate one approach to comparing the strength of CDD across species using a meta-analysis framework. The same approach can be used to compare CDD across sites, plots, or any other unit of interest, as long as it is possible to generate reliable estimates of the strength of CDD (suitable sample sizes, etc.). As noted in the main text, “Correct propagation of uncertainty in CDD estimates requires meta-regressions in frequentist or Bayesian frameworks. Weighted regressions (e.g., lm, lmer, gam) can also estimate how CDD varies e.g., across latitude or between species with different life-history strategies, but incorrectly estimate the associated uncertainty.”\nWe use a subset of the BCI seedling data (Comita et al. 2023) of only 30 species to compare how species abundance is related to the strength of CDD at the species level. This is the same dataset used in section 2. This analysis is for demonstration purposes only, and biological conclusions should not be made about the results, given this is only a small subset of the data.\nAn advantage of using meta-regressions over simple weighted regressions is that the models are able to simultaneously account for uncertainty in species-specific CDD estimates as well as systematic differences in species’ CDD when regressed against a predictor via the inclusion of random effects. A simple weighted regression, on the other hand, assumes that there is only one error for which the relative strength is known when regressing the estimates against a predictor. The latter can lead to incorrect weighting of species in the metaregression.\nIn this tutorial, we use relative average marginal effect (𝑟𝐴𝑀𝐸) calculated in the previous chapter as our response variable, calculated separately for each species. 𝑟𝐴𝑀𝐸 in this case estimates the relative increase in the probability of annual mortality with the addition of one new conspecific neighbor, while keeping total densities constant. Positive numbers indicate a relative increase in mortality with an increase in conspecific density, a signature of NCDD. In principle, any metric of the strength of CDD can be used, though care must be taken to ensure that the metrics are comparable across species and sites (see main text for more information).\nWe use the popular metafor package to fit the meta-regression models.\n\n\n\n\n\n\nNote\n\n\n\nThe following code is adapted from the latitudinalCNDD repository by Lisa Hülsmann."
  },
  {
    "objectID": "metaregression-comparisons.html#load-libraries-and-data",
    "href": "metaregression-comparisons.html#load-libraries-and-data",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.2 Load libraries and data",
    "text": "4.2 Load libraries and data\n\n\nCode\n# Load libraries\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(metafor)\n\n# Load in species abundances for BCI data subset\nabundances &lt;- read_csv(\nhere(\"./data/BCI seedling data - 30 species - abundance 2023_05_18.csv\")\n  )\n\n# Load marginal effects calculations from previous section\nload(here(\"./data/mortality.Rdata\"))\n\n# Subset down to just equilibrium change\nrAME &lt;- rAME %&gt;%\n  filter(change == \"equilibrium\")\n\n# Join marginal effects and abundance data\nrAME &lt;- left_join(rAME, abundances, by = c(\"sp\" = \"spp\"))\n\n# Add in average abundance for 'rare' species\nrAME$abundance[rAME$sp == \"data_deficient_seedling\"] &lt;- \n  mean(abundances$abundance[abundances$spp %in% \n                              nsp$spp[nsp$data_deficient]])\n\n# Log transform abundance to use in models\nrAME$log_abundance &lt;- log(rAME$abundance)\n\n\n\nLet’s take a quick look at the data set we’ll be working with:\n\n\nCode\nhead(rAME, n = 10)\n\n\n       term    estimate   std.error offset change.value      change     sp\n1  con_dens  0.12190548 0.122096554      1          + 1 equilibrium ACALDI\n2  con_dens  0.46332386 0.140120748      1          + 1 equilibrium AEGICE\n3  con_dens  0.02026319 0.003624488      1          + 1 equilibrium BEILPE\n4  con_dens  0.42931069 0.087277501      1          + 1 equilibrium CAPPFR\n5  con_dens  0.04121409 0.042571758      1          + 1 equilibrium CECRIN\n6  con_dens  0.10440629 0.241547627      1          + 1 equilibrium CORDLA\n7  con_dens  0.11658356 0.200294935      1          + 1 equilibrium DAVINI\n8  con_dens  0.01113154 0.123539647      1          + 1 equilibrium DESMAX\n9  con_dens  0.23375216 0.227252222      1          + 1 equilibrium HEISCO\n10 con_dens -0.06498043 0.061086207      1          + 1 equilibrium JUSTGR\n   abundance log_abundance\n1        169      5.129899\n2        153      5.030438\n3       5574      8.625868\n4       1604      7.380256\n5         51      3.931826\n6        248      5.513429\n7         27      3.295837\n8         55      4.007333\n9         56      4.025352\n10       136      4.912655"
  },
  {
    "objectID": "metaregression-comparisons.html#reformat-data-for-model-fitting",
    "href": "metaregression-comparisons.html#reformat-data-for-model-fitting",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.3 Reformat data for model fitting",
    "text": "4.3 Reformat data for model fitting\nFirst, we use the ‘escalc’ function in the metafor package to essentially repackage our data frame into a format used in the meta-regression model fitting. Since we already calculated our effect size (𝑟𝐴𝑀𝐸), we just pass through the 𝑟𝐴𝑀𝐸 estimate and corresponding standard error using the ‘GEN’ option for the ‘measure’ argument, rather than calculating an effect size within the ‘escalc’ function.\n\n\nCode\n# Reformat data for model fitting\n# Set measure to generic, which passes the observed effect sizes or \n# outcomes via the yi argument and the corresponding sampling \n# variances via the vi argument (or the standard errors via the sei \n# argument) to the function.\n    dat_meta = metafor::escalc(measure = \"GEN\", \n                               yi = estimate, # observed outcomes\n                               sei = std.error, # standard errors\n                               slab = sp, # label for species\n                               data = rAME)"
  },
  {
    "objectID": "metaregression-comparisons.html#fit-meta-regression-model",
    "href": "metaregression-comparisons.html#fit-meta-regression-model",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.4 Fit meta-regression model",
    "text": "4.4 Fit meta-regression model\nNext, we use the ‘rma’ function to fit a meta-regression model, where 𝑟𝐴𝑀𝐸 is our response variable (renamed as yi in the previous step) and log species abundance is our predictor. While not shown here, it is possible to fit mixed effects meta-regression models with the ‘rma.mv’ function. We suggest consulting the extensive documentation for the metafor package for further details.\n\n\nCode\n# Fit model\nmetamod = metafor::rma(yi = yi,\n                       vi = vi,\n                       mods = ~ log_abundance,\n                       method = \"REML\",\n                       data = dat_meta)"
  },
  {
    "objectID": "metaregression-comparisons.html#print-model-summary",
    "href": "metaregression-comparisons.html#print-model-summary",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.5 Print model summary",
    "text": "4.5 Print model summary\n\n\nCode\nsummary(metamod)\n\n\n\nMixed-Effects Model (k = 22; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n  2.2364   -4.4727    1.5273    4.5145    3.0273   \n\ntau^2 (estimated amount of residual heterogeneity):     0.0166 (SE = 0.0090)\ntau (square root of estimated tau^2 value):             0.1287\nI^2 (residual heterogeneity / unaccounted variability): 91.39%\nH^2 (unaccounted variability / sampling variability):   11.62\nR^2 (amount of heterogeneity accounted for):            0.00%\n\nTest for Residual Heterogeneity:\nQE(df = 20) = 74.0420, p-val &lt; .0001\n\nTest of Moderators (coefficient 2):\nQM(df = 1) = 0.3751, p-val = 0.5402\n\nModel Results:\n\n               estimate      se     zval    pval    ci.lb   ci.ub    \nintrcpt          0.1935  0.1401   1.3815  0.1671  -0.0810  0.4681    \nlog_abundance   -0.0141  0.0231  -0.6125  0.5402  -0.0594  0.0311    \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "metaregression-comparisons.html#plot-the-estimated-of-rame-for-all-species-with-a-forest-plot",
    "href": "metaregression-comparisons.html#plot-the-estimated-of-rame-for-all-species-with-a-forest-plot",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.6 Plot the estimated of rAME for all species with a forest plot",
    "text": "4.6 Plot the estimated of rAME for all species with a forest plot\nIn this case, a forest plot shows the estimates of the strength of CDD for individual species, here ordered by least to most abundant going from top to bottom.\n\n\nCode\nforest(metamod, \n       header = \"Species\", \n       xlab = \"rAME\",\n       order = log_abundance)"
  },
  {
    "objectID": "metaregression-comparisons.html#model-diagnostics",
    "href": "metaregression-comparisons.html#model-diagnostics",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.7 Model diagnostics",
    "text": "4.7 Model diagnostics\nThe plot method displays model diagnostics of the meta-regression model (more info here) in addition to a forest plot.\n\n\nCode\nplot(metamod)"
  },
  {
    "objectID": "metaregression-comparisons.html#model-predictions-of-how-species-abundance-is-related-to-strength-of-cdd",
    "href": "metaregression-comparisons.html#model-predictions-of-how-species-abundance-is-related-to-strength-of-cdd",
    "title": "4  Using Meta-regressions to Compare CDD across Species or Sites",
    "section": "4.8 Model predictions of how species abundance is related to strength of CDD",
    "text": "4.8 Model predictions of how species abundance is related to strength of CDD\nHere, we generate predictions and corresponding confidence intervals for how our predictor of interest is related to the related to strength of CDD using the ‘predict’ function. The y-axis here indicates the relative increase in annual mortality probablity with the addition of one conspecific neighbor. Higher values indicate stronger negative conspecific density dependence. In this example, our predictor of interest is species abundance. We also scale the size of the points based on their weight in the meta-analysis, with larger points indicating higher weights. Note that this analysis is for demonstration purposes only, and biological conclusions should not be made about the results, given this is only a small subset of the data.\n\n\nCode\n# Generate a prediction dataframe\npred &lt;- expand_grid(log_abundance = seq(min(dat_meta$log_abundance, \n                                            na.rm = TRUE), \n                                        max(dat_meta$log_abundance, \n                                            na.rm = TRUE),\n                                        length.out = 50))\n\npred$abundance &lt;- exp(pred$log_abundance) # Back transform abundance\n\n# Bind predictions to dataframe\npred &lt;- cbind(pred, predict(object = metamod, \n                            newmods = pred$log_abundance))\n\n# Extract observed values\nobserved_values &lt;- broom::augment(metamod)\n\n# Add in variance estimates to be able to scale size of points by \n# amount of variance in estimate\nobserved_values &lt;- left_join(observed_values,\n                             dat_meta %&gt;% \n                               dplyr::select(sp, vi, log_abundance),\n                             by = c(\".rownames\" = \"sp\"))\n\n\n\n# Set abundance values for x axis\nabundances_x_axis &lt;- c(25, 50, 100, 1000, 5000)\n\n# Plot prediction\nggplot(pred, aes(x = log_abundance, y = pred)) + \n  geom_ribbon(aes(ymin = ci.lb, ymax = ci.ub), \n              fill = \"steelblue2\", alpha = 0.75) + \n  geom_line() + \n  geom_hline(yintercept = 0, lty = 2) + \n  labs(x = \"Species Abundance\", y = \"rAME of CDD\") + \n  scale_x_continuous(breaks = log(abundances_x_axis),\n                     labels = abundances_x_axis) + \n  # Add observed points\n  geom_point(data = observed_values,\n             aes(x = log_abundance, y = .observed, size = 1/vi), \n             alpha = 0.75) + \n  theme_bw(15) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nComita, Liza S, Salomón Aguilar, Stephen P Hubbell, and Rolando Pérez. 2023. “Long-Term Seedling and Small Sapling Census Data from the b Arro c Olorado i Sland 50 Ha f Orest d Ynamics p Lot, p Anama.” Ecology 104 (9): e4140."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barber, Cristina, Andrii Zaiats, Cara Applestein, Lisa Rosenthal, and T\nTrevor Caughlin. 2022. “Bayesian Models for Spatially Explicit\nInteractions Between Neighbouring Plants.” Methods in Ecology\nand Evolution.\n\n\nBroekman, Maarten JE, Helene C Muller-Landau, Marco D Visser, Eelke\nJongejans, SJ Wright, and Hans de Kroon. 2019. “Signs of\nStabilisation and Stable Coexistence.” Ecology Letters\n22 (11): 1957–75.\n\n\nCanham, Charles D, Michael J Papaik, Marı́a Uriarte, William H\nMcWilliams, Jennifer C Jenkins, and Mark J Twery. 2006.\n“Neighborhood Analyses of Canopy Tree Competition Along\nEnvironmental Gradients in New England Forests.” Ecological\nApplications 16 (2): 540–54.\n\n\nComita, Liza S, Salomón Aguilar, Stephen P Hubbell, and Rolando Pérez.\n2023. “Long-Term Seedling and Small Sapling Census Data from the b\nArro c Olorado i Sland 50 Ha f Orest d Ynamics p Lot, p Anama.”\nEcology 104 (9): e4140.\n\n\nCurrie, Iain D. 2016. “On Fitting Generalized Linear and\nNon-Linear Models of Mortality.” Scandinavian Actuarial\nJournal 2016 (4): 356–83.\n\n\nUriarte, Marı́a, Nathan G Swenson, Robin L Chazdon, Liza S Comita, W John\nKress, David Erickson, Jimena Forero-Montaña, Jess K Zimmerman, and Jill\nThompson. 2010. “Trait Similarity, Shared Ancestry and the\nStructure of Neighbourhood Interactions in a Subtropical Wet Forest:\nImplications for Community Assembly.” Ecology Letters 13\n(12): 1503–14."
  }
]