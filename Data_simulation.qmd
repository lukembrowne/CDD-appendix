---
output: html_document
editor_options: 
  chunk_output_type: console
execute: 
  cache: true
---

## Overview

In this section, we aim to generate simulated seedling census data based on a predefined 'truth' (e.g., fixed conspecific effect). After simulating the data, we will fit a model to it and strive to recover the underlying 'truth.' We invite readers to actively engage with the simulated data. The provided code is designed to be flexible, enabling users to modify conditions and explore further potential limitations through independent experimentation.

The **AME** represents the average absolute change in mortality probability due to an increase in conspecific density. In contrast, the **rAME** offers a more nuanced perspective by expressing this change as a proportion of the base predicted probability for each species.

```{r, message=FALSE}
library(boot)
library(broom)
library(dplyr)
library(ggplot2)
library(gratia)
library(here)
library(kableExtra) 
library(knitr) 
library(lubridate)
library(mgcViz)
library(mgcv)
library(MASS)
library(readr)
library(skimr)
library(stringr)
library(tidyr)
library(purr)
library(foreach)
library(doParallel)
```

## Data simulation

We start by setting parameter values for our simulated data. You could change these parameter to test other observed 'truth'.

```{r, message=FALSE}

# Set seed for reproducibility
set.seed(42)

# Set parameters
n_species <- 5
max_ind_per_plot <- 50
beta_con_dens <- 0.03 # fixed conspecific effect/AME
n_census <- 6
n_plots <- 80
n_sites <- 1
species_sd <- 0.002 


# Create names
spp_names <- paste0("SPP", sprintf("%03d", 1:n_species))
plot_names <- paste0("P", sprintf("%04d", 1:n_plots))
census_names <- paste0("C", sprintf("%02d", 1:n_census))
site_names <- paste0("S", sprintf("%02d", 1:n_sites))

# Adjusted simulation function
adjusted_simulate_data <- function() {
  
  data <- data.frame()
  
  for (plot in plot_names) {
    for (census in census_names) {
      
      remaining_total_dens <- max_ind_per_plot
      current_census_data <- data.frame()
      
      used_species <- sample(spp_names, sample(3:5, 1), replace = FALSE)
      
      for (spp in used_species) {
        available_species <- setdiff(spp_names, used_species)
        
        # Only attempt to change species if there are available species left
        if (length(available_species) > 0 && runif(1) < 0.2) {
          spp <- sample(available_species, 1)
        }
        
        if (remaining_total_dens < 3) break
        
        # Ensure we don't go over max_ind_per_plot in the last loop
        total_dens_for_individual <- sample(2:min(30, remaining_total_dens), 1)
        
        con_dens_for_individual <- total_dens_for_individual - 1
        height_for_individual <- rnorm(1, 0.85, 0.05)
        interval_for_individual <- runif(1, 0.95, 1.05)

        # Generate species-specific effects around the community effect
        species_effects <- abs(rnorm(n_species, 0, species_sd))
        names(species_effects) <- spp_names

        linear_pred <- (beta_con_dens + species_effects[spp]) * con_dens_for_individual - 0.20 * height_for_individual - 1.5
        prob_mortality <- exp(linear_pred) / (1 + exp(linear_pred))
        status_for_individual <- rbinom(1, 1, prob_mortality)
        
        current_species_data <- data.frame(
          spp = spp,
          plot = plot,
          site = sample(site_names, 1),
          census = census,
          height = height_for_individual,
          total_dens = total_dens_for_individual,
          con_dens = con_dens_for_individual,
          interval = interval_for_individual,
          status = status_for_individual
        )
        
        current_census_data <- rbind(current_census_data, current_species_data)
        remaining_total_dens <- remaining_total_dens - total_dens_for_individual
      }
      
      data <- rbind(data, current_census_data)
    }
  }
  
  return(data)
}

# Simulate the adjusted data
data <- adjusted_simulate_data()

# Verify that the sum of total_dens by plot and census is at most max_ind_per_plot
plot_census_totals <- aggregate(total_dens ~ plot + census, adjusted_data, sum)
if (any(plot_census_totals$total_dens > max_ind_per_plot)) {
  print("Warning: Some plot-census combinations exceed the maximum total_dens.")
} else {
  print("Data generation successful!")
}

hist(adjusted_data$con_dens)
table(adjusted_data$spp)


```

## Set sample sizes and parameter values

Here, we set the sample sizes and parameter values for the simulated data, determining what is the 'truth'

```{r Setting sample sizes and parameters, message=FALSE}

# Set sample sizes
  max_ind_per_plot <- 40   # Maximum number of individuals of each species per plot
  n_species <- 10 # Number of species in overall data set
  n_census <- 5 # Number of censuses in data set
  n_plots <- 25 # Number of seedling plots nested within each site
  n_sites <- 1 # Number of sites
  
# Set parameter values (response will be on logit scale)
  u_intercept <- 1.8 # Overall intercept

# Variances of random effects
  sigma_species     <-  .25 # amount of variation across species
  sigma_census      <-  .25 # amount of variation across censuses
  sigma_site        <-  .25 # amount of variation across sites
  sigma_plot        <-  .25 # amount of variation across plots
  sigma_noise       <-  .10 # Residual noise - increasing this adds more noise to the data
  
# Set strength of NDD effects on logit scale  
  beta_NDD <- -1.0 # Community-wide non-linear NDD effects  
  beta_NDD2 <- -0.4 # Quadratic term for non-linear NDD effects
  
  beta_height <- -0.35 # Overall effect of height across species

```

## Simulate variation in seedling densities across plots

We start with a maximum data set with 'max_ind_per_plot' individuals per species at each plot and then subset this maximal data set later to create variation in densities across plots

```{r Initializing dataset, message=FALSE}

# Create names for species, plots, censuses, and sites  
  spp_names <- paste0("SPP", str_pad(1:n_species, width = 3, pad = 0))
  plot_names <- paste0("P", str_pad(1:n_plots, width = 4, pad = 0))
  census_names <-  paste0("C", str_pad(1:n_census, width = 2, pad = 0))
  site_names <-  paste0("S", str_pad(1:n_sites, width = 2, pad = 0))

# Use expand.grid to start to build out dataset
 dat.sim <-  expand.grid( ind_num = 1:max_ind_per_plot,
                                spp = spp_names,
                                plot = plot_names,
                                site = site_names)
 
# Create unique IDs for each individual 
 dat.sim$id <- paste0("IND", str_pad(1:nrow(dat.sim), width = 5, pad = 0))
 
 
# Expand out censuses so that each individual gets censused n_census times
 dat.sim <- left_join(dat.sim , 
                            expand.grid(id = dat.sim$id,
                                        census = census_names)) %>%
                  dplyr::select(id, everything()) %>%
                  as_tibble()


# Create variation in densities across plots
  # For each species plot combination, sample just a fraction of the seedlings that are there
 
  # Using a beta function for the sampling distribution to weight towards lower density plots
    # Can try out different shapes with this code: 
    # hist(rbeta(1000, shape1 = .25, shape2 = 1.5), breaks = 50)
 
  dat.sim <- dat.sim %>%
    group_by(spp, plot, census, site) %>% # Group by species and plot
    nest() %>% # Turns data into a list-column that can be sampled
    ungroup() %>%
    mutate(fraction_to_keep = rbeta(nrow(.), # Generate what fraction of total maximum # of seedlings to keep at each plot
                                    shape1 = .25, 
                                    shape2 = 1.5)) %>% 
    mutate(samp = purrr::map2(data, fraction_to_keep, sample_frac)) %>% # Sampling happens here
    dplyr::select(-c(data, fraction_to_keep)) %>% # Clean up and unnest
    unnest(samp)


# Create unique IDs for plots nested within sites
 dat.sim$plot <- paste0(dat.sim$site, "-", dat.sim$plot)

 # Add species.census column
 dat.sim$species.census <- paste0(dat.sim$spp, "-", dat.sim$census)
 
 # Take a look at the data set
 dat.sim
 

 # Print out stats about simulated data set

  cat("Final # observations in dataset: ", nrow(dat.sim), " ...
      Final # individuals in dataset: ", length(table(dat.sim$id)), " ...
      Final # species in dataset: ", length(table(dat.sim$spp)), " ...
      Final # plots in dataset: ", length(table(dat.sim$plot)), " ...
      Final # sites in dataset: ", length(table(dat.sim$site)), " ...
      Final # censuses in dataset: ", length(table(dat.sim$census)), " ...")

```

## Calculate con-specific density at each plot

```{r Calculate conspecific density, message=FALSE}
# Calculate conspecific density at each plot and add as 'con_dens' column
dat.sim <- dat.sim %>%
  group_by(spp, census, plot, site) %>%
  add_tally(name = "con_dens") %>% # Counts individuals in plot
  mutate(con_dens = con_dens - 1) %>% # Subtract by 1 to remove counting self
  ungroup() %>%
  group_by(census, plot, site) %>%
  add_tally(name = "total_dens") %>% # Counts total seedlings
  ungroup()

# Scale con_dens
dat.sim$con_dens_scaled <- scale(dat.sim$con_dens)[, 1]


# Plot histogram of conspecific densities
ggplot(dat.sim, aes(x = con_dens)) +
  geom_histogram(binwidth = 1, color = "black", fill = "steelblue2") +
  labs(x = "Conspecific density", y = "Count", title = "Conspecific densities") + 
  theme_bw(12)

```

## Plot effect of CDD on survival

The red line shows survival plotted as a linear function of conspecific density, while the blue line shows survival as a non-linear, quadratic function of conspecific density.

```{r Simulate effect of internannual variation in NDD on survival}


# Simulate overall NDD response averaged across all species
  dat.sim$NDD.effect <- beta_NDD
  dat.sim$NDD2.effect <- beta_NDD2 # Quadratic term for non-linear effects


# Plot community-wide effect of NDD on survival
  p = ggplot(dat.sim, aes(x = con_dens_scaled, y = con_dens_scaled * NDD.effect + con_dens_scaled^2 * NDD2.effect)) + 
      geom_line(col = "steelblue2", lwd = 2) +
      geom_line(aes(x = con_dens_scaled, y = con_dens_scaled * NDD.effect), col = "red", lwd = 2) +
      labs(x = "Conspecific density (scaled)", y = "Marginal effect on survival (logit scale)", title = "Effect of NDD on survival") +
      theme_bw()
  
  p
  
```

```{r Simulate heights at previous census for each observation}
 # Add random height
 dat.sim$s_height <- rnorm(nrow(dat.sim), mean = 0, sd = 1)

 # Generate height effect
  dat.sim$height.effect <- beta_height
  
  #interval betwen census
 dat.sim$interval <- rnorm(nrow(dat.sim), mean = 1, sd = 0.1)
  
# Plot histogram of heights
ggplot(dat.sim, aes(x = s_height)) +
  geom_histogram(color = "black", fill = "steelblue2") +
  labs(x = "Height at last census", y = "Count", title = "Height at last census") + 
  theme_bw(12)
  
```

## Simulating random intercepts

Here, we simulate random effects for species, census, plots, and sites

```{r Simulating species-level intercepts, message=FALSE}
 # Generate species intercepts 
  dat.sim <- dat.sim %>% 
    distinct(spp) %>% 
    mutate(spp.effect = rnorm(nrow(.), mean = 0, sd = 1)) %>%
    mutate(spp.effect = (spp.effect - mean(spp.effect)) / 
             (( 1 / sigma_species) * sd(spp.effect))) %>% # Rescale to get actual simulated sigma
    left_join(., dat.sim)

# Generate census effects
    dat.sim <- dat.sim %>% 
      distinct(census) %>% 
      mutate(census.effect = rnorm(nrow(.), mean = 0, sd = 1)) %>%
      mutate(census.effect = (census.effect - mean(census.effect)) / 
                             (( 1 / sigma_census) * sd(census.effect))) %>% # Rescale to get actual simulated sigma
      left_join(., dat.sim)
 
# Generate plot effects
  dat.sim <- dat.sim %>% 
    distinct(plot) %>% 
    mutate(plot.effect = rnorm(nrow(.), mean = 0, sd = 1)) %>%
    mutate(plot.effect = (plot.effect - mean(plot.effect)) / 
             (( 1 / sigma_plot) * sd(plot.effect))) %>% # Rescale to get actual simulated sigma
    left_join(., dat.sim)
  

# Generate site effects
  dat.sim <- dat.sim %>% 
    distinct(site) %>% 
    mutate(site.effect = rnorm(nrow(.), mean = 0, sd = 1)) %>%
    mutate(site.effect = (site.effect - mean(site.effect)) / 
             (( 1 / sigma_site) * sd(site.effect))) %>% # Rescale to get actual simulated sigma
    left_join(., dat.sim)

# If simulating only 1 site, set this effect to 0
if(n_sites == 1){
  dat.sim$site.effect <- 0
} 
```

## Simulate residual noise

We add some residual noise to the data to simulate the fact that it is very unlikely that we have perfect knowledge of all the factors generating our response variable in a real data set.

```{r Simulate residual noise}
  dat.sim$noise <- rnorm(nrow(dat.sim), mean = 0, sd = sigma_noise)

```

## Calculate survival probability based on parameters above

Based on the parameters set above, we calculate the probability of survival using both the linear and quadratic form. So far each seedling, there are two separate survival probabilities - one based on survival being a linear function of CDD and one based on survival being a non-linear, quadratic function of CDD.

```{r Simulate survival probability based on parameters above, message=FALSE}

# First calculate effect on logit scale
  dat.sim$surv.logit <- with(dat.sim, # using 'with' function here to condense code
                                   c(u_intercept + 
                                     spp.effect + 
                                     census.effect +
                                     plot.effect +
                                     site.effect +
                                     s_height+
                                     noise))

  # Add in NDD effects
   dat.sim$surv.logit.linear =  with(dat.sim, surv.logit + 
                                   NDD.effect * con_dens_scaled)
   
    dat.sim$surv.logit.quadratic =  with(dat.sim, surv.logit + 
                                   NDD.effect * con_dens_scaled +
                                   NDD2.effect * con_dens_scaled^2 )

  # Convert from logit scale to probability of survival 
   dat.sim$surv.prob.linear <- plogis(dat.sim$surv.logit.linear)
   dat.sim$surv.prob.quadratic <- plogis(dat.sim$surv.logit.quadratic)

```

## Determine survival status

To determine whether each seedling lived or died across the census interval, we draw from a binomial distribution where 1 = alive, 0 = dead, based on the calculated survival probability

```{r Determining if seedling is alive or dead}
# Determine if alive or dead
  dead.sim.linear <- foreach(i = 1:nrow(dat.sim), .combine = "c") %dopar% 
                rbinom(n = 1, size = 1, prob = dat.sim$surv.prob.linear[i])

  dead.sim.quadratic <- foreach(i = 1:nrow(dat.sim), .combine = "c") %dopar% 
                rbinom(n = 1, size = 1, prob = dat.sim$surv.prob.quadratic[i])

  # Save in main dataframe
  dat.sim$status.linear <- dead.sim.linear
  dat.sim$status.quadratic <- dead.sim.quadratic


  # Summary stats - number of alive and dead
  cat("Survival rate (linear): ", round(sum(dat.sim$status.linear == 1)/nrow(dat.sim), 2)*100, "%")
  cat("Survival rate (quadratic): ", round(sum(dat.sim$status.quadratic == 1)/nrow(dat.sim), 2)*100, "%")

```

## Function for fitting models

We use here a Generalized Additive Model (GAM) with a complementary log-log (cloglog) link function to model the seedling status ('alive'=0 or 'dead'=1) as a function of conspecific density **con_dens**, total density **total_dens** and tree height or size, the latter serving as a potential confounder or precision covariate.

```{r, message=FALSE, warning=FALSE}


model_fit = function(data, speciesinfo, reduced = F) {
  
  # create new factor with correct factor levels per species 
  data$census = factor(data$census)
  
  
  # create model formula
  term_c = ifelse(length(unique(data$census)) > 1, "+ s(census, bs = 're')", "") 
  #term_p = "+ s(plot, bs = 're')"
  
  if (reduced) {
    form =  as.formula(paste0("status ~ s(height, k = k1) + s(total_dens, k = k2)"
                              , term_c)) # reduced model #,term_p
  } else {
    form =  as.formula(paste0("status ~ s(height, k = k1) + s(total_dens, k = k2)  + s(con_dens, k = k3)" 
                              , term_c)) # full model #, term_p
  }
  
  # Choose penalty
  # set to default k=10 
  k1 = k2 = k3 = 10
  if (k1 > speciesinfo$unique_height) k1 = speciesinfo$unique_height - 2
  if (k2 > speciesinfo$unique_total_dens) k2 = speciesinfo$unique_total_dens - 2
  if (k3 > speciesinfo$unique_con_dens) k3 = speciesinfo$unique_con_dens - 2
  
  
  # Fit model
  mod = try(gam(form
                , family = binomial(link=cloglog)
                , offset = log(interval)
                , data = data
                , method = "REML"
  ) , silent = T
  )
  
  return(mod)
  
}

# check model run

model_convergence = function(model) {
  
  # gam not available
  if (!any(class(model)=="gam")) {
    print(paste(spp, "gam failed"))
  } else {
    
    # gam not converged
    if (!model$converged) {
      print(paste(spp, "no convergence"))
    } else {
      
    
# Explore warning "glm.fit: fitted probabilities numerically 0 or 1 occurred (complete separation)"
      eps <- 10 * .Machine$double.eps
      glm0.resids <- augment(x = model) %>%
        mutate(p = 1 / (1 + exp(-.fitted)),
               warning = p > 1-eps,
               influence = order(.hat, decreasing = T))
      infl_limit = round(nrow(glm0.resids)/10, 0)
      # check if none of the warnings is among the 10% most influential observations, than it is okay..
      num = any(glm0.resids$warning & glm0.resids$influence < infl_limit)
      
      # complete separation
      if (num) {
        print(paste(spp, "complete separation is likely"))
      } else {
        
        # missing Vc
        if (is.null(model$Vc)) {
          print(paste(spp, "Vc not available"))
        } else {
        
        # successful model
        return(model)
        }
      }
    }
  }
}

```

## Handling "data deficient" species

In our simulating data set, we did not generated "data deficient" species. In the following section, we further evaluate to confirm this assumption

```{r, message=FALSE, warning=FALSE}
nval = 4  ## this is the number of 'unique' conspecific values
minrange = 1    

data %>% 
  dplyr::group_by(spp) %>% 
  summarise(
            range_con_dens = max(con_dens) - min(con_dens),
            max_con_dens = max(con_dens),
            unique_con_dens = length(unique(con_dens)),
            unique_total_dens = length(unique(total_dens)),
            unique_height = length(unique(height))
  ) %>% 
  
  
  mutate(issue_nval = unique_con_dens < nval,              
  issue_range = range_con_dens < minrange,                  
  trymodel = !(issue_nval|issue_range),    
  data_deficient = !trymodel                                   
  ) -> nsp

# visualize table
nsp %>%
  head() %>%
  kable(format = "html", table.attr = "class='table table-striped'") %>%
  kable_styling()
```

## Fitting models

In this section, we fitted a model for each species in our data set, using the approach described in the previous chapter. The results of the model fitting will be stored in the res_red_mod and res_mod lists.

```{r, message=FALSE, warning=FALSE}

table(data_deficient = nsp$data_deficient, trymodel = nsp$trymodel) # 
data$spp <- as.character(data$spp)
data_deficient_species <- nsp$spp[nsp$data_deficient == TRUE]

data <- data %>%
  mutate(spp = ifelse(spp %in% data_deficient_species, "data_deficient_seedling", spp))


data %>%
  group_by(spp) %>%
  summarise(
    range_con_dens = max(con_dens, na.rm = TRUE) - min(con_dens, na.rm = TRUE),
    max_con_dens = max(con_dens, na.rm = TRUE),
    unique_con_dens = length(unique(con_dens)),
    unique_total_dens = length(unique(total_dens)),
    unique_height = length(unique(height))
  ) %>%
  mutate(
    issue_nval = unique_con_dens < nval,                  
    issue_range = range_con_dens < minrange,              
    trymodel = !(issue_nval | issue_range),               
    data_deficient = !trymodel                                     
  ) -> nsp_data_deficient


####
## Fit model for each species
###

res_mod = list()      # main model fits
res_red_mod = list()  # reduced model fits for Pseudo R2

  
  for(spp in nsp_data_deficient$spp[nsp_data_deficient$trymodel]) {
  
  # select data for individual species
  dat_sp = data[data$spp == spp, ]
  
  # model fit and reduced fit for Pseudo R2
  mod = model_fit(data = dat_sp, speciesinfo = nsp_data_deficient[nsp_data_deficient$spp == spp, ])
  mod_red = model_fit(data = dat_sp, speciesinfo = nsp_data_deficient[nsp_data_deficient$spp == spp, ], reduced = T)
  
  # check model success
  res = model_convergence(model = mod)
  res_red = model_convergence(model = mod_red)
  
  # save result
  if (is.character(res)) {
    nsp$data_deficient[nsp$spp == spp] = T  
  } else {
    res_mod[[spp]] = res
    res_red_mod[[spp]] = res_red
  }
}

```

## Plotting models

```{r}
# Set the PDF file name
pdf_file <- "mortality_simulated.pdf"

# Open the PDF file for writing
pdf(pdf_file)

# Loop through all species in res_mod
for (i in 1:length(res_mod)) {
  
  # Get the vizmod for the current species
  vizmod <- getViz(res_mod[[i]], post = T, unconditional = T)
  pl <- plot(vizmod, nsim = 20, allTerms = T) + 
    l_ciLine() + l_fitLine() + l_simLine() + 
    l_ciBar() + l_fitPoints(size = 1) + 
    l_rug() + 
    labs(title = names(res_mod)[i]) 

  # Save the plot to the PDF
  print(pl, pages = 1)
}

# Close the PDF file
dev.off()
```

## Setting AMEs We are only extracting **AME** and **rAME** values for con_dens with a change +1

```{r}

#### chose predictors for local density -setting AMEs
  
predictors <- c(con_dens = "con_dens", 
                total_dens = "total_dens")

# change in conspecific density for AME calculations----

additive=1     # One more neighbor or for adult trees: pi*((dbh_neighbor/1000)/2)^2 *
                                        #dec_fun(decay_con,dist_neighbor, decay_type) 
  
# different change settings for con-specific densities

interval = 1

change = list(equilibrium = data.frame(con_dens = "paste('+', additive)"))
iter = 500

```

## Setting up a function to calculate AMEs and rAMEs

```{r}
setstep = function(x) {
  eps = .Machine$double.eps
  return(x + (max(abs(x), 1, na.rm = TRUE) * sqrt(eps)) - x)
}


# Function to compute Average and relative Marginal Effects (AME and rAME) for a given model--------------------

get_AME = function(mod, data, term
                   , change = NULL
                   , at = NULL
                   , offset = 1
                   , relative = F
                   , iterations = 1000
                   , seed = 10
                   , samples = F) {
  
    # Prepare two dataframes for different scenarios in marginal effect computation
  d0 = d1 = data
  
 # Adjust the 'term' in the data based on the 'change' parameter
  if (is.null(change)) {
    
    # If change is NULL, adjust the term for numerical derivative computation
    d0[[term]] = d0[[term]] - setstep(d0[[term]])
    d1[[term]] = d1[[term]] + setstep(d1[[term]])
    
  } 
  
  # If change has an additive component, adjust the term accordingly
  if (grepl("\\+", paste(change, collapse = "_"))) {
    
    d1[[term]] = d1[[term]] + as.numeric(gsub("\\+", "", change))
    
  } 
  
  # If change is explicit with two values, set the term values directly
  if (length(change) == 2) {
    
    d0[[term]] = as.numeric(change[1])
    d1[[term]] = as.numeric(change[2])
    
  }
  
   # If 'at' is specified, set predictor values in the data to these fixed values
  # (allows the function to calculate the marginal effects at the specified values)
  if (!is.null(at)) {
    for (i in names(at))
      d0[[i]] = at[[i]]
      d1[[i]] = at[[i]]
  }
  
   # Create matrices for prediction based on the model
  Xp0 <- predict(mod, newdata = d0, type="lpmatrix")
  Xp1 <- predict(mod, newdata = d1, type="lpmatrix")
  
 # Extract model parameters
  ilink <- family(mod)$linkinv
  beta <- coef(mod)
  vc <- mod$Vc # covariance matrix 
  

 # Compute marginal effects based on the adjusted data
  pred0   <- 1 - (1-ilink(Xp0 %*% beta))^offset
  pred1   <- 1 - (1-ilink(Xp1 %*% beta))^offset
  ME <- (pred1-pred0)
  
 # Adjust for numerical derivative if change is NULL
  if (is.null(change)) {
    ME <- ME/(d1[[term]] - d0[[term]])
  } 
  
  # convert to relative if requested
  if (relative == T) ME = ME/pred0
  
  # average marginal effect
  AME = mean(ME)
  
  
  # Simulate AMEs to compute uncertainty in the estimates
  
   # Compute the variance of the average marginal effect through a "posterior" simulation.
   # This involves simulating from a multivariate normal distribution using the model's  
   #coefficient means and covariance matrix
  
  if (!is.null(seed)) set.seed(seed)
  coefmat = mvrnorm(n = iterations
                    , mu = beta
                    , Sigma = vc)
  
    # For each simulated coefficient vector, estimate the Average Marginal Effect (AME).
  AMEs = apply(coefmat, 1, function(coefrow) {
    
    # Calculate marginal effects based on the simulated coefficient
    pred0   <- 1 - (1-ilink(Xp0 %*% coefrow))^offset
    pred1   <- 1 - (1-ilink(Xp1 %*% coefrow))^offset
    ME <- (pred1-pred0)
    
    # if change is NULL, use numerical derivative
    if (is.null(change)) {
      ME <- ME/(d1[[term]] - d0[[term]])
    } 
    
    # convert to relative if requested
    if (relative == T) ME = ME/pred0
    
    # average marginal effect
    AME = mean(ME)
    return(AME)
  })
  
  # Combine results
   # If the 'samples' flag is FALSE, return the summary results.
  # Otherwise, return both the summary and the sample results.
  
  if (!samples) {
    res = data.frame(term
                     , estimate = AME
                     , std.error = sqrt(var(AMEs))  
                     , estimate.sim = mean(AMEs)    
                     , offset
                     , change.value = paste(change, collapse = "_"))
    return(res) 
    
  } else {
    
    res_sums = data.frame(term
                     , estimate = AME
                     , std.error = sqrt(var(AMEs)) 
                     , offset
                     , change.value = paste(change, collapse = "_"))
    
    res_samples = data.frame(term
                             , estimate = AMEs
                             , MLE = AME
                             , offset
                             , change.value = paste(change, collapse = "_"))
    res = list(res_sums, res_samples)
    return(res)  
    
  }
}

```

## Calculating AME and rAME

```{r}
# Absolute AMEs -----------------------------------------------------------

# Initialize empty data frames to store the results
AME = data.frame()
AMEsamples = data.frame()

# Loop through predictor names that match "con_"
for (i in names(predictors)[grepl("con_", names(predictors))]) { 

# Loop through different change settings (e.g., equilibrium, invasion, iqr)
  for (j in names(change)) {
    
# Calculate the AME for each model in res_mod
    temp = lapply(res_mod, function(x){
      
# If the change is based on IQR (interquartile range), calculate the 1st and 3rd quartiles
      if (j == "iqr") {
        q1 = quantile(x$model$con_dens, probs = 0.25)
        q3 = quantile(x$model$con_dens, probs = 0.75)
      }

# Use the get_AME function to calculate the AME for the current model
      get_AME(x
              , data = x$model
              , offset = interval
              , term = i
              , change = eval(parse(text = change[[j]][,i]))
              , iterations = iter
              , samples = T
      )
    }
    )
    
    # AME
    tempAME = lapply(temp, function(x) x[[1]])
    tempAME = Map(cbind, tempAME, change = j, sp = names(tempAME))
    tempAME = do.call(rbind, tempAME)
    AME = rbind(AME, tempAME)
    
    # AME samples
    tempSamples = lapply(temp, function(x) x[[2]])
    tempSamples = Map(cbind, tempSamples, change = j, sp = names(tempSamples), iter = iter)
    tempSamples = do.call(rbind, tempSamples)
    AMEsamples = rbind(AMEsamples, tempSamples)
  }
}
head(AME)

# Relative rAMEs -----------------------------------------------------------


# Calculate relative rAMEs based on manual function get_AME
rAME = data.frame()
rAMEsamples = data.frame()
for (i in names(predictors)[grepl("con_", names(predictors))]) { 
    temp = lapply(res_mod, function(x){
      get_AME(x
              , data = x$model
              , offset = interval
              , term = i
              , change = eval(parse(text = change$equilibrium[, i]))
              , iterations = iter
              , relative = T
              , samples = T
      )
    }
    )
    
    # rAME
    tempAME = lapply(temp, function(x) x[[1]])
    tempAME = Map(cbind, tempAME, change = "equilibrium", sp = names(tempAME))
    tempAME = do.call(rbind, tempAME)
    rAME = rbind(rAME, tempAME)
    
 # rAME samples
  tempSamples = lapply(temp, function(x) x[[2]])
  tempSamples = Map(cbind, tempSamples, change = "equilibrium", sp = names(tempSamples), iter = iter)
  tempSamples = do.call(rbind, tempSamples)
  rAMEsamples = rbind(rAMEsamples, tempSamples)
}

head(rAME)

```

## Plot fixed with estimated AME

We had introduced various source of noise (e.g., to the species' response) in the simulation. Therefore, a fixed B_con_dens = 0.03 is not guaranteed but should be close to the fixed one. If desired, you can adjust the simulation to reduce this noise.

```{r}
# Add fixed rAME to data frame
AME$fixed_estimate <- 0.03

# Plotting
ggplot(AME, aes(y = sp)) +
  geom_point(aes(x = estimate)) +
  geom_errorbarh(aes(xmin = estimate - 1.96*std.error, 
                     xmax = estimate + 1.96*std.error), 
                 height = 0.25) +
  geom_point(aes(x = fixed_estimate), color = "steelblue2", shape = 16, position = position_dodge(width = 0.5)) +
  labs(title = "Estimated vs Fixed AME", 
       x = "AME", 
       y = "Species") +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0))

```
